{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session-1_DL with Pytorch-MNIST Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "1a991e7f539673da033393d34767c7335318ab9a",
        "id": "b5iappu36zJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Agenda:**\n",
        "<br>\n",
        "For this tutorial in  Deep Learning(DL) with Pytorch, we are going to explore Multi Layered Perceptron architecture and learn Pytorch by implementing  algorithms under a certain usecase.We will cover the following:\n",
        "1. Deep Learning basics with Pytorch\n",
        "2. Multilayered Perceptron (MLP) implemention on  MNIST\n",
        "<br>\n",
        "\n",
        "[Kaggle Kernel to run this notebook](https://www.kaggle.com/u6yuvi/dl-with-pytorch-mnist-classification?scriptVersionId=9647143)\n",
        "\n",
        "Lets get started !!\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f02da80b362a4233b75cb0f9e9656525e37befa",
        "id": "ib3wAE1B6zJi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](images/mlp.png)"
      ]
    },
    {
      "metadata": {
        "_uuid": "6145a827010b47e713d5bcdb6f89d8042040d75f",
        "id": "oWt4Syze6zJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **1. Deep Learning basics with Pytorch**\n",
        "<br>\n",
        "In this part we will cover the following:\n",
        "1. Learn to play with tensors on numpy and pytorch \n",
        "2. Learn to build a simple feed forward network from scratch with random data \n",
        "3. Learn to build an end to end MLP for MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "AopEXUDP6zJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Imports"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:21:32.419945Z",
          "start_time": "2019-02-04T11:21:27.622375Z"
        },
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "_3sIvxNa6zJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a241e3e9-05ea-4029-82e2-8c1f358c6894"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "#print(\"List of files\",os.listdir(\"../input\"))\n",
        "import torch\n",
        "import numpy as np\n",
        "print(\"Torch Version:\",torch.__version__)\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Torch Version: 1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1e8017c3ed94f083df4e2bf071f7f5f422c40ca1",
        "id": "I5rb-9Yi6zKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f08b918b1e2513ad7c1f382cfba39b0205aba6e",
        "id": "CqUj0MRg6zKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def test_network(net, trainloader):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # Create Variables for the inputs and targets\n",
        "    inputs = Variable(images)\n",
        "    targets = Variable(images)\n",
        "\n",
        "    # Clear the gradients from all Variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, then backward pass, then update weights\n",
        "    output = net.forward(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc2337505994f4530913183436b9f4bf8a118599",
        "id": "XiYzJ1F66zKG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensors\n",
        "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
        "\n",
        "Tensors are similar to NumPyâ€™s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
        "\n",
        "<img src=\"images/tensor_examples.svg\" width=600px>\n"
      ]
    },
    {
      "metadata": {
        "id": "3m_L3SnC6zKH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Construct a randomly initialized 5x3 matrix:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:21:36.971841Z",
          "start_time": "2019-02-04T11:21:36.623384Z"
        },
        "id": "_5sm5BSn6zKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d20b038a-e4a8-4437-c188-9d87902351d6"
      },
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6379, 0.7592, 0.6302],\n",
            "        [0.2110, 0.1513, 0.5589],\n",
            "        [0.9906, 0.0054, 0.9263],\n",
            "        [0.7238, 0.0602, 0.4015],\n",
            "        [0.2661, 0.0522, 0.2575]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y73ziPBd6zKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Construct a tensor directly from data:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:22:21.040158Z",
          "start_time": "2019-02-04T11:22:21.033606Z"
        },
        "id": "2GCKtxXO6zKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a977f530-7175-4ea1-b67e-a936c1b4e5af"
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([5.5, 3])\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.5000, 3.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1afe587702c9d925968dc34d4e7f515b700d9089",
        "id": "VMzFwRz36zKW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numpy to Torch and back\n",
        "\n",
        "PyTorch has a great feature for converting between Numpy arrays and Torch tensors. Let us see how easy it is to switch between the two\n",
        "\n",
        "### Ceate a tensor using numpy array"
      ]
    },
    {
      "metadata": {
        "_uuid": "35d8b903e1564ee833e2d0ed2ea00d40b61aa16e",
        "id": "7eOQ1-JX6zKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ba12a10c-8c36-4042-a08d-704d87f21dae"
      },
      "cell_type": "code",
      "source": [
        "np_array=np.random.randn(5,3)\n",
        "print(f' Numpy array:\\n {np_array}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Numpy array:\n",
            " [[ 0.29078775 -0.24078193  0.39532692]\n",
            " [-0.76269509 -0.46594533 -1.25577234]\n",
            " [-0.33151538 -0.67835184 -0.47210172]\n",
            " [-1.47579871 -0.81589375 -0.12434217]\n",
            " [-0.46677091 -0.15411824  0.216228  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1d82db999b723fa737510352cf47d96a62b8d63b",
        "id": "OY8cE3v66zKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert to torch tensor"
      ]
    },
    {
      "metadata": {
        "_uuid": "1cbd6fd48cf54645f924d354fc9fb5856bf1ddc1",
        "id": "rsT_nVzi6zKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6e9f8dae-f60d-481c-cc95-5f756af7020c"
      },
      "cell_type": "code",
      "source": [
        "torch_tensor=torch.from_numpy(np_array)\n",
        "print(f'Torch tensor:\\n {torch_tensor}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch tensor:\n",
            " tensor([[ 0.2908, -0.2408,  0.3953],\n",
            "        [-0.7627, -0.4659, -1.2558],\n",
            "        [-0.3315, -0.6784, -0.4721],\n",
            "        [-1.4758, -0.8159, -0.1243],\n",
            "        [-0.4668, -0.1541,  0.2162]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "31fe8979751ba061cc81055bbc5079bed1cb0124",
        "id": "Gh-DauqP6zKp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert back to numpy array"
      ]
    },
    {
      "metadata": {
        "_uuid": "e72bac0d8c4870f97b1d1724c0bce3fd84b50450",
        "id": "Sb7zmqpo6zKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "308a2524-f7a6-43bb-98a5-d7b62d940b73"
      },
      "cell_type": "code",
      "source": [
        "torch_tensor.numpy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.29078775, -0.24078193,  0.39532692],\n",
              "       [-0.76269509, -0.46594533, -1.25577234],\n",
              "       [-0.33151538, -0.67835184, -0.47210172],\n",
              "       [-1.47579871, -0.81589375, -0.12434217],\n",
              "       [-0.46677091, -0.15411824,  0.216228  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "532f4e612d823389740276fdb2f1d0c9d69cb78d",
        "id": "N6kFqpBr6zKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***An important thing to note here is memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well.*       \n",
        "Let see what does it mean**"
      ]
    },
    {
      "metadata": {
        "_uuid": "7a08bc968bbb69b18d7d376748f7a5333f931efe",
        "id": "6m33BM0n6zKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "622c0785-6b2b-4944-8ec3-b7d33df80634"
      },
      "cell_type": "code",
      "source": [
        "# Add 2 to PyTorch Tensor, in place\n",
        "torch_tensor.add_(2)\n",
        "\n",
        "#Anand: If we use _ after a method, that means the operations are done in-place"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.2908, 1.7592, 2.3953],\n",
              "        [1.2373, 1.5341, 0.7442],\n",
              "        [1.6685, 1.3216, 1.5279],\n",
              "        [0.5242, 1.1841, 1.8757],\n",
              "        [1.5332, 1.8459, 2.2162]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "15c233bd4f3539d1cf8c07878d01802607a6ab89",
        "id": "1l_0MtSN6zK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Numpy array matches new values from Tensor"
      ]
    },
    {
      "metadata": {
        "_uuid": "3d4047f95de1ca8055e276de3246da2c6a01ccee",
        "id": "Qa_0xss96zK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3200bd62-f38c-4a4b-b76b-c5a0809e7646"
      },
      "cell_type": "code",
      "source": [
        "np_array"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.29078775, 1.75921807, 2.39532692],\n",
              "       [1.23730491, 1.53405467, 0.74422766],\n",
              "       [1.66848462, 1.32164816, 1.52789828],\n",
              "       [0.52420129, 1.18410625, 1.87565783],\n",
              "       [1.53322909, 1.84588176, 2.216228  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b183396c1d82a600e1c3b1848e94e3ce5a6fbe28",
        "id": "xweAv4OV6zK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Simple Neural Network using Pytorch \n",
        " Let us see how we can use PyTorch to build a simple neural network.\n",
        "![](images/simple_neuron.PNG)\n",
        "\n",
        "Mathematically this looks like: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
        "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "With vectors this is the dot/inner product of two vectors:\n",
        "\n",
        "$$\n",
        "h = \\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots  x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_1 \\\\\n",
        "           w_2 \\\\\n",
        "           \\vdots \\\\\n",
        "           w_n\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "_uuid": "496c485f3e7f43a27bdce042df78fa11eb296631",
        "id": "wg6SHT1A6zK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
      ]
    },
    {
      "metadata": {
        "_uuid": "93562b119ff2797b3660bb0968bc6c31e890b7e5",
        "id": "jyEqAmVE6zLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Generate some random data \n",
        " We will create a tensor with shape (1, 5), one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one."
      ]
    },
    {
      "metadata": {
        "_uuid": "fd1ee538877f19c8a2cfac8ea123ecbdb2f4a94d",
        "id": "DO-EMOG46zLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e956c792-4c54-4d67-db5d-12805fbe53bd"
      },
      "cell_type": "code",
      "source": [
        "features=torch.randn(1,3)\n",
        "print(f'Number of Inout features:{features.shape[1]}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Inout features:3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b0673d28508dd5eed49da6731c4cc1408f1efd5c",
        "id": "gZbQ_da06zLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize Weights and Biases "
      ]
    },
    {
      "metadata": {
        "_uuid": "7f39730f8dbaff0834b223744d0bab6bfc516e2c",
        "id": "4EHohMTZ6zLP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weights = torch.randn_like(features) creates another tensor with the same shape as features, again containing values from a normal distribution.\n",
        "\n",
        "Finally, bias = torch.randn((1, 1)) creates a single value from a normal distribution."
      ]
    },
    {
      "metadata": {
        "_uuid": "3d2b5d527e0650f3d3c1942a5ee1da56bbbac7de",
        "id": "cPtnmfMh6zLQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_input=features.shape[1]\n",
        "n_hidden=2\n",
        "n_output=1\n",
        "#Weights for input to hidden layer\n",
        "W1=torch.randn(n_input,n_hidden)\n",
        "W2=torch.randn(n_hidden,n_output)\n",
        "#Bias term for hidden and output layer\n",
        "B1=torch.randn(n_hidden)\n",
        "B2=torch.randn(n_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd62c805fb7363693b56ec9dbcd00a8c404adcba",
        "id": "5Zy8N3Ew6zLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Using a Sigmoid Activation Function\n",
        "def activation(x):\n",
        "    return(1/1+torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbfeec62cfa4a35cb829897b71a4450b70ec8392",
        "id": "01pqfVfA6zLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate Weight and Biases\n",
        "We will calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`."
      ]
    },
    {
      "metadata": {
        "_uuid": "bd98acc8aa8bb1d2fe06cb77ffd4a5054c07a40b",
        "id": "Ut17RCag6zLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c74be450-cdba-487a-90b9-34f6cd55059d"
      },
      "cell_type": "code",
      "source": [
        "h1=activation(torch.matmul(features,W1)+B1)\n",
        "print(f'Hidden Layer activations:{h1}')\n",
        "out=activation(torch.matmul(h1,W2)+B2)\n",
        "print(f'Output of the network:{out}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden Layer activations:tensor([[ 1.9513, 16.2414]])\n",
            "Output of the network:tensor([[1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "21b8946d94159fc86b191a82bbbb4e34d2289f53",
        "id": "AODQYWro6zLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building our Network\n",
        "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image using MNIST data\n",
        "For now our goal will be to build a neural network that can take one of these images and predict the digit in the image.First, let's try to build this network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's `nn` module which provides a much more convenient and powerful method for defining network architectures."
      ]
    },
    {
      "metadata": {
        "_uuid": "c4c2229e3f3534db73e8d22ba73485022765f3af",
        "id": "NYSs40i-6zLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](images/mnist.png)"
      ]
    },
    {
      "metadata": {
        "_uuid": "d2974386321060bd66905127f3a48b9f1b177310",
        "id": "5eSZRuGK6zLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import helper\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "237092ef839e8201ecc710f53ed7154e0f865b2e",
        "id": "cQqmPQwA6zLp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Dataset \n",
        "First up, we need to get our dataset.Right now we will be using MNIST dataset which is already in`torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. "
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:27:05.872957Z",
          "start_time": "2019-02-04T11:26:45.305539Z"
        },
        "_uuid": "45cde9b49e2c0e1802ff640ebab0d766233e6abe",
        "id": "4Ju9FCad6zLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Anand(up): 'datasets' module above is used to download small in-built datasets like MNISt for learning\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              ])\n",
        "#Anand(up): We can download as numpy array or as pandas. To transform our dataset to tensor, we use .ToTensor method...\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "#Anand(up): trainset loads the MNIST dataset and trainloader loads the dataset in batches of size 64 which will be iterated later on"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5qq1t3P6zLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### what is dataset ?\n",
        "Dataset contains two data methods `__getitem__` and `__len__` so using these methods. we can directly call a single data with index"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:31:37.364338Z",
          "start_time": "2019-02-04T11:31:37.283418Z"
        },
        "id": "WNXQsnYE6zL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1921
        },
        "outputId": "14865991-f080-4c98-8bd4-5530a01ce4f8"
      },
      "cell_type": "code",
      "source": [
        "# like this way\n",
        "trainset[0]\n",
        "\n",
        "#Anand(up): dataset has two inbuilt overriding methods getitem for indexing and len for getting len of a tensor. (remember IBM edX course)\n",
        "#Anand(up): MNIST dataset has a dimension of 28*28 for each image"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]), tensor(5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "wUH2IhDc6zL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### what is dataloader?\n",
        "\n",
        "It simply uses the generator to provide data giving single- or multi-process iterators over the dataset."
      ]
    },
    {
      "metadata": {
        "_uuid": "00ab24a0af33038559ec91ea83458558e4d574bf",
        "id": "01wBT7936zL7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have the training data loaded into trainloader \n",
        "\n",
        "With dataloaded we make  an iterator with iter(trainloader). Later, we'll use this to loop through the dataset for training, like below:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:04.781132Z",
          "start_time": "2019-02-04T11:38:04.633040Z"
        },
        "_uuid": "9808e8dab8f56248ae40759f20b1e59dad3ede7b",
        "id": "E16bbUld6zL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8317d2a9-d9c1-4ca3-c98d-954ac07e243a"
      },
      "cell_type": "code",
      "source": [
        "#Anand(down): we iterate over 64 data at a time .ie. we apply a for loop over the dataset for 64 items at a time\n",
        "dataiter = iter(trainloader)\n",
        "#Anand(down): Assigning 64 image data into images and labels seperately\n",
        "images, labels = dataiter.next()\n",
        "#Anand(down): We see that type is tensor\n",
        "print(type(images))\n",
        "#Anand(down): Pytorch prints in Batch x Channel x Lenth x Width ...so 64 x 1 x 28 x 28\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "#Printing the size of one image\n",
        "#Anand(down): Numpy expects in batch x height x width x channel for visualization.\n",
        "print(images[1].numpy().squeeze().shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f544046855f3efd0eb1d88fb11f0e4e27318d5eb",
        "id": "fufMmR3b6zMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "eb046225-3a4b-409a-e1bd-b7840cc33fcd"
      },
      "cell_type": "code",
      "source": [
        "#Look at the image\n",
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAHwCAYAAACym4blAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne5r9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 249,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:08.273406Z",
          "start_time": "2019-02-04T11:38:08.264208Z"
        },
        "_uuid": "b11c0c543d20cfb96cbb0635eea5f93e37c58cb0",
        "id": "O36Eff-m6zMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sigmoid Activation Function\n",
        "def activation(x):\n",
        "    return (1/(1+torch.exp(-x)))\n",
        "\n",
        "#Input 64x784\n",
        "inputs=images.view(images.shape[0],-1)\n",
        "#Number of input features-784\n",
        "n_input=inputs.shape[1]\n",
        "#Number of neurons in hidden layer-256\n",
        "n_hidden=256\n",
        "#Number of output neuron-10\n",
        "n_out=10\n",
        "#Weight at hidden neuron-784x256\n",
        "W1=torch.randn(n_input,n_hidden)\n",
        "#Bias at hidden neuron-256\n",
        "B1=torch.randn(n_hidden)\n",
        "#Weight at output neuron-256x10\n",
        "W2=torch.randn(n_hidden,n_out)\n",
        "#Bias at output neuron-10\n",
        "B2=torch.randn(n_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:13.520858Z",
          "start_time": "2019-02-04T11:38:13.510149Z"
        },
        "_uuid": "e494abcbcf8bf4e1210b1acdc0790ca77f826e21",
        "id": "YV00H5_Y6zMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c90ababd-970a-4a93-e68e-77beccf3531e"
      },
      "cell_type": "code",
      "source": [
        "print(\"Shape of a batch of an image:\",images.shape)\n",
        "print(\"Shape of the input to the network:\",inputs.shape)\n",
        "print(\"Shape of the input features:\",n_input)\n",
        "print(\"Shape of the Weight matrix of neurons in the hidden layer\",W1.shape)\n",
        "print(\"Shape of the Bias vector of neurons in the hidden layer\",B1.shape)\n",
        "print(\"Shape of the Weight matrix of neurons in the output layer\",W2.shape)\n",
        "print(\"Shape of the Bias vector of neurons in the output layer\",W2.shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of a batch of an image: torch.Size([64, 1, 28, 28])\n",
            "Shape of the input to the network: torch.Size([64, 784])\n",
            "Shape of the input features: 784\n",
            "Shape of the Weight matrix of neurons in the hidden layer torch.Size([784, 256])\n",
            "Shape of the Bias vector of neurons in the hidden layer torch.Size([256])\n",
            "Shape of the Weight matrix of neurons in the output layer torch.Size([256, 10])\n",
            "Shape of the Bias vector of neurons in the output layer torch.Size([256, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:23.336380Z",
          "start_time": "2019-02-04T11:38:22.958331Z"
        },
        "_uuid": "e751026ecf95253bf33db0462ce8bc250c082bb7",
        "id": "8WZsnck86zMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hidden layer activations\n",
        "h1=activation(torch.mm(inputs,W1)+B1)\n",
        "#Output layer activations\n",
        "out=activation(torch.mm(h1,W2)+B2)\n",
        "\n",
        "#Anand (note): for matrix multiplication, both mm and matmul does the same. It is just for ease for people familiar with numpy...both does the same. See: https://github.com/myazdani/numpy-pytorch-cheatsheet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:26.658141Z",
          "start_time": "2019-02-04T11:38:26.649686Z"
        },
        "_uuid": "2cb2a0f4a6d9852da5d4811e89734c8906bbb878",
        "id": "7fgG0OgY6zMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94b7ae25-617a-45b5-88c3-c3dc3779449c"
      },
      "cell_type": "code",
      "source": [
        "print(f'Shape of the Hidden activation of the network{h1.shape}')\n",
        "print(f'Shape of the Output of the network{out.shape}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the Hidden activation of the networktorch.Size([64, 256])\n",
            "Shape of the Output of the networktorch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:38:38.560819Z",
          "start_time": "2019-02-04T11:38:38.553507Z"
        },
        "_uuid": "66671445031ee39d4142f84c832b2ab097f6d14a",
        "id": "Cx-0otcV6zMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c028562a-2788-4715-c911-34f65faccf87"
      },
      "cell_type": "code",
      "source": [
        "#Let us see the network output to one of the feeded input image\n",
        "out[1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9.9880e-01, 1.0000e+00, 5.1209e-01, 1.0000e+00, 2.3608e-03, 1.3468e-12,\n",
              "        3.8673e-04, 6.5829e-05, 1.0000e+00, 1.0000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8c7ab4cad2d8a2d619e0b376780e0d5b2dfa7be3",
        "id": "sIR3Y6sr6zMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have 10 outputs for our network. This raw output is usually called **logits or scores**.\n",
        "<br>\n",
        "However,We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to."
      ]
    },
    {
      "metadata": {
        "_uuid": "1903f5520e57c8ca69b4252ed7a7c4d5e5beb51a",
        "id": "UBJN8ANa6zMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Probability Distribution using Softmax\n",
        "To calculate this probability distribution, we often use the [softmax function](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "$$\n",
        "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
        "$$\n",
        "What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:40:41.701155Z",
          "start_time": "2019-02-04T11:40:41.697492Z"
        },
        "_uuid": "f4d7ba5cc02acf1610fa113490099e51e84978aa",
        "id": "QDcPVfIg6zMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    #Anand(down): we need to understand why the dimension is equal to 1. It is '1' because we need to calculate across columns (horizontally) and not across rows (vertically)...ie. first dimension\n",
        "    #Anand (down): In PyTorch dimension '1' means across column and domension '0' means across row\n",
        "    return(torch.exp(x)/torch.sum(torch.exp(x),dim=1).view(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdf3ab4229b26432555148c0d1b67f6e857055e6",
        "id": "6h8HcBFa6zM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us understand what we are doing above by an example\n",
        "<br>\n",
        "Step 1:Calculating the numerator of the softmax function"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:40:42.976206Z",
          "start_time": "2019-02-04T11:40:42.970484Z"
        },
        "_uuid": "d06ddfc05e73456929293906a0ee9672cf2f45a6",
        "id": "lA0lWB4-6zNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e3cb7600-ffef-411b-cbc7-eb8ca7fbb593"
      },
      "cell_type": "code",
      "source": [
        "#Anand(down): is we have 64 input it is hard to visualize...so lets take only for first 2 images out of 64 images. We can see the probability of \n",
        "              #different numbers from 1 to 10 for the first image\n",
        "torch.exp(out[1:3])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.7150, 2.7183, 1.6688, 2.7183, 1.0024, 1.0000, 1.0004, 1.0001, 2.7183,\n",
              "         2.7183],\n",
              "        [2.7183, 1.1796, 1.9579, 2.7168, 2.5085, 1.0000, 1.0002, 1.3614, 2.7182,\n",
              "         2.7183]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0fd149046af107a72d73208528b3a5cf1a6eb13e",
        "id": "5jZAthlR6zNV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 2:For every predicted image output, calculate the sum over the predicted values over all classes"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:40:45.248689Z",
          "start_time": "2019-02-04T11:40:45.236760Z"
        },
        "_uuid": "2277f07ce4566f91bf8ad70e3969f2c4790e676d",
        "id": "vqG553706zNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a97b92ab-8dc7-4d9d-997b-af7b148e81d9"
      },
      "cell_type": "code",
      "source": [
        "#print(torch.sum(torch.exp(out[1:3])))\n",
        "#Dim=1 says, we want to take the sum across all columns\n",
        "torch.sum(torch.exp(out[1:3]),dim=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([19.2597, 19.8791])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e247c285cf5dfdd02702bc79ddc8e49383bc17c1",
        "id": "zuBPEbVF6zNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step3:Rearrange the sums in an order for broadcasting to work"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:40:47.116271Z",
          "start_time": "2019-02-04T11:40:47.106860Z"
        },
        "_uuid": "8c226fe7a4a2e9f31f63ddca18ed71959d9a139d",
        "id": "kYJWVTMw6zNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b3f7770-a846-489c-c2e9-4d640d169c6d"
      },
      "cell_type": "code",
      "source": [
        "torch.sum(torch.exp(out[1:3]),dim=1).view(-1,1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19.2597],\n",
              "        [19.8791]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ecbcd104b966f3ae051d09048e7d708f992f1a41",
        "id": "Z8LXUHCS6zOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 3:For every predicted image output, divide the predictions of each class with the sum over all classes."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T11:41:41.871814Z",
          "start_time": "2019-02-04T11:41:41.808608Z"
        },
        "_uuid": "fce3f0e89d812e35f44a46ee5eab0fc85bf05406",
        "id": "fdZNm1EB6zOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "db9facda-d718-4cbf-87a6-74bbf4123b58"
      },
      "cell_type": "code",
      "source": [
        "#print(torch.exp(out[1:3])/torch.sum(torch.exp(out[1:3]),dim=1))\n",
        "temp=torch.exp(out[1:3])/torch.sum(torch.exp(out[1:3]),dim=1).view(-1,1)\n",
        "print(temp)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1410, 0.1411, 0.0866, 0.1411, 0.0520, 0.0519, 0.0519, 0.0519, 0.1411,\n",
            "         0.1411],\n",
            "        [0.1367, 0.0593, 0.0985, 0.1367, 0.1262, 0.0503, 0.0503, 0.0685, 0.1367,\n",
            "         0.1367]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0973f2a30eacf6d699df25a24ce00f9487f35559",
        "id": "iS0i1ZuQ6zOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Voila!! We got the softmax output .One last thing to do is check whether the sum across all classes sum to 1 for understanding the predicted class"
      ]
    },
    {
      "metadata": {
        "_uuid": "5f899daaaf970fbb7b821a9cf4f70af1b00a9773",
        "id": "vSfZtNU16zOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "180b1910-cf0c-4427-9717-7016a1097ae6"
      },
      "cell_type": "code",
      "source": [
        "temp.sum(dim=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "da2f00d033b1ef41cf3941998ef34e24167a08cc",
        "id": "oT42sQRm6zOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b64b949-78b9-4124-830c-b15157c62aea"
      },
      "cell_type": "code",
      "source": [
        "probabilities = softmax(out)\n",
        "# Does it have the right shape? Should be (64, 10)\n",
        "print(probabilities.shape)\n",
        "# Does it sum to 1?\n",
        "#print(probabilities.sum(dim=1))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f9cd86feb1156d1a7ba69037197aabf876735be6",
        "id": "8Ie4xnJp6zO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building our Network with Pytorch\n",
        "\n",
        "![](images/mlp_mnist.png)"
      ]
    },
    {
      "metadata": {
        "_uuid": "6b0737a96ecaa7d4bcd0f6bc4592961e5ffc8e42",
        "id": "cVwebet96zO4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output."
      ]
    },
    {
      "metadata": {
        "_uuid": "e82cac77674746147dc9dd5d9367b21c1e02d40c",
        "id": "K8moMGvf6zO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72a3786079aa4a387146ff066176d5af56b11b4b",
        "id": "TfwsMv9J6zPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  #Anand (down): In init, we define all the layers we want to use. \n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        #Anand (down): nn.linear means arrange all the pixels linearly...like here we want the 28x28 to be in linear of 784\n",
        "        self.hidden=nn.Linear(784,256)\n",
        "        self.output=nn.Linear(256,10)\n",
        "        #Anand (down): we are using 2 fns- Sigmod fn is used for input and hidden layers AND softmax for output layer.\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "        self.softmax=nn.Softmax(dim=1)\n",
        "    \n",
        "   #Anand (down): Here, 'x' is the input layer which is our 784 images. What we are doing is telling the sequence of operations. Here, pass x to hidden layer, \n",
        "         #then to apply sigmoid fn, then to output layer and then apply softmax fn.\n",
        "   #Anand (down): Here, we are overriding the forward() method as we want.\n",
        "  def forward(self,x):\n",
        "        x=self.hidden(x)\n",
        "        x=self.sigmoid(x)\n",
        "        x=self.output(x)\n",
        "        x=self.softmax(x)\n",
        "        return x\n",
        "      \n",
        "    #Anand (down): Actually, we also have to give backward() override. But, we will be suing autograd fn which includes the backpropagation fn. If we are using a custom loss fn, then, we also have to give a backward() method in this class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1546fd3ac7e6ab35a81dfee3e704817627a5618",
        "id": "8if_-MMT6zPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go through this bit by bit.\n",
        "\n",
        "```python\n",
        "class Network(nn.Module):\n",
        "```\n",
        "\n",
        "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
        "\n",
        "```python\n",
        "self.hidden = nn.Linear(784, 256)\n",
        "```\n",
        "\n",
        "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network once it's create at `net.hidden.weight` and `net.hidden.bias`.\n",
        "\n",
        "```python\n",
        "self.output = nn.Linear(256, 10)\n",
        "```\n",
        "\n",
        "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
        "\n",
        "```python\n",
        "self.sigmoid = nn.Sigmoid()\n",
        "self.softmax = nn.Softmax(dim=1)\n",
        "```\n",
        "\n",
        "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "```\n",
        "\n",
        "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
        "\n",
        "```python\n",
        "x = self.hidden(x)\n",
        "x = self.sigmoid(x)\n",
        "x = self.output(x)\n",
        "x = self.softmax(x)\n",
        "```\n",
        "\n",
        "Here the input tensor `x` is passed through each operation a reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
        "\n",
        "Now we can create a `Network` object."
      ]
    },
    {
      "metadata": {
        "_uuid": "36bc9a286ffafd0a0b1ac7890906092965f5fee0",
        "id": "BzyRecYq6zPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "de2ce054-bcda-458c-bae9-ae0f887632fc"
      },
      "cell_type": "code",
      "source": [
        "model=Network()\n",
        "model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (softmax): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "aae8f6f5e7c42a3df7d79b6e3a20293f3a673b0a",
        "id": "holBYiLQ6zPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can define the network somewhat more concisely and clearly using the `torch.nn.functional` module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as `F`, `import torch.nn.functional as F`.\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "1ebea291b4d96d74d435885e60bc2a792c4abd12",
        "id": "_W9iooVr6zPi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Anand (down): In Pytroch, we define all our layers using .nn \n",
        "#Anand (down): Whenever we want to use loss fns, we use .functional...Tis way we don;t have to define them in our __init__ \n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "41e688fd7486e6ac4ea420cdc23e464a4e2c3432",
        "id": "kTprwqAP6zPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(784, 128)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Hidden layer with sigmoid activation\n",
        "        #Anand (down): The x=x.hidden() is given in one step and passed into sigmoid fn for readability purpose.\n",
        "        x = F.sigmoid(self.hidden(x))\n",
        "        # Output layer with softmax activation\n",
        "        x = F.softmax(self.output(x), dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "30f4a6c129eb490f265d2570e16f86dc96534952",
        "id": "QPs60A-f6zP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fde88dbd-9641-4cb6-b9fd-5c9fb090f44b"
      },
      "cell_type": "code",
      "source": [
        "model=Network()\n",
        "model"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8c3820bb60313a368c0a3d8322b21b7d8adeafeb",
        "id": "Cp0XD_ia6zP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initializing weights and biases\n",
        "\n",
        "The weights and bias are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
      ]
    },
    {
      "metadata": {
        "_uuid": "d148d7e2446d76d5ca6a8948d23ec513f103cdba",
        "id": "pwpPgA7_6zQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "930d5d83-55b3-461f-e260-5bb115fd9404"
      },
      "cell_type": "code",
      "source": [
        "#Anand (down): model.<layer-name>.weight gives the weights\n",
        "#Anand (down): Similarly, model.<layer-name>.weight.shape gives the shape of the weight\n",
        "print(model.hidden.weight,model.hidden.weight.shape)\n",
        "\n",
        "\n",
        "#Anand (down): model.<layer-name>.bias gives the biases\n",
        "#Anand (down): Similarly, model.<layer-name>.bias.shape gives the shape of the biases\n",
        "print(model.hidden.bias,model.hidden.bias.shape)\n",
        "\n",
        "#Anand (note): requires_grad= True allows backward propagation. If it is False, we can't do backpropagation. 'False' is used when we want to freeze our n/w"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0225,  0.0081, -0.0315,  ...,  0.0345,  0.0303,  0.0008],\n",
            "        [-0.0217,  0.0066, -0.0025,  ...,  0.0286, -0.0233,  0.0324],\n",
            "        [-0.0082,  0.0054,  0.0215,  ..., -0.0149, -0.0248,  0.0345],\n",
            "        ...,\n",
            "        [-0.0146,  0.0105,  0.0036,  ..., -0.0126,  0.0107, -0.0047],\n",
            "        [ 0.0258,  0.0241, -0.0151,  ...,  0.0286, -0.0334, -0.0001],\n",
            "        [ 0.0139, -0.0043, -0.0269,  ...,  0.0014,  0.0285, -0.0149]],\n",
            "       requires_grad=True) torch.Size([128, 784])\n",
            "Parameter containing:\n",
            "tensor([-0.0291,  0.0250,  0.0277,  0.0074,  0.0071, -0.0084, -0.0290, -0.0333,\n",
            "        -0.0275,  0.0011, -0.0295, -0.0017, -0.0242, -0.0063,  0.0033, -0.0149,\n",
            "         0.0043, -0.0291,  0.0106, -0.0201,  0.0275,  0.0008, -0.0250, -0.0203,\n",
            "        -0.0119,  0.0232,  0.0178, -0.0254, -0.0263,  0.0216,  0.0039,  0.0232,\n",
            "        -0.0148,  0.0291,  0.0248,  0.0268,  0.0165, -0.0275,  0.0212,  0.0131,\n",
            "         0.0164,  0.0044,  0.0341, -0.0304,  0.0263, -0.0291, -0.0085,  0.0306,\n",
            "         0.0086,  0.0122, -0.0164, -0.0181,  0.0289, -0.0063,  0.0173, -0.0057,\n",
            "         0.0306,  0.0128, -0.0327, -0.0059,  0.0209, -0.0112,  0.0046,  0.0345,\n",
            "        -0.0052, -0.0273, -0.0281, -0.0284, -0.0114, -0.0021,  0.0282, -0.0250,\n",
            "        -0.0225,  0.0189,  0.0039,  0.0053, -0.0317,  0.0078, -0.0251,  0.0087,\n",
            "        -0.0056, -0.0001,  0.0084,  0.0269, -0.0319,  0.0107, -0.0233,  0.0020,\n",
            "        -0.0357, -0.0147,  0.0237,  0.0344,  0.0356,  0.0151,  0.0007,  0.0165,\n",
            "         0.0275, -0.0255,  0.0229,  0.0260,  0.0092,  0.0035, -0.0270, -0.0160,\n",
            "        -0.0142,  0.0272, -0.0010,  0.0150,  0.0288, -0.0221, -0.0010,  0.0015,\n",
            "        -0.0099,  0.0031, -0.0307, -0.0209, -0.0139, -0.0217,  0.0009, -0.0005,\n",
            "        -0.0090, -0.0187, -0.0201, -0.0153, -0.0285, -0.0183,  0.0088,  0.0115],\n",
            "       requires_grad=True) torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1a1989c40df55dd4324cad2736e72481c148299d",
        "id": "wdtXzhBZ6zQf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For custom initialization, we can these tensors in place."
      ]
    },
    {
      "metadata": {
        "_uuid": "dc98f7292fe8d6f40b1ea58b681acf522f500bb2",
        "id": "QvBc4dW16zQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d34ac46b-1647-4b97-bad8-78d443501d36"
      },
      "cell_type": "code",
      "source": [
        "# Set biases to all zeros\n",
        "#Anand (down): If we want to manipulate the data of a tensor, we use the .data. Here, we are filling the data of bias with zero for initial\n",
        "model.hidden.bias.data.fill_(0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "354f3f9fbcf8b0c6808394ea97e794b9f43ffe60",
        "id": "rR6qBf_v6zQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8512f040-c073-418c-b527-cd8d7367cebf"
      },
      "cell_type": "code",
      "source": [
        "# sample from random normal with standard dev = 0.01\n",
        "model.hidden.weight.data.normal_(std=0.01)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0072,  0.0176,  0.0071,  ..., -0.0210,  0.0149, -0.0031],\n",
              "        [-0.0083,  0.0044,  0.0141,  ..., -0.0083, -0.0009, -0.0056],\n",
              "        [-0.0036, -0.0019, -0.0021,  ...,  0.0056,  0.0075, -0.0117],\n",
              "        ...,\n",
              "        [ 0.0150,  0.0050,  0.0055,  ..., -0.0175, -0.0112, -0.0103],\n",
              "        [-0.0067, -0.0020, -0.0043,  ..., -0.0066, -0.0065, -0.0157],\n",
              "        [-0.0062,  0.0039, -0.0023,  ..., -0.0064,  0.0127, -0.0014]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3b948402fbcd6e8529fe5ebcd00347f1db63a1c",
        "id": "JCW_8M3m6zRQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "netowrk=Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1eb03d730d47d46308e593ff4debbd6899280bbe",
        "id": "8s1Q2Gs26zRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forward pass\n",
        "\n",
        "Now that we have a network, let's see what happens when we pass in an image."
      ]
    },
    {
      "metadata": {
        "_uuid": "59df5bda3337698adad159777311294b7f8cfce4",
        "id": "p7e0oLdD6zRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "933c2c7d-f33b-473f-d35d-d687139b7080"
      },
      "cell_type": "code",
      "source": [
        "# Grab some data \n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
        "#Anand (down): Making into a flat thing..ie. making into a single line. Here, images.shape[0] means we are currently passing only the first image\n",
        "images.resize_(images.shape[0], 1, 784)\n",
        "\n",
        "# Forward pass through the network\n",
        "#Anand (down): Image index=0 .ie. the first image...Then, performing a forward pass on only the first image (we already trained the model)\n",
        "img_idx = 0 \n",
        "ps = model.forward(images[img_idx,:])\n",
        "\n",
        "img = images[img_idx]\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAHHCAYAAABeJdLdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACgh/8PWnLf9tQB0S0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 424,
              "height": 227
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2103e632ea834fcbd4741de8b9ee7edace23ba6b",
        "id": "FMYOlzHv6zRk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, , all the weights are random!"
      ]
    },
    {
      "metadata": {
        "_uuid": "6d611ac89023080d6ee0e42b9369376ed261a777",
        "id": "dOfkvndq6zRm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Add-on-People from the keras would love this!!!\n",
        "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential`.\n",
        "Lets try to build the above network using this method:"
      ]
    },
    {
      "metadata": {
        "_uuid": "e6f5405fe980380825786437c0aa8e94cb8cb92e",
        "id": "D4X74-ut6zRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2e3b55c9-d137-4689-b4a2-020cc68b5b11"
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters for our network\n",
        "input_size = 784\n",
        "hidden_sizes = [128]\n",
        "output_size = 10\n",
        "\n",
        "#Anand (down): nn.sequential is used when we have a set of sequemtial layers. We are using RelU...we can even use sigmoid\n",
        "model=nn.Sequential(nn.Linear(input_size,hidden_sizes[0]),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(hidden_sizes[0],output_size),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Softmax(dim=1))\n",
        "print(model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d461b698ae5973918405c9120db6e94194347a27",
        "id": "d8wrg6VZ6zRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b18d5014-11ee-4034-b65a-d0a2bddabb87"
      },
      "cell_type": "code",
      "source": [
        "# Forward pass through the network and display output\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(images.shape[0], 1, 784)\n",
        "ps = model.forward(images[0,:])\n",
        "view_classify(images[0].view(1, 28, 28), ps)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAHHCAYAAABeJdLdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xu8pfXc//HXLmomJuluCnepwe3j\nGEIHoRrn/ObOKTlU5HDrgDAOkTRC7h8NZdA4VEOEHFJDdbtvmqJSiFTyoVtROW1+mBFFmt8f17Vn\nL7u9p/muvfa69rXW6/l4zONqX9f1Xeuzv63Zs977c13fNbJ27VokSZIkSRtmo6YLkCRJkqQ2MURJ\nkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQV\nMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVuFPTBUiSJPVKROwGHADsAdwLmAP8CvgFcAbw\nqcz8/YQxOwDXAmTmSD/r7aWIWAG8aIrDtwK/B74HnJKZX+hzTW/PzCUz/FzXAdsDe2Xmqg0cswQ4\nGvhEZr643rcDk7weImIV1evqoMxc0aOy1VKGKEmS1HoRcVfgZGDfetcPgZXAGuBfgYVUb4DfEREv\nyswzGim0P64AvjFh3xZAAHsDe0fE54HnZ+Y/+l3cLPNt4ATg0g049wvAD4Afde6MiHcDR7Q5gKuc\nIUqSJLVaRGwKfB3YmeoN7kGZeemEc7YElgCvAr4YEftl5uf7XWufXJSZr5nsQETsAXyFKmx+D/i/\n/SxstsnMc4FzN/DcD05x6NG9q0ht4T1RkiSp7Y6hClDXAntMDFAAmfn/MvPVwPHACPCBiJjX3zKb\nl5nnA2+vvzykyVoGQUSMAI9qug71n50oSZLUWhFxd+Cw+svXZObv7mDIUVSX+H0+M9dswONvDSwG\nngbcF9gEGAUuBN6Tmd+ZZMzDgTcCjwW2AW4GbqTqAB2fmb+ccP6ewOHALsB8YDVwPfAl4AOZ+cc7\nqrPQqnq7fURsMfb4EbG23r8NcCTwAuD/ZWZ01LoV1XwsAnYANgZ+SdUJPC4zfzLVk0bEtsA7gScB\nWwG/Bc4C3jbxPrX6/H2Al1OFlH8BbgF+CnwOeH9m3rKe59odeCtVuL4L8DPglHrcrR3nLWHCPVHr\necxVdNwTNfEetI75OxhYDtwG3Dszb5zi8X4K3A/YPzM/vb7n1uxjJ0qSJLXZ3lRvkn9JFVLWKzP/\nnJlvy8wr7ujciLgXcBlVINqc6s37yVSLVDwHuCginjxhzOOo7rN5HtUb9xXAF6nec70B+HZE3Lvj\n/OdR3b/0dKr7uD4OfBXYkqpjdH5EbH5HtRbqDGVzJjl+GPBC4Ezgyx21/hvwfeAI4K51nZ8DbqIK\nO9+PiIVTPOe/AJdQXfr2FeCzwFzgUKrvcbPOkyPibfVzP6V+zo/V43YA3g2cGxFTvY99PHAesGld\n39lUAfg9wEenGNONr1EFszEn1H++DnyL6v/5gZMNjIhHUgWo1VRhWS1jJ0qSJLXZbvX2m5l5W48f\n+3CqRSm+Azw+M28eOxAR76DqdCwFHtox5kiqN+8HZ+ZHOs4foXqD/SrgNcDr6kNLqC4vXJSZ/9Vx\n/p2pAsAzgRcDH+jh93W/ensTMFnn7gDgkZn58wn7PwFsC5wOvHBCR+ddwFuAUyPiPpN0iV5B1Z05\nPDPX1mO2obov68FUwe299f67Uc0jwH6ZuS5k1GOuAvakmpsvTlL/kcALOlcgjIixYHVQRJw4WQex\nVGaeFhEXAQfVX6+7Dy0iTqLqRL6YKvRN9Lx6+7nM/Ot0a1H/2YmSJElt9q/19toZeOxzgFcCizsD\nVG0sID0kIrbo2L+g3l7UeXIdHI4EHgP85wac/3eqy8J2Bk7t9huYwtjlj1/tDEIdvj4xQNWdk92A\nvwOHTTJuCdVljvcC/s8kj3kT8KaxAAWQmb8BltVfPrfj3DsB/wG8nmpZeiaMGeuOPW6ybw5YNXEJ\n98y8gKpzNPG5ZsrnqS4bvX99aeE6daAeq+GUiQPVDnaiJElSm9213t7U6weuP2to1RSHO+9zuRvj\nl8glcH9gWUS8PDN/2vF4a4CLJz4NVSfrpIh4bef9M5n5W6r7hqYtIjah6kAdThVyVlN10iZz3iT7\nHl9vL53svrPM/HtEnE91meNu3L5DtGqKjstYeHxoRGycmf+o74/6xHq+nbE5utsUx8+eYv9FwFOB\nh6/nsXsiM2+KiM9SXeZ4ENU9dGN2Be5dnZYTXw9qCUOUJElqs9X1ttf3DQHrlgQ/FHgk1aIPkz1P\n5+cDvbY+dw/gJxHxQ6p7ZM4Fzqs7TJ1eBvw31ZLjz46I73Scf2GXlyi+IiJesZ7jVwMHdAa8CSYL\nbmMds/V1/K6rt/ee5NjPphgzFog2Be5OfXlhfR/Yq6kC3/ZUC1FMfN861ecy3dFz3XOK4712ElWI\nem5EvDoz/1LvH7uUb0Wf6tAMMERJkqQ2G7vs7IG9fuCIeCXVvUgjVG/Az6F6kz92KdvhE8dk5v9G\nxI5UYer5wI71n9cCoxHxjsxc1nH+pRHxEKpFJ55NtULfLlT3F/08It6UmZ8rLH2yD9u9Ffg91aIX\n599BOPvzJPvGFn5Y3/07Y5c8zp3k2FSdws7LJOfAus/0uojqw4FvperinE11eRxUnZxd1lPHHT3X\nZItp9FxmXhIRV1Hd8/Uc4JP1Yhj7Av8APtmPOjQzDFGSJKnNLqIKKI+LiLkbcpN+RGySmX+7g3Pm\nUy0aMUK1BPY7Ou/nqd8M3y5EAdSXo70VeGtE3A94MrAf1SVxH6gvWzu+4/zrqbour64D1VOplhd/\nBPDZiLglM7888XnWY8oP252GsWCy2XrOGTs2WQibKrh07h/r1BxFFaB+TvW5XxPvzzqG9YeoO3qu\nv0xxfCacBLwP2J8qND2OqhN27sSl7tUuLiwhSZLa7Byq+5HuRrUS2npFxMbAJRFxWkSs77Ku3ag+\nE+pPwLs6A1TtPhtSXGZek5kfzsw9qFaog2qxiqnOvzIzj8vMnRhf1W3K8/to7BK59X3fY8cmu+Rv\nskv8ALartzcBf6j/e496u2ySFQLvqIYNea4b7mB8L50K/A3Yq/7MsRfW+11QouUMUZIkqbUy8ybg\nuPrLYyPi/ncw5FiqhQUey+QdkzFjV+uszsx/THL8kI7/HoHqg38jYt+IeOoUj3lavf3X+vx7RsT+\nEfGYDTm/Yavq7aPrZcb/SUTMoVp2vPPcTnvVy7ZP9Nh6+/2OoDo293+YeHJE3AN4Rv3lVPdEPWWK\n/WPP9b0pjk9LvereP6kX4TiT6nvan+qyvj/U+9RihihJktR2/0l1D9AWwAURsc/EEyJifkR8hOqD\nc2+iWlhhzcTzOlxdb7eNiH9azS0iDqa65G603jXW4diG6rOdPhkRD5jkMfertz+otztSdSpOmaIr\nNvH8xmTm5VRzfCeqlQc3HjtWh4f3UM3/VcD/TPIQWwDv6NxRf5jxWJfttI5DY3P/9AnnbwucRfUB\nyDA+7xM9NSL2njD2CcATgNuoPui3VzpfQ9tPcc5J9fZoqsUzPjPJ52ipZbwnSpIktVpm/iMing6c\nCLwI+HJEXEu1nPgfqTo5TwTuAlwPPCMzL5vq8erHvDoiVgKLqILZmcAtwO7APerHOwbYG/h4RHw9\nMw+JiHdTLQrxw4hYRXVp20ZUC188hirALa6f478i4tNUl3hdExHfoLrUbBOq+6EeQRXU3jb9WeqJ\nlwDnUy2MsFNEXEzVDXoU1T1MvwWeP8WiFcuAAyLiaVT/XzajCklbApcCH+84971UHZtnRcQlVKHp\nXsCTgC/Uj3UpsGdEnAaclplf6Ri/BDgjIi4AflKPfXpd6/GZedU052GdzPx9RFwH7ED1OrkSOLPz\ng5apVl+8nvHQt6JXz6/m2ImSJEmtl5k3Z+ZBwKOBD1KtIvd0qg9tfQzVG/dDgbijANXhQKoP1b2J\nqiv0JKrV7XbOzO8Bb6ZaCW976vt4MvNI4FlUS5Q/qH6MA4CtgY8Cj8jMzg/WPYAqnFxMFUZeQrUE\n9iZUCxI8bD1LkfdVfX/SI6k6f7dQrSb4LGAt1SIcD8vMK6YY/huqxSAuB/ah+h7/TPU9Pqlz6ffM\n/C7VJXuXUHXrXkC1GMNrgBdl5neA91Pdr/Z0qrnt9A3Gu07PB55G9Xlcr6L6AN9eOxD4EVUncieq\nDyRepw6VY/dAXVXXr5YbWbt24n2SkiRJknolIt5DtYz9KzPzQ03Xo+kzREmSJEkzJCL+heqyzn8A\n29aLoajlvJxPkiRJmgERMRf4NDAPWGqAGhwuLCFJkiT1UEQcSLWk+pOoFp24EPi/Tdak3rITJUmS\nJPXWbsDLgbsCHwKe1rl4htrPe6IkSZIkqYCdKEmSJEkqYIiSJEmSpAKDtrCE1yZKUnuNNF2AJEkb\nwk6UJEmSJBUwREmSJElSAUOUJEmSJBUYtHuiJEnqt7Wjo2uarqFx8+fPA8C5cC46ORfjnItxs2ku\n5s+f19X9uHaiJEmSJKmAIUqSJEmSChiiJEmSJKmAIUqSJEmSChiiJEmSJKmAIUqSJEmSChiiJEmS\nJKmAIUqSJEmSChiiJEmSJKmAIUqSJEmSChiiJEmSJKmAIUqSJEmSChiiJEmSJKmAIUqSJEmSChii\nJEmSJKnAnZouQJKkNlu0+MymS5jUyUcsbLoESRpYdqIkSZIkqYAhSpIkSZIKGKIkSZIkqYAhSpIk\nSZIKGKIkSZIkqYAhSpIkSZIKGKIkSZIkqYCfEyVJGgoREcBRwBOBuwM3Ap8G3pWZNzdZmySpXexE\nSZIGXkQ8FPgOsC/wKeDlwNeAtwJnRMRIg+VJklrGTpQkaRgcB8wDnp2ZX6r3fTIifg0cTRWuTm+q\nOElSu9iJkiQNtIjYFNgL+EVHgBqzFPgrcEDfC5MktZadKEnSoNsKuDNw7cQDmbkmIn4G7Nz3qmbY\n/Pnzhup5ZyPnYpxzMc65GNfmubATJUkadKvr7fwpjv8V2Doi5vSpHklSy9mJkiQNtLrbdAXwkIh4\ncGZeNXYsIh4APLz+8q7AwKzSNzq6pq/PN/Yb5X4/72zkXIxzLsY5F+Nm01x02w0zREmShsGxwGeA\nL0fEy4CfALtQLThxA7ADcEtj1UmSWsXL+SRJAy8zPwu8CtgGWAX8EjgZeB/wbeBvwE1N1SdJahdD\nlCRpKGTmB4GtgUcDOwH3yMwPA/evDudtTdYnSWoPL+eTJA2NzLwZ+O7Y1xFxT6p7ok5orChJUuvY\niZIkDbyIWBoRf6oXkuj0TmAt8NEGypIktZSdKKklVq9efccnTeLxj3988ZgrrriieMwee+xRPOYb\n3/hG8RipS18AXgl8LSKOB34HPAvYBzgyM3/cZHGSpHaxEyVJGniZeTHwVOAa4CjgROCewPMy89gm\na5MktY+dKEnSUMjM84Dzmq5DktR+dqIkSZIkqYAhSpIkSZIKGKIkSZIkqYAhSpIkSZIKuLCEJEnT\nsHLpPoyOrmm6DElSH9mJkiRJkqQChihJkiRJKmCIkiRJkqQChihJkiRJKmCIkiRJkqQCrs4ntcSR\nRx7Z1bgrr7yyeMzIyEjxmAsuuKB4zKGHHlo85g1veEPxGIAFCxZ0NU6SJGkiQ5QkSdOwaPGZTZfA\nyUcsbLoESRoqXs4nSZIkSQUMUZIkSZJUwBAlSZIkSQUMUZIkSZJUwBAlSZIkSQUMUZIkSZJUwCXO\nJUlDISIeDLwFWAhsBfwRuAh4b2Z+q8naJEntYidKkjTwIuIRwKXA3sDHgZcA7wceBZwfEYsaLE+S\n1DJ2oiRJw+CtwGbAMzPza2M7I+JLwNXAMcDKhmqTJLWMnShJ0jC4b739ZufOzPwx8Ftgh34XJElq\nL0OUJGkYXF1v79+5MyLuBmwBXNn3iiRJrTWydu3apmvopYH6ZjS4li9fXjzmsMMO6+q5RkZGuhrX\nD938/Lnwwgu7eq5dd921q3Hqqxl7sUbEQ4ALgOuBw4AfA/cE3k51n9RTM3NVN4+9aPGZjf/bs3Lp\nPk2XIElt1dW/PXaiJEkDLzOvBHYDNqa6pG8U+CGwC/CUbgOUJGk4ubCEJGngRUQAZwObAq+l6kRt\nDSwGVkbEszPzvxsscVpGR9c0XQLz588DZkctTXMuxjkX45yLcbNpLsZqKWWIkiQNg48D/wo8MDOv\nHdsZEZ8HrgFOiYgFmfn3pgqUJLWHl/NJkgZaRNwF2B24rDNAAWTmX4FVVAHr/rcfLUnS7RmiJEmD\nbi7VjcNzpjg+Z8JWkqT1MkRJkgZaZv4O+CmwY0Q8qPNYRGwJLARW4zLnkqQN5D1RkqRhsBg4A/hm\nRHwQ+AmwFXA41edEHZyZtzRYnySpRQxRkqSBl5krI+KxwJuAQ4C7A2uA7wKHZua5TdYnSWoXQ5Qk\naShk5reBZzZdhySp/bwnSpIkSZIKGKIkSZIkqYAhSpIkSZIKeE+UNE3Lly8vHnP44YfPQCWSJEnq\nB0OUJEnTsHLpPoyOrmm6DElSH3k5nyRJkiQVMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQV\nMERJkiRJUgGXOJckaRoWLT6z6RI4+YiFTZcgSUPFTpQkSZIkFTBESZIkSVIBQ5QkSZIkFTBESZIk\nSVIBF5aQOvzxj38sHvPud7+7eMytt95aPGbt2rXFYwC23nrr4jF77bVX8ZjPfe5zxWO6+Z4uvvji\n4jEAu+66a1fjJEmSJrITJUmSJEkFDFGSJEmSVMDL+SRJAy0iNuS60QWZed1M1yJJGgyGKEnSoNt3\nPcfeDdwNGO1TLZKkAWCIkiQNtMz8wmT7I+IZwP2AF2fmTf2tSpLUZt4TJUkaOhExD1gGfDMzP9F0\nPZKkdrETJUkaRkcB9wL2brqQXpg/f17TJawzm2ppmnMxzrkY51yMa/Nc2ImSJA2ViNgaOAz4ZGZe\n0XQ9kqT2sRMlSRo2bwTmAO9qupBeGR1d03QJ636jPBtqaZpzMc65GOdcjJtNc9FtN8xOlCRpaETE\n3YFDgK9k5jVN1yNJaidDlCRpmLwA2AxwMQlJUtcMUZKkYbIvcAtwTtOFSJLay3uipA6bbrpp8ZhH\nP/rRxWNuvPHG4jH77bdf8RiA5cuXF4+56qqrisecfvrpxWO6sdtuu/XleTR4IuKuwGOACzPzr03X\nI0lqLztRkqRhsSNwZ+DKpguRJLWbIUqSNCzuX2+va7IISVL7GaIkScPi7vW2+TV1JUmt5j1RkqSh\nkJnvB97fdB2SpPazEyVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAhSUkSZqGlUv3YXTUBf8k\naZjYiZIkSZKkAoYoSZIkSSpgiJIkSZKkAoYoSZIkSSrgwhJSh7lz5xaPOfXUU4vH/PrXvy4es2DB\nguIx3TrllFP68jybbbZZ8ZhtttlmBiqRJEnacHaiJEmSJKmAnShJkqZh0eIzmy6Bk49Y2HQJkjRU\n7ERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVcIlzSdJQiIinAUcA\nOwG3At8H3pmZ32i0MElS69iJkiQNvIh4CXB2/eXhwBLgPsC5EbFnQ2VJklrKTpQkaaBFxD2ADwD/\nAzwlM2+r968ELgaeDqxqrEBJUusYoiRJg+5FwF2AJWMBCiAzfwZs01hVkqTWMkRJ0zR37tziMQsW\nLCgec8sttxSPATjggAOKx3z1q1/t6rlKffOb3ywe083caeg9CVhD1XUiIjYG7pSZ3f2lkiQNPUOU\nJGnQPQD4X+DhEfF+YHdg44i4imphic82Wl0PzJ8/r+kS1plNtTTNuRjnXIxzLsa1eS5cWEKSNOi2\nBLYAvgpcCDwDeFW97zMR8dIGa5MktZCdKEnSoNsE2AF4YWaeNrYzIr4KXA0cGxErMvMfDdU3baOj\na5ouYd1vlGdDLU1zLsY5F+Oci3GzaS667YbZiZIkDbo/AzcD/3TZXmZeC5wHbA08sIG6JEktZYiS\nJA2665j637vf1tvN+1OKJGkQGKIkSYPuYqpL+h40ybHt6+0N/StHktR2hihJ0qBbUW+PjoiRsZ0R\nsSPwOOCHmfmLJgqTJLWTC0tIkgZaZl4SEcuoVuQ7KyJOp+pAvRa4FTi8yfokSe1jiJIkDYPDgR8B\nBwMfBW6hWu58SWZ+p8nCJEntY4iSJA28zFwLLK//SJI0Ld4TJUmSJEkFDFGSJEmSVMDL+aQB98tf\n/rJ4zM033zwDldzewx/+8L48jyRJUi/ZiZIkSZKkAnaiJEmahpVL92F0dE3TZUiS+shOlCRJkiQV\nMERJkiRJUgFDlCRJkiQVMERJkiRJUgFDlCRJkiQVMERJkiRJUgGXOJckaRoWLT6z6RI4+YiFTZcg\nSUPFTpQkSZIkFTBESZIkSVIBQ5QkSZIkFfCeKKklbrvttq7G/epXvyoeMzIyUjxmu+22Kx4jSZLU\nRnaiJEmSJKmAIUqSJEmSChiiJEmSJKmA90RJkgZeRKwAXrSeU16bmcf3qRxJUssZoiRJw+RQYHSS\n/T/odyGSpPYyREmShsk5mXld00VIktrNe6IkSZIkqYCdKEnS0ImIOcCtmXlr07X0wvz585ouYZ3Z\nVEvTnItxzsU452Jcm+fCTpQkaZgcFhHXAn8FbomIb0fE3k0XJUlqFztRkqRh8hTgWOBGYEfgDcBX\nIuIFmfnZRiubhtHRNU2XsO43yrOhlqY5F+Oci3HOxbjZNBfddsMMUZKkYbAU+AywKjNvqfedHRFn\nUa3MtzQiTs/M2xqrUJLUGoYoSdLAy8wrgCsm2f+jiFgFPAl4IHBVn0uTJLWQIUoD68YbbyweM29e\neUt38803Lx7TjWOOOaarcT//+c97XMnkli1b1pfnkWbAb+ptf/4yS5JazxAlSRpoEbE5sAj4fWae\nO9kp9fb6/lUlSWozV+eTJA26vwEfAlZExFadByLiicCjgUsz84YmipMktY+dKEnSQMvMmyPicGAF\ncGlELAd+DTwCOAT4E/CK5iqUJLWNnShJ0sDLzE8AC4FrgLcAJwHPAT4N7JSZP2iwPElSy9iJkiQN\nhcw8Dziv6TokSe1nJ0qSJEmSChiiJEmSJKmAIUqSJEmSChiiJEmSJKmAC0tIkjQNK5fuw+jomqbL\nkCT1kZ0oSZIkSSpgiJIkSZKkAl7Op1Y455xzise84AUvKB4zZ86c4jEvf/nLi8ccc8wxxWMuueSS\n4jHd2nHHHYvHPPGJT5yBSiRJkmYfO1GSJEmSVMAQJUmSJEkFDFGSJEmSVMB7oiRJmoZFi89suoR/\ncvIRC5suQZIGnp0oSZIkSSpgiJIkSZKkAoYoSZIkSSpgiJIkSZKkAoYoSZIkSSpgiJIkSZKkAoYo\nSdLQiYhjImJtRKxouhZJUvsYoiRJQyUiHgy8qek6JEntZYiSJA2NiNgI+BhwVdO1SJLa605NF6D2\nuvXWW4vHHHXUUV0913ve857iMWvXri0es3r16uIxZ511VvGYq64qf/+2atWq4jEAIyMjxWOOPvro\n4jFz5swpHiM14BBgN+AJwNcbrkWS1FJ2oiRJQyEitgXeDXwqM7/RdD2SpPayEyVJGhYfAv4OvK7p\nQmbS/Pnzhvr5ZxPnYpxzMc65GNfmuTBESZIGXkQ8B/h34KWZOdp0PZKkdjNESZIGWkRsASwDzgdO\nabicGTc6uqaR5x37jXJTzz+bOBfjnItxzsW42TQX3XbDvCdKkjTo3gtsCRycmeUrzkiSNIGdKEnS\nwIqIxwMvBU4A/lwvLtFps3rfTZn5h74XKElqJTtRkqRBthAYAV4DXD/hD8C+9X+/v5HqJEmtZCdK\nkjTITgO+O8WxlVSfFXU846FKkqQ7ZIiSJA2szPwJ8JPJjkUEwA2Z+ZW+FiVJaj0v55MkSZKkAnai\nJElDKTNHmq5BktROdqIkSZIkqYCdKHXtxBNPLB7z3ve+t6vnGhnpzy+Mu3meK6+8si9jup2DZz3r\nWcVjnvzkJ3f1XJIkScPATpQkSZIkFTBESZIkSVIBL+eTJGkaVi7dh9HRNU2XIUnqIztRkiRJklTA\nECVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBVziXJKkaVi0+MymS+DkIxY2XYIk\nDRU7UZIkSZJUwBAlSZIkSQW8nE8AXHPNNcVjjj322BmoZHLLli0rHrPzzjsXj9lll12Kx/TL5ptv\n3tW4E044oXjM3Llzu3ou9c/VV19dPOZ+97tfV8915zvfuatxkiQNKjtRkiRJklTAECVJkiRJBQxR\nkiRJklTAECVJkiRJBVxYQpI0FCLiocAbgccC9wJWAxcBx2bmJU3WJklqFztRkqSBFxG7Ad8GFgIf\nA15Wb/cCvhkRj2mwPElSy9iJkiQNg+XACLB7Zl43tjMiLgXOAN4E7NNMaZKktjFESZIGWkRsBHwC\nWNMZoGr/XW/v3deiJEmtZoiSJA20zLwNeN8Uhx9Qb3/Yp3IkSQPAECVJGioRsQVwV6oFJo4DrgWW\nNFnTdM2fP6/pEtaZTbU0zbkY51yMcy7GtXkuDFGSpGHzh3q7FjgFeGNm/r7BeiRJLWOIkiQNm72A\nuwCPAA4FFkbEvpn53WbL6t7o6JqmS1j3G+XZUEvTnItxzsU452LcbJqLbrthhigB8IUvfKF4zOjo\naPGYHXfcsXgMwEEHHVQ85hWveEVXzzVbrV69uqtxj3zkI4vHLF68uHjMtttuWzxm9913Lx6z3Xbb\nFY/p1vXXX1885lvf+lbxmBtuuKF4zJIlS4rH7LvvvsVjAFasWNHVuNkqM1fV//nViPgUcBlwWkQ8\noL5/SpKk9fJzoiRJQ6tere/rwL8B9222GklSW9iJkiQNtIh4IPA14L8z8yWTnLJFvfXfREnSBrET\nJUkadD8F5gD7RsSCzgMRcV9gd2AU+EkDtUmSWsjfukmSBlpm3hoRrwI+DVwSER8CfgYsAF4JzAUO\ny8x/NFimJKlFDFGSpIGXmZ+NiJ8Db6IKTlsAq4HvAO/LzK81WZ8kqV0MUZKkoZCZFwPPaLoOSVL7\neU+UJEmSJBUwREmSJElSAUPJWzPzAAAQkklEQVSUJEmSJBUwREmSJElSAReWkCRpGlYu3YfR0TVN\nlyFJ6iM7UZIkSZJUwE6UAHjzm99cPGajjcoz+OWXX148BmDvvfcuHrNq1ariMSMjI8VjNttss+Ix\nS5cuLR5z8MEHF48B+O1vf1s85k1velNXz1Vq7dq1xWO6+X/UT/36nrp53W288cbFYyRJ0u3ZiZIk\nSZKkAoYoSZIkSSpgiJIkSZKkAoYoSZIkSSrgwhKSJE3DosVnNl0CJx+xsOkSJGmo2ImSJEmSpAKG\nKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkq4BLnkqSBFxHzgbcBzwS2Af4IfAt4\nR2Ze1mRtkqT2sRMlSRpoEbE1cBnwUuBz9fYjwBOAb0XEIxosT5LUQnaiBMBGG5Xn6ZGRkRmoZHIX\nXHBB8Zhu6nvYwx5WPGb58uXFY3beeefiMfe5z32KxwAccMABxWNGR0e7eq5+6Ofrrl9m8+tuQLwT\n2BZ4dmZ+aWxnRHwH+DLwZuC5DdUmSWohO1GSpEH3S+AzwBkT9p8LrAV27HtFkqRWsxMlSRpomblk\nikPzgBFgdf+qkSQNAkOUJGlYHVxvP91oFT0wf/68pktYZzbV0jTnYpxzMc65GNfmufByPknS0ImI\np1Gt1vc94MSGy5EktYydKEnSUImIA4GPA9cBizLzb81WNH2jo2uaLmHdb5RnQy1Ncy7GORfjnItx\ns2kuuu2G2YmSJA2NiDgK+ARwOfDYzPxVwyVJklrITpQkaShExPHA4cBZwPMz8y8NlyRJaik7UZKk\ngVd3oA4HTgGeZYCSJE2HnShJ0kCLiL2At1N9TtTLMvO2hkuSJLWcIUqSNOiOq7f/AzwrIiY752y7\nU5KkDWWIkiQNup3q7YfWc84CqtX6JEm6Q4YoSdJAy8yRpmuQJA0WQ5QAWLZsWfGYV7/61TNQSe/s\nvvvuxWMWL15cPGbnnXcuHtONJz7xiV2Nu+KKK4rHrFnTn89tuOCCC4rHjIx09374kksu6Wpcqde/\n/vXFY+5xj3sUj5k7d27xGEmS1BuuzidJkiRJBQxRkiRJklTAECVJkiRJBbwnSpKkaVi5dB9GR/tz\nH6EkaXawEyVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAJc4lSZqG\nRYvPbLoETj5iYdMlSNJQsRMlSZIkSQXsRAmAgw8+uHjMpZdeWjzmU5/6VPEYgDe/+c3FY9761rcW\nj9l0002Lx8x2W221VV/GdGPBggV9eR6AAw88sG/PJUmSBpudKEmSJEkqYIiSJEmSpAKGKEmSJEkq\nYIiSJEmSpAKGKEmSJEkq4Op8kqShERGbAO8EXg9ckJl7NluRJKmNDFGSpKEQEQGcBtwfGGm4HElS\ni3k5nyRp4EXE3YHLgI2BRzVcjiSp5QxRkqRhsAnwSWDXzMymi5EktZuX80mSBl5m/gY4pOk6JEmD\nwRAlSVLLzZ8/r+kS1plNtTTNuRjnXIxzLsa1eS68nE+SJEmSCtiJEgAbbVSep1esWNGXMZKk9Rsd\nXdN0Cet+ozwbammaczHOuRjnXIybTXPRbTfMTpQkSZIkFTBESZIkSVIBQ5QkSZIkFTBESZIkSVIB\nF5aQJA28iHgQ8KAJu+dHxHM6vj47M//Sx7IkSS1liJIkDYPnAkdP2Pcg4PMdXy8ArutXQZKk9jJE\nSZIGXmYuAZY0XIYkaUB4T5QkSZIkFTBESZIkSVIBQ5QkSZIkFTBESZIkSVIBF5aQJGkaVi7dh9HR\nNU2XIUnqIztRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBVziXJKk\naVi0+MymS+DkIxY2XYIkDRU7UZIkSZJUwBAlSZIkSQUMUZIkSZJUwBAlSZIkSQUMUZIkSZJUwBAl\nSZIkSQVc4lySNBQiYkvgaOAZwD2B3wFnA0dl5q+arE2S1C52oiRJAy8i5gKrgEOALwIvBj4C7Adc\nGBF3b6w4SVLr2ImSJA2D1wAPBQ7LzA+P7YyIy4EzgKOA1zVUmySpZexESZKGwYHATcBJE/afCdwA\n7B8RI32vSpLUSoYoSdJAi4jNgQcAl2XmLZ3HMnMtcCkwH1jQQHmSpBbycj5J0qDbvt7eMMXxX9Tb\n+wA/m/lyem/+/HlNl7DObKqlac7FOOdinHMxrs1zYSdKkjToxv6V/ssUx2+acJ4kSetlJ0qSpJYb\nHV3TdAnrfqM8G2ppmnMxzrkY51yMm01z0W03zE6UJGnQra63d5ni+F0nnCdJ0noZoiRJg+5aYC2w\n7RTHx+6Z+ml/ypEktZ0hSpI00DLzJuCHwE4RMafzWERsDDwGuD4zfzHZeEmSJjJESZKGwUnAZsAr\nJuzfH9ga+HjfK5IktZYLS0iShsFy4IXAcRGxPfBd4MHA64ArgOMarE2S1DJ2oiRJAy8z/w48GVgG\nPBtYAbyIqgO1Z2ZOtfy5JEm3YydKkjQUMnM1VefpdU3XIklqNztRkiRJklTAECVJkiRJBQxRkiRJ\nklTAECVJkiRJBVxYQpKkaVi5dB9GR9c0XYYkqY/sREmSJElSAUOUJEmSJBUwREmSJElSAUOUJEmS\nJBUwREmSJElSAUOUJEmSJBUwREmSJElSAUOUJEmSJBUwREmSJElSAUOUJEmSJBUwREmSJElSAUOU\nJEmSJBUwREmSJElSAUOUJEmSJBUwREmSJElSAUOUJEmSJBUYWbt2bdM19NJAfTOSNGRGmi5AkqQN\nYSdKkiRJkgoYoiRJkiSpgCFKkiRJkgrcqekCeszr6SVJkiTNKDtRkiRJklTAECVJkiRJBQxRkiRJ\nklTAECVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBQxRkiRJklTAECVJkiRJBQxR\nkiRJklTgTk0XIEnSbBMRWwJHA88A7gn8DjgbOCozf7UB4x8DHAXsCswFfgJ8DPhgZq6dqbpnQg/m\n4rH1+J2BOcD1wBeBd2Tmn2eq7pkw3bmY8FhzgMuB+wN7Zeaq3lY7s3rwutgUOALYH9iuHv9V4MjM\n/N1M1T0TejAX+wMHAw8DNgF+AXwFeGdm/n6m6p4pEbEJ8E7g9cAFmblnwdjW/Oy0EyVJUoeImAus\nAg6herP/YuAjwH7AhRFx9zsYvxA4D/g3YAnwcqo3Ah8A3j9DZc+IHszFC4FvUr1JPrp+nB8CbwS+\nFhGteR8y3bmYxFFUAap1evC6uBNVYHorVVh4GfAF4KXA+fWb8FbowVwcC5wK3Bl4C1WYWgW8Cvh2\nRGw+Q6XPiIgI4GKq+RgpHNuqn512oiRJ+mevAR4KHJaZHx7bGRGXA2dQvfl93XrGfxi4GXhcx2+h\nT42ILwOvjohTMvPymSm957qei7rTcCJV52mXzPxTfejkiDiD6rf2T6X6jX0bTPd1sU5EPBR4A/B9\n4BG9L3XGTXcuDgaeALwoMz9Z7/tURPwOeAmwC1X4boPp/B3Zkup1cB3w+My8pT50Sj0XRwAHASfM\nWPU9VAfGy4CfAo8Cflz4EK362dma3wBJktQnBwI3ASdN2H8mcAOwf0RM+hvWiNgFCOD0SS7j+SDV\nb2b37225M6rruQDuAXwJeHdHgBozFpx27FWhfTCduVin7r59DPg5VceijaY7F4dRvdE+tXNnZr4z\nM++TmW0JUDC9ubg3VUPj0o4ANeaCertDj+rsh02ATwK7ZmaWDGzjz05DlCRJtfrSmQcAl018U1Nf\nj38pMB9YMMVD7FxvL57k2CX1dpcelDrjpjsXmfnzzHxxZp44yeG71dvVPSx5xvTgddHplVSvgYOB\niW+cZ73pzkVEbFuP/9rYPS4RMWdDAuhs04PXxbVUr4F/m+TYDvX2yp4U2weZ+ZvMPCQzb+5ieOt+\ndhqiJEkat329vWGK47+ot/eZ4vgOU43PzDXAH9czdraZ7lxMqr7f5SXAX4Avd1da3/VkLiJiO+Bd\nwKmZ+fUe1dZv052LB9Tb/42IwyPiOuCvwF8j4ssRcb+eVNkf05qLukP7DuAREbEsIu4bEVtHxP8B\njgR+AHy6lwXPYjvU29b87DRESZI0bl69/csUx2+acF4346caO9tMdy5up+NStgdSrVz2y+7L66te\nzcWJwN+Axb0oqiHTnYst6+2LgFdQhcp9qC5tXES1GMM9e1BnP0z7dZGZ76JaQOFlwDXAb4CVVPfL\nLeyyq9NGrfvZ6cISkiRpxtWrmJ1GtaDEhzLzfQ2X1FcR8Tzg6cBLMnO06XoaNLby3jbAQzqW8D4r\nIn5DFaoWUy2PPfAi4hCq1ee+BnwGGKW6bO2NwNkR8bTM/GODJWoKhihJksaN3aNzlymO33XCed2M\nb8V9QEx/LtaJiPnAWVSf/fKOzHzb9Mvrq2nNRb0K2wnA+Zl5So9r67fpvi7GPhvsrEk+A+kkqhC1\nZ9fV9dd0XxdBFaC+nplP7zj0X/Xqfl+mWvb8jT2odbZr3c9OQ5QkSeOuBdYC205xfOweiJ9Ocfxn\n9fZ24yPiblQLKlw2nQL7aLpzAUBEbEO1XPUC4KDMXNGrAvtounPxXmALYEm9sMKYsc8Qml/vH51k\nlbbZZrpzcV293XiSY7+rH7stn4003blYSPVe/EuTHDunfuy9plNgi7TuZ6f3REmSVMvMm6g+DHan\niJjTeSwiNgYeA1yfmb+YbDxwUb3dfZJjj6u33+pFrTOtB3MxtnrZuVRLOf97SwNUL+biCVSXsZ1H\n9blZY3/GLmk8vf56t95X31s9mIsfAX8CHj7Jse2olrKeaqGGWaUHczHWdZkzybFNqeZismODqHU/\nOw1RkiT9s5OAzahueu+0P7A18PGxHRHxgIhYt3xxZv6A6rel+3Z2HOrlm18L/B34xMyV3nNdz0Xt\nBKo3y8/PzHNmstA+mM5cvIRq0YSJf46vj7+l/vqKGam896bzd+RvVPfGPTIiFk0Y/8p6u7LnFc+c\n6bwuxoLDfpMs8b7vhHMGyiD87PRyPkmS/tly4IXAcRGxPfBd4MHA66je5B7Xce7VQDK+bDPAoVQd\nhwsi4niqpXmfR3XpzlGZ+b8z/h30TtdzERE7Uq3A9iNg44h4ziSPP5qZ589c+T3V9Vxk5jcme8CI\n2Kr+z4szc9XMlD0jpvt35GjgKcDnI+I/qS7xWwgcQLWs9/IZrr+XpvO6uCgiPk8VmL4VEadTLSzx\naKoPJB5baKMVIuJBwIMm7J4/4e/+2Zn5FwbgZ6edKEmSOmTm34EnA8uAZwMrqMLAx4E96zcA6xt/\nCfB44MfAMVRLN9+DalW2d85c5b03zbnYiepypAcBn5/iz9tnqvZem+7rYpD04O/IKNUiI58A/gP4\nKLAH1eWNe2bmX2es+B7rwevi+cCrqC7fe1c9/pnAycAj13e57Cz0XP757zfc/u//1lMNbtvPzpG1\na9c2XYMkSZIktYadKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKG\nKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmS\npAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAKGKEmSJEkqYIiSJEmSpAL/H9TVOIMLzPO8\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 424,
              "height": 227
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d137ebac2a3367c41e6c001f2edf2a37b0b22211",
        "id": "kiZg8nxE6zR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Access Layers of the network\n",
        "We can access layers  by integer "
      ]
    },
    {
      "metadata": {
        "_uuid": "0cf54881f8e022928a962d1d83fd5614a97a039e",
        "id": "7_69eLUk6zR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "80583288-a156-4560-bc6b-a6ae1059c1bb"
      },
      "cell_type": "code",
      "source": [
        "#Anand (down): model[0] is the first layer output. \n",
        "print(model[0])\n",
        "model[0].weight"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=128, bias=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0206,  0.0214, -0.0124,  ...,  0.0244, -0.0306,  0.0187],\n",
              "        [-0.0165, -0.0134,  0.0105,  ...,  0.0070,  0.0239,  0.0059],\n",
              "        [ 0.0311, -0.0173, -0.0168,  ...,  0.0215, -0.0004, -0.0254],\n",
              "        ...,\n",
              "        [-0.0282,  0.0107,  0.0204,  ..., -0.0003, -0.0239,  0.0140],\n",
              "        [-0.0207, -0.0128,  0.0327,  ...,  0.0155, -0.0293,  0.0346],\n",
              "        [-0.0067, -0.0233, -0.0084,  ...,  0.0226,  0.0266,  0.0131]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0499ff7a787fc1885f5998f81f2395fc6bcc4108",
        "id": "ipG_4qdk6zSE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ordered Dict- Better way to create a network\n",
        "We can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
      ]
    },
    {
      "metadata": {
        "_uuid": "ca7567562b0610e6ec4ec7d98f03032bbe32c061",
        "id": "EeoqltNA6zSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "77e7e3cb-73e8-45be-fda7-988dda55b65d"
      },
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "model = nn.Sequential(OrderedDict([\n",
        "                      ('hidden', nn.Linear(input_size, hidden_sizes[0])),\n",
        "                      ('relu1', nn.ReLU()),\n",
        "                      ('output', nn.Linear(hidden_sizes[0], output_size)),\n",
        "                      ('softmax', nn.Softmax(dim=1))]))\n",
        "model"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c67ffda9a2ace0093371017d9abf4bc02a8656dc",
        "id": "jnkFZfof6zSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Access Layers using integer or name \n",
        "Now we can access layers  either by integer or name"
      ]
    },
    {
      "metadata": {
        "_uuid": "6da7947d84c734eea2bf19acc3127bdf87ccce1c",
        "id": "K7aKA92k6zSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "074777df-4a96-44dd-f0eb-ec60ae0f052c"
      },
      "cell_type": "code",
      "source": [
        "print(model[0])\n",
        "print(model.hidden)\n",
        "print(model.hidden.weight)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=128, bias=True)\n",
            "Linear(in_features=784, out_features=128, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[-1.6818e-03, -9.4320e-03, -2.9041e-02,  ..., -8.4733e-03,\n",
            "         -1.3702e-02,  2.4871e-02],\n",
            "        [-6.4225e-03,  1.1425e-02, -6.6743e-04,  ...,  1.9467e-02,\n",
            "         -3.5642e-02, -1.2637e-02],\n",
            "        [ 2.7824e-02,  2.0677e-02,  1.3983e-02,  ...,  9.2808e-03,\n",
            "          1.8387e-02, -1.2967e-02],\n",
            "        ...,\n",
            "        [-1.6838e-02, -2.7560e-02,  1.1889e-02,  ...,  1.6769e-03,\n",
            "         -2.5749e-02,  3.3788e-02],\n",
            "        [-9.2459e-03, -2.0910e-05,  9.9962e-03,  ...,  1.4970e-02,\n",
            "         -1.9747e-02, -3.4973e-02],\n",
            "        [-2.1697e-02,  2.7523e-02, -4.4115e-03,  ..., -1.6222e-02,\n",
            "         -2.2439e-02,  1.3916e-02]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "df59c9d06c7e7d2ec2b6582c6bb0934e2d49c58f",
        "id": "iwaA0jFF6zSP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Recollect everything \n",
        "Before we go ahead and train a neural network to accuractly predict the numbers appearing in the MNIST images,let us recollect the important modules that is necessary for any model training exercise"
      ]
    },
    {
      "metadata": {
        "_uuid": "f0df7ddfb84bf00cee50f9b916a9c61f1a65aaa9",
        "id": "K_Ly0jQg6zSQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:03:17.978997Z",
          "start_time": "2019-02-04T13:03:17.973595Z"
        },
        "_uuid": "ff53d2bd4296f89b134eb961832043cce0e43e4d",
        "id": "klonhsi26zS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets,transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a1f47e2928ea88a7c9b45edb08ea75016b6fca6",
        "id": "hD6Ekj6r6zTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load Data"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:07:13.687456Z",
          "start_time": "2019-02-04T13:06:40.910295Z"
        },
        "_uuid": "c2c9d59a82c6bce28a611e1f409ced8b4f8fdc39",
        "id": "xfh2cpwN6zTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "#Anand (down): We are using batch size of 64. We choose batch size based on GPU and meory size. If very powerful GPU, we can even load the entire dataset at once.\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7161f89857d013b0f837c620c0ed24657bed1b2",
        "id": "TzzD1_XI6zTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build a feedforward Network"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:03:47.741595Z",
          "start_time": "2019-02-04T13:03:47.735142Z"
        },
        "_uuid": "0e73fc284a626e4d7bbca629f817585aeb06fdd6",
        "id": "ucMMUuuU6zTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Build a feed-forward network in one of the three ways mentioned above:\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dfdb486a01347095c7c8d75de1d5b74443bbfd4f",
        "id": "RKbA0xQo6zTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Lets run one image through the network to check our work"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:03:49.422306Z",
          "start_time": "2019-02-04T13:03:49.121144Z"
        },
        "_uuid": "0864af9a75655db17435ef4940945a7d9532c671",
        "id": "QDMUybnc6zTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fd5eeed-d263-42a3-fd1d-b78cc8a14bd8"
      },
      "cell_type": "code",
      "source": [
        "# Get our data\n",
        "images, labels = next(iter(trainloader))\n",
        "# Flatten images\n",
        "#Anand (down): .view in PyTorch does a reshape operation to whatever shape we want. Usually, we use -1 because we don't know the exact ending dimension.\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "# Forward pass, get our logits\n",
        "#Anand (down): logits can be considered as the output of the model....ie. the calculated values\n",
        "logits = model(images)\n",
        "print(logits.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0d9976b19b6ca3910a3a4855754bdc1e3de1f52f",
        "id": "XAaW3rv16zTn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define a loss function"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:03:52.643210Z",
          "start_time": "2019-02-04T13:03:52.638129Z"
        },
        "_uuid": "22466cb3a58aad481e903428de2642ff67e9bc1f",
        "id": "ULq66mlW6zTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Anand (down): NLL is Negative Log Le..will be covered in upcoming sessions...In PyToch criterion is the name for loss fn\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "#Anand (note): Instead of .NLLLoss(), we can also use CrossEntropy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:03:55.680366Z",
          "start_time": "2019-02-04T13:03:55.547651Z"
        },
        "_uuid": "2e3bda9d30daf829a8e19537715860ff7c678329",
        "id": "OmtnX_Mg6zT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da62baca-7dd2-4a56-b096-c66f1bad4caf"
      },
      "cell_type": "code",
      "source": [
        "# Calculate the loss with the logits and the labels\n",
        "loss=criterion(logits,labels)\n",
        "print(loss)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3122, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8a7dd6c45e94460f48dd048b35229f74a399a988",
        "id": "o3vxLyb46zT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Autograd\n",
        "\n",
        "Now that we know how to calculate a loss, how do we use it to perform backpropagation? Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way.\n",
        "\n",
        "PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "d3361f46b9d9dbf06bc4a6b257f472129c2b01e1",
        "id": "NcPX8UtN6zUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see an example to understand it better.Then again we will head back to our modelling task"
      ]
    },
    {
      "metadata": {
        "_uuid": "5ab21186791f7e319778106ca1ffb327d0847e16",
        "id": "66DXk12k6zUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "439dcc5d-e99c-4573-b19c-96e6f62a09a0"
      },
      "cell_type": "code",
      "source": [
        "#Anand (down): Backpropagation is done below for an example functionn of y=x^2\n",
        "x = torch.randn(2,2, requires_grad=True)\n",
        "print(x)\n",
        "y = x**2\n",
        "print(y)\n",
        "## grad_fn shows the function that generated this variable\n",
        "print(y.grad_fn)\n",
        "z = y.mean()\n",
        "print(z)\n",
        "#Anand (down): Below, if we do x.grad before doing backpropagation .ie. differentiation, we get None (remember IBM DL class)\n",
        "print(x.grad)\n",
        "z.backward()\n",
        "#Anand (down): Below, if we do x.grad after doing backpropagation .ie. differentiation, we get the differentiated value\n",
        "print(x.grad)\n",
        "print(x/2)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.4558, -1.4075],\n",
            "        [ 0.0460, -1.7109]], requires_grad=True)\n",
            "tensor([[2.1193e+00, 1.9810e+00],\n",
            "        [2.1144e-03, 2.9271e+00]], grad_fn=<PowBackward0>)\n",
            "<PowBackward0 object at 0x7fc66bb9fb70>\n",
            "tensor(1.7574, grad_fn=<MeanBackward1>)\n",
            "None\n",
            "tensor([[-0.7279, -0.7037],\n",
            "        [ 0.0230, -0.8554]])\n",
            "tensor([[-0.7279, -0.7037],\n",
            "        [ 0.0230, -0.8554]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "94d46a1835ad4987ff28365b28ee304d366336dc",
        "id": "rE1opnZf6zUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss and Autograd together"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:04:00.684688Z",
          "start_time": "2019-02-04T13:04:00.535643Z"
        },
        "_uuid": "b0c5e229539704b93a5991c27849bd7b41abae80",
        "id": "AlliFqpe6zUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1f9d5e97-4df0-45a4-f468-20b5a1e14cad"
      },
      "cell_type": "code",
      "source": [
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "#Anand (down): logits is the log of the output. Can also be renamed output for our understanding\n",
        "logits = model(images)\n",
        "#Anand (down): loss will give one single loss value\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "\n",
        "print('Before backward pass: \\n', model[0].weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('After backward pass: \\n', model[0].weight.grad)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before backward pass: \n",
            " None\n",
            "After backward pass: \n",
            " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d184b346f9ed4194de74a29675da09076e52e0c4",
        "id": "jtHTkkvU6zUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the optimizer"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:04:04.271653Z",
          "start_time": "2019-02-04T13:04:04.266694Z"
        },
        "_uuid": "efadf9e312ec51f9f36d25f043d011c27f4f8fe2",
        "id": "16i9lWtA6zUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91c0777ba686873bd972d96e384c090c9baae3d8",
        "id": "bWyGca5c6zUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training for real"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:05:20.593146Z",
          "start_time": "2019-02-04T13:04:05.998666Z"
        },
        "_uuid": "6b6ac5071aad8b3232596ed1d4807d73aab09df3",
        "id": "k9PUV9ri6zUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ba76626a-338d-4648-f1ae-081a00a46461"
      },
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        output=model.forward(images)\n",
        "        # TODO: Training pass\n",
        "        \n",
        "        loss = criterion(output,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #Anand (down): zero_grad means we are not allowing the model to accumulate the gradients. ie. next time it is run, we don't want to do a second order differenation. So, after every pass, it resets the old gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Anand (down): For every iteration where the dataset (here: every batch of 64 images) is getting exhausted, it is calculating the running_loss\n",
        "        #Anand (down): loss.item() will give the value of loss. .shape[0] will give the loss for number of images (for eg, one batch could have 64 images, other batch could have only 10 images. So we are scaling that loss by number of imaes in that batch)\n",
        "        running_loss += loss.item()* images.shape[0]\n",
        "    else:\n",
        "        print(f\"Epoch:{e} Training loss: {running_loss/len(trainloader.dataset)}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0 Training loss: 1.1782748711585997\n",
            "Epoch:1 Training loss: 1.517994990348816\n",
            "Epoch:2 Training loss: 1.6252919858296713\n",
            "Epoch:3 Training loss: 1.9030922147750855\n",
            "Epoch:4 Training loss: 1.871261965751648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:06:05.063846Z",
          "start_time": "2019-02-04T13:06:04.983101Z"
        },
        "_uuid": "6d837662f725dc7196ba3d24130aa23090dc1616",
        "id": "Bm_fuwOh6zUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "#import helper\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logits = model.forward(img)\n",
        "\n",
        "# Output of the network are logits, need to take softmax for probabilities\n",
        "ps = F.softmax(logits, dim=1)\n",
        "#helper.view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "37a44994683bfe1fee884125d2e58ed08b62720b",
        "id": "_SjknpjQ6zUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference and Validation\n",
        "\n",
        "The goal of validation is to measure the model's performance on data that isn't part of the training set. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are precision and recall and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
      ]
    },
    {
      "metadata": {
        "_uuid": "e94326ca7cdc742c2c575294d260c257a97cab87",
        "id": "tv0MhuPH6zUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference on a batch of images\n",
        "Let us try to do this for a batch of images.Before that we will make some changes in our architecture"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:08:55.536357Z",
          "start_time": "2019-02-04T13:08:55.509217Z"
        },
        "_uuid": "e4ec78de08b2f24f5032ba181ec7a7d79428261a",
        "id": "U0Bn2Jnj6zUx",
        "colab_type": "code",
        "colab": {},
        "outputId": "87ce5648-84df-4b02-9dee-1ff6277fd5b7"
      },
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(testloader))\n",
        "images.shape,labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:09:03.958533Z",
          "start_time": "2019-02-04T13:09:03.928394Z"
        },
        "_uuid": "3f2244405b587c87c9347abd1ed4ab150f8d80aa",
        "id": "dCzx1yqO6zU4",
        "colab_type": "code",
        "colab": {},
        "outputId": "671677c4-ea7e-431d-9717-1400e20fe4f4"
      },
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(testloader))\n",
        "img = images.view(images.shape[0], 784)\n",
        "# Get the class probabilities\n",
        "ps = torch.exp(model(img))\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:09:06.432988Z",
          "start_time": "2019-02-04T13:09:06.341035Z"
        },
        "_uuid": "72a2ebdd77e3380b972e24e1565bc7744beedd27",
        "id": "2wDCCUob6zU9",
        "colab_type": "code",
        "colab": {},
        "outputId": "e59a49fa-4803-4650-f095-a520448f8e6f"
      },
      "cell_type": "code",
      "source": [
        "top_prob,top_class=ps.topk(1,dim=1)\n",
        "top_prob.shape,top_class.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1]), torch.Size([64, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:10:21.313112Z",
          "start_time": "2019-02-04T13:10:21.305216Z"
        },
        "_uuid": "9f6368720bf311e1ee9977f0f2562a02b1afc12f",
        "id": "fVSiVeWD6zVA",
        "colab_type": "code",
        "colab": {},
        "outputId": "29211cc4-2939-4ff9-98b7-cd5633e1be7b"
      },
      "cell_type": "code",
      "source": [
        "top_class.view(64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8, 1, 1, 1, 1, 8, 1, 1, 1, 8, 1, 1, 8, 1, 1, 1, 8, 1, 8, 8, 2, 8, 1, 2,\n",
              "        8, 1, 8, 8, 8, 1, 1, 8, 8, 8, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 8, 1,\n",
              "        2, 1, 1, 8, 1, 1, 8, 8, 8, 1, 1, 8, 1, 1, 8, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a9a05d7571a12641d0da644ae00b1608346faf6a",
        "scrolled": true,
        "id": "G7QFR-G26zVK",
        "colab_type": "code",
        "colab": {},
        "outputId": "8033aa4e-8298-47f9-adc1-ffc511cceb6f"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame({\"Predicted\":top_class.view(top_class.shape[0]),\"Actual\":labels})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Predicted  Actual\n",
              "0           1       7\n",
              "1           1       0\n",
              "2           1       4\n",
              "3           1       4\n",
              "4           1       4\n",
              "5           1       5\n",
              "6           1       3\n",
              "7           1       3\n",
              "8           1       3\n",
              "9           1       3\n",
              "10          1       9\n",
              "11          1       3\n",
              "12          1       4\n",
              "13          1       7\n",
              "14          1       8\n",
              "15          1       0\n",
              "16          1       7\n",
              "17          1       3\n",
              "18          1       3\n",
              "19          1       9\n",
              "20          1       8\n",
              "21          1       2\n",
              "22          1       1\n",
              "23          1       1\n",
              "24          1       4\n",
              "25          1       2\n",
              "26          1       8\n",
              "27          1       6\n",
              "28          1       5\n",
              "29          1       6\n",
              "..        ...     ...\n",
              "34          1       3\n",
              "35          1       0\n",
              "36          1       4\n",
              "37          1       7\n",
              "38          1       6\n",
              "39          1       6\n",
              "40          1       8\n",
              "41          1       6\n",
              "42          1       6\n",
              "43          1       5\n",
              "44          1       5\n",
              "45          1       3\n",
              "46          1       9\n",
              "47          1       3\n",
              "48          1       9\n",
              "49          1       1\n",
              "50          1       2\n",
              "51          1       3\n",
              "52          1       0\n",
              "53          1       5\n",
              "54          1       0\n",
              "55          1       4\n",
              "56          1       6\n",
              "57          1       7\n",
              "58          1       5\n",
              "59          1       3\n",
              "60          1       7\n",
              "61          1       6\n",
              "62          1       8\n",
              "63          1       0\n",
              "\n",
              "[64 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3f958e5da0aed9b404c743c8798b64e0d4341fdd",
        "id": "3ExMeo9h6zVl",
        "colab_type": "code",
        "colab": {},
        "outputId": "904931a9-76ce-4e05-f317-ea067e1c7b49"
      },
      "cell_type": "code",
      "source": [
        "equals=top_class == labels.view(*top_class.shape)\n",
        "accuracy=torch.mean(equals.type(torch.FloatTensor))\n",
        "accuracy.item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.046875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:27:29.556056Z",
          "start_time": "2019-02-04T13:27:29.541115Z"
        },
        "_uuid": "352673a9d93a8f6b52165952364a7793ff43b8f0",
        "id": "rYg7-dBM6zVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model=Network()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "criterion=nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:29:20.825396Z",
          "start_time": "2019-02-04T13:27:30.220213Z"
        },
        "_uuid": "d7ef3898265ce167202ee321100129a95e3b748e",
        "id": "dCf_wBqr6zVx",
        "colab_type": "code",
        "colab": {},
        "outputId": "95b62e6d-66b1-4530-b8d3-79c0410b570d"
      },
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        #Anand (down): We MUST put torch to no_grad since during testing, we don't want to compute gradients.\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5..  Training Loss: 0.405..  Test Loss: 0.216..  Test Accuracy: 0.945\n",
            "Epoch: 2/5..  Training Loss: 0.312..  Test Loss: 0.174..  Test Accuracy: 0.958\n",
            "Epoch: 3/5..  Training Loss: 0.283..  Test Loss: 0.190..  Test Accuracy: 0.957\n",
            "Epoch: 4/5..  Training Loss: 0.272..  Test Loss: 0.189..  Test Accuracy: 0.960\n",
            "Epoch: 5/5..  Training Loss: 0.272..  Test Loss: 0.165..  Test Accuracy: 0.963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:24:46.424543Z",
          "start_time": "2019-02-04T13:24:46.420315Z"
        },
        "id": "-PGbVF8w6zVz",
        "colab_type": "code",
        "colab": {},
        "outputId": "f4c64b00-4ce1-4673-853d-a81acd491f13"
      },
      "cell_type": "code",
      "source": [
        "running_loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114.80688328295946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-04T13:13:40.841068Z",
          "start_time": "2019-02-04T13:13:40.833492Z"
        },
        "id": "vniu4mfU6zV1",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c0ee9d5-1147-4e05-8a3b-bcbf0dc54cde"
      },
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,3])==torch.tensor([1,3,2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "086d29056a5322e8c76820fcdc87ddc5424539ff",
        "id": "Vpw5brn_6zV3",
        "colab_type": "code",
        "colab": {},
        "outputId": "bb49695f-28fe-499b-c234-5fc60aef3474"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f46085a94a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "height": 250,
              "width": 373
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5612d928a89de850888b2caa3251617ff1ebc9fc",
        "id": "m20fQHC06zV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference time"
      ]
    },
    {
      "metadata": {
        "_uuid": "e908e92d92cf3f2634fa665acbca4012bacdb3bf",
        "id": "D-tgbKHs6zV5",
        "colab_type": "code",
        "colab": {},
        "outputId": "bd5c25ea-b476-46e0-9f42-ff52365be7ea"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "img = images[0]\n",
        "# Convert 2D image to 1D vector\n",
        "img = img.view(1, 784)\n",
        "\n",
        "# Calculate the class probabilities (softmax) for img\n",
        "with torch.no_grad():\n",
        "    output = model.forward(img)\n",
        "\n",
        "ps = torch.exp(output)\n",
        "top_prob,top_class=ps.topk(1,dim=1)\n",
        "top_class.item(),labels[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, tensor(0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "086a197d3e597422ee8b592832f811444577b876",
        "id": "TtPF5yGS6zV7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The parameters for PyTorch networks are stored in a model's state_dict\n",
        " Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizerâ€™s state, as well as the hyperparameters used.\n",
        "\n",
        "Because state_dict objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers."
      ]
    },
    {
      "metadata": {
        "_uuid": "2dbde41ca47a505766ec29ad2c89443f9a4d8d7a",
        "id": "sSyZ4PmW6zV7",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ddc0576-61f0-4711-e61a-20b95d084f1c"
      },
      "cell_type": "code",
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "fc1.weight \t torch.Size([256, 784])\n",
            "fc1.bias \t torch.Size([256])\n",
            "fc2.weight \t torch.Size([128, 256])\n",
            "fc2.bias \t torch.Size([128])\n",
            "fc3.weight \t torch.Size([64, 128])\n",
            "fc3.bias \t torch.Size([64])\n",
            "fc4.weight \t torch.Size([10, 64])\n",
            "fc4.bias \t torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "67e383a0385379d68f1c4f0d38143504b098a8d5",
        "id": "x88hK9m96zV_",
        "colab_type": "code",
        "colab": {},
        "outputId": "57290a50-d2ee-45d0-9274-6f0bd1cedbff"
      },
      "cell_type": "code",
      "source": [
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {139938764448248: {'step': 4690, 'exp_avg': tensor([[-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        ...,\n",
            "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45]]), 'exp_avg_sq': tensor([[1.0672e-12, 1.0672e-12, 1.0672e-12,  ..., 1.0672e-12, 1.0672e-12,\n",
            "         1.0672e-12],\n",
            "        [5.6438e-12, 5.6438e-12, 5.6438e-12,  ..., 5.6438e-12, 5.6438e-12,\n",
            "         5.6438e-12],\n",
            "        [2.6544e-12, 2.6544e-12, 2.6544e-12,  ..., 2.6544e-12, 2.6544e-12,\n",
            "         2.6544e-12],\n",
            "        ...,\n",
            "        [3.5010e-10, 3.5010e-10, 3.5010e-10,  ..., 3.5010e-10, 3.5010e-10,\n",
            "         3.5010e-10],\n",
            "        [2.6235e-13, 2.6235e-13, 2.6235e-13,  ..., 2.6235e-13, 2.6235e-13,\n",
            "         2.6235e-13],\n",
            "        [1.5348e-13, 1.5348e-13, 1.5348e-13,  ..., 1.5348e-13, 1.5348e-13,\n",
            "         1.5348e-13]])}, 139938764448320: {'step': 4690, 'exp_avg': tensor([ 5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  1.9869e-03,  5.6052e-45, -3.8861e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  4.1826e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45, -7.0754e-05,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  6.2205e-04,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  0.0000e+00,  5.6052e-45,  5.6052e-45,  5.7664e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -7.6798e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "        -7.5801e-05,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45, -3.6247e-04,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  0.0000e+00,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -5.6052e-45,\n",
            "         5.6052e-45,  1.9959e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  8.9999e-05,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         4.8723e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45, -6.2868e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  3.2150e-04,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  1.3377e-03,\n",
            "         5.6052e-45,  0.0000e+00,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45, -1.2911e-04,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45, -2.5788e-03,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -2.2109e-05,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  2.3889e-04,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45]), 'exp_avg_sq': tensor([1.0672e-12, 5.6438e-12, 2.6544e-12, 1.7969e-10, 2.7641e-10, 1.1351e-10,\n",
            "        1.9261e-11, 2.6192e-05, 2.1720e-09, 1.5409e-05, 3.9248e-12, 1.6586e-12,\n",
            "        6.5444e-12, 1.1637e-09, 1.0256e-13, 2.6695e-09, 3.0330e-14, 1.1122e-14,\n",
            "        2.7887e-11, 6.3554e-10, 8.3343e-13, 2.4236e-10, 2.2410e-12, 1.2457e-12,\n",
            "        4.8346e-12, 2.4334e-13, 2.7988e-12, 9.8203e-10, 3.5959e-10, 1.3392e-09,\n",
            "        4.1938e-10, 3.2680e-11, 1.4212e-10, 9.3625e-12, 8.1192e-11, 3.6879e-14,\n",
            "        1.3181e-11, 5.7402e-13, 1.5562e-10, 2.0967e-13, 5.7243e-12, 3.0011e-12,\n",
            "        3.4388e-10, 3.7287e-10, 3.2670e-05, 4.5105e-12, 1.0419e-10, 1.1023e-06,\n",
            "        4.0710e-12, 1.1458e-11, 2.9676e-12, 6.2899e-10, 1.2500e-05, 2.8583e-11,\n",
            "        3.3725e-12, 6.4890e-13, 5.6665e-14, 4.1907e-11, 2.1881e-05, 5.2413e-10,\n",
            "        6.3047e-11, 4.0407e-10, 3.5480e-11, 5.2225e-10, 8.4388e-08, 3.6585e-10,\n",
            "        3.0677e-13, 4.5140e-12, 1.5527e-10, 2.2364e-13, 6.8517e-11, 2.2283e-10,\n",
            "        9.2202e-15, 6.9845e-12, 1.9735e-12, 1.7842e-11, 9.0682e-11, 1.3479e-10,\n",
            "        1.3722e-09, 3.8547e-12, 3.6745e-10, 3.9654e-11, 1.5636e-11, 9.7886e-14,\n",
            "        3.6133e-09, 2.3397e-13, 5.8986e-11, 4.6151e-11, 5.3210e-10, 1.8769e-11,\n",
            "        6.6370e-10, 0.0000e+00, 5.1186e-12, 1.1946e-10, 3.0534e-05, 2.6922e-10,\n",
            "        2.4455e-15, 1.4132e-14, 1.0190e-11, 1.8254e-14, 1.3637e-13, 4.0029e-12,\n",
            "        2.5329e-10, 1.0020e-10, 3.3058e-11, 3.9839e-12, 2.8040e-09, 1.0039e-09,\n",
            "        1.6956e-13, 2.6669e-05, 7.9699e-10, 3.6006e-12, 1.0587e-11, 1.1921e-12,\n",
            "        1.2113e-10, 1.5312e-12, 4.9272e-13, 4.5935e-13, 3.9149e-10, 1.9017e-09,\n",
            "        2.6293e-05, 1.6448e-09, 3.4819e-15, 7.5784e-10, 1.9334e-12, 9.3651e-12,\n",
            "        4.5913e-12, 1.7880e-15, 3.7740e-13, 1.0218e-09, 1.0727e-12, 6.4843e-12,\n",
            "        9.9876e-06, 4.4223e-12, 1.6688e-10, 1.9079e-11, 3.1336e-11, 7.8380e-14,\n",
            "        1.4823e-12, 2.5071e-10, 5.0848e-12, 1.4520e-09, 4.9023e-10, 2.5450e-05,\n",
            "        5.5733e-13, 1.3309e-09, 1.3828e-11, 1.2068e-14, 2.2570e-13, 0.0000e+00,\n",
            "        2.8853e-13, 1.6869e-10, 2.1133e-11, 1.2078e-11, 1.5241e-07, 7.1458e-13,\n",
            "        1.9263e-05, 4.2083e-14, 1.2893e-14, 1.0257e-10, 8.3892e-10, 4.5716e-05,\n",
            "        7.1241e-10, 6.0215e-10, 2.3477e-12, 2.3442e-05, 9.6258e-13, 1.2284e-13,\n",
            "        2.7751e-10, 5.3265e-11, 1.3688e-10, 3.5399e-05, 8.4617e-13, 3.0231e-13,\n",
            "        2.9974e-13, 3.0907e-10, 8.6281e-10, 1.9042e-05, 1.0379e-09, 4.4594e-11,\n",
            "        1.7976e-09, 1.2312e-09, 1.2393e-09, 9.4911e-10, 6.4908e-13, 3.2711e-10,\n",
            "        1.9321e-12, 2.7228e-11, 1.3660e-08, 4.6847e-05, 6.4379e-11, 0.0000e+00,\n",
            "        5.7597e-10, 1.1104e-09, 1.8395e-09, 7.4474e-11, 9.9066e-11, 3.4771e-12,\n",
            "        1.1544e-05, 2.4175e-10, 3.6509e-13, 4.4882e-10, 1.3791e-12, 6.2391e-10,\n",
            "        3.0720e-13, 1.0717e-09, 5.0825e-13, 1.5498e-11, 2.1542e-11, 1.1633e-12,\n",
            "        1.3219e-12, 4.4117e-10, 2.6094e-13, 3.8293e-05, 1.0151e-12, 1.0421e-11,\n",
            "        4.7784e-11, 1.7600e-09, 1.0329e-09, 3.4237e-10, 8.7960e-15, 4.2933e-12,\n",
            "        2.6447e-10, 9.7544e-12, 9.1105e-11, 4.7322e-10, 2.9594e-10, 2.6980e-10,\n",
            "        7.2375e-10, 2.7878e-06, 1.5102e-11, 6.6121e-13, 3.8656e-10, 4.6128e-12,\n",
            "        3.3457e-14, 7.4658e-13, 2.9440e-12, 3.8289e-10, 1.0048e-12, 4.0302e-11,\n",
            "        1.9687e-11, 1.9553e-09, 2.1182e-05, 9.2287e-10, 6.6251e-11, 2.6063e-12,\n",
            "        2.6851e-12, 2.4373e-12, 1.8735e-13, 6.1066e-10, 4.6465e-11, 2.7108e-11,\n",
            "        2.0267e-08, 3.5010e-10, 2.6235e-13, 1.5348e-13])}, 139938764448032: {'step': 4690, 'exp_avg': tensor([[-5.6052e-45, -5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
            "         -5.6052e-45,  0.0000e+00],\n",
            "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  0.0000e+00],\n",
            "        [-5.6052e-45,  5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45,  0.0000e+00],\n",
            "        ...,\n",
            "        [-5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45, -5.6052e-45],\n",
            "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -5.6052e-45,\n",
            "         -5.6052e-45, -5.6052e-45],\n",
            "        [ 5.6052e-45,  5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
            "          5.6052e-45, -5.6052e-45]]), 'exp_avg_sq': tensor([[7.9434e-14, 1.0418e-11, 1.5213e-12,  ..., 7.2758e-08, 3.2244e-14,\n",
            "         0.0000e+00],\n",
            "        [9.8376e-12, 2.8354e-12, 1.0845e-10,  ..., 4.5710e-14, 4.4737e-12,\n",
            "         0.0000e+00],\n",
            "        [8.4480e-12, 9.1435e-12, 3.7939e-12,  ..., 1.6609e-07, 1.0777e-13,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.0025e-12, 3.7958e-13, 4.0860e-14,  ..., 2.7594e-10, 2.1368e-15,\n",
            "         7.2536e-16],\n",
            "        [3.5580e-12, 1.0942e-11, 3.1708e-12,  ..., 8.5364e-14, 1.7037e-13,\n",
            "         7.6908e-18],\n",
            "        [1.8645e-12, 5.9150e-13, 4.0718e-14,  ..., 4.5469e-15, 4.1382e-14,\n",
            "         9.2481e-15]])}, 139938764448392: {'step': 4690, 'exp_avg': tensor([ 5.6052e-45, -7.1913e-04,  5.6052e-45,  1.3756e-03,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45, -1.0154e-03,  1.0552e-05, -6.8788e-04,\n",
            "         1.3395e-04, -4.4091e-04,  5.6052e-45,  5.6052e-45, -4.5439e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45, -2.3769e-04, -3.4736e-05,\n",
            "         2.4078e-04,  5.6052e-45, -1.7613e-05, -4.7469e-05, -7.3205e-06,\n",
            "        -6.2514e-05,  5.6052e-45, -6.4710e-04, -3.3509e-04,  5.1641e-05,\n",
            "         5.6052e-45,  2.5371e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "        -3.3937e-04, -7.6775e-04,  5.6052e-45, -1.1263e-03, -1.0148e-04,\n",
            "        -8.7690e-05,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         2.2413e-04, -6.3344e-04,  1.9339e-03,  5.6052e-45,  5.6052e-45,\n",
            "        -1.5525e-03,  6.6449e-04,  5.6052e-45,  5.2412e-29, -5.6052e-45,\n",
            "        -2.1826e-04,  6.7769e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         3.2539e-04,  5.6052e-45,  5.8819e-04, -5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  5.6052e-45,  1.1595e-05, -1.8346e-04,  5.6052e-45,\n",
            "         5.6052e-45,  1.6366e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         7.1367e-04, -6.8712e-04,  5.6052e-45,  5.6052e-45,  1.3969e-16,\n",
            "        -3.0182e-03,  5.6052e-45, -3.8315e-06,  5.6052e-45,  3.5855e-04,\n",
            "         7.1612e-07,  5.6052e-45, -1.2283e-03, -1.4189e-03,  5.6052e-45,\n",
            "        -3.3793e-04,  5.6052e-45,  3.6493e-05, -1.1643e-03,  9.2152e-04,\n",
            "         2.1364e-03,  5.6052e-45,  5.6052e-45, -2.8686e-03,  4.3256e-03,\n",
            "        -2.5910e-04,  5.6052e-45, -9.8143e-05,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45, -3.7262e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
            "         5.6052e-45,  1.8485e-04,  5.6052e-45,  5.6052e-45,  4.7573e-04,\n",
            "         5.6052e-45,  5.6052e-45,  5.6052e-45,  1.8171e-04,  5.6052e-45,\n",
            "         5.6052e-45, -7.0720e-19,  8.2645e-04,  5.6052e-45,  2.4895e-07,\n",
            "         5.6052e-45,  4.4287e-04,  5.6052e-45]), 'exp_avg_sq': tensor([3.0625e-09, 3.7561e-05, 9.0181e-09, 8.0511e-06, 1.8948e-08, 3.7467e-09,\n",
            "        1.2617e-10, 2.0565e-05, 2.9852e-06, 1.6851e-05, 6.0831e-05, 4.5191e-05,\n",
            "        6.3829e-12, 2.3772e-09, 3.5977e-05, 6.1131e-09, 1.2825e-07, 8.2513e-08,\n",
            "        1.7493e-05, 7.2988e-07, 4.2514e-05, 1.1519e-07, 1.6583e-05, 9.4784e-05,\n",
            "        6.7253e-06, 1.5528e-05, 6.4440e-11, 4.6124e-05, 1.0704e-05, 4.5202e-06,\n",
            "        5.8952e-08, 1.4539e-05, 9.4094e-10, 3.7332e-08, 4.1061e-09, 3.8045e-05,\n",
            "        1.6538e-05, 1.7228e-07, 4.6733e-05, 1.9921e-05, 3.6860e-06, 1.5960e-09,\n",
            "        6.5657e-11, 5.7696e-10, 9.0410e-08, 1.6904e-05, 3.7576e-06, 3.6612e-05,\n",
            "        2.6237e-11, 1.9321e-06, 4.3232e-05, 8.2078e-06, 6.0868e-10, 8.0649e-07,\n",
            "        2.6894e-09, 2.8251e-05, 5.2627e-05, 5.9598e-07, 1.1437e-10, 1.2803e-09,\n",
            "        2.1045e-05, 1.1579e-09, 7.0277e-06, 1.7403e-08, 2.7567e-10, 1.5016e-07,\n",
            "        3.2373e-07, 1.6453e-05, 4.5019e-06, 1.3831e-09, 1.2823e-09, 4.7300e-05,\n",
            "        1.3828e-07, 1.9206e-11, 2.4236e-09, 2.6732e-05, 3.1049e-05, 9.5279e-09,\n",
            "        3.8238e-10, 1.3876e-05, 2.4552e-05, 6.3225e-10, 2.4355e-06, 6.5541e-10,\n",
            "        6.8401e-06, 4.3883e-06, 3.7382e-09, 3.9870e-05, 2.3104e-05, 5.0632e-09,\n",
            "        9.3330e-06, 2.0109e-06, 1.5914e-05, 1.7909e-05, 2.8939e-06, 3.4523e-05,\n",
            "        6.7039e-11, 3.0427e-09, 3.1710e-05, 8.4305e-05, 3.2208e-05, 1.4317e-11,\n",
            "        7.8699e-06, 1.9220e-09, 8.8705e-09, 8.2019e-09, 1.3335e-05, 1.0542e-09,\n",
            "        3.9586e-09, 1.4460e-08, 2.3743e-10, 3.4270e-05, 3.4869e-07, 1.1575e-09,\n",
            "        2.7310e-05, 9.3249e-10, 3.4640e-07, 7.7720e-09, 5.9898e-05, 1.4325e-09,\n",
            "        2.3490e-10, 2.1520e-07, 1.4228e-05, 6.5757e-09, 6.3056e-06, 1.2339e-08,\n",
            "        4.7670e-06, 8.2343e-11])}, 139938764448464: {'step': 4690, 'exp_avg': tensor([[-5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
            "         -6.2412e-21, -5.6052e-45],\n",
            "        [-5.6052e-45, -1.3829e-09, -5.6052e-45,  ...,  5.6052e-45,\n",
            "         -1.0002e-29,  5.6052e-45],\n",
            "        [ 5.6052e-45,  4.9096e-14,  5.6052e-45,  ..., -5.6052e-45,\n",
            "          1.2612e-44,  5.6052e-45],\n",
            "        ...,\n",
            "        [-5.6052e-45,  6.2319e-05,  5.6052e-45,  ...,  5.6052e-45,\n",
            "          3.3463e-03,  5.6052e-45],\n",
            "        [ 5.6052e-45, -2.6765e-09,  5.6052e-45,  ...,  5.6052e-45,\n",
            "         -5.2536e-07, -5.6052e-45],\n",
            "        [ 5.6052e-45, -7.3111e-04,  5.6052e-45,  ..., -5.6052e-45,\n",
            "         -3.2900e-07, -5.6052e-45]]), 'exp_avg_sq': tensor([[4.1102e-10, 7.6559e-08, 2.9449e-10,  ..., 4.6118e-11, 2.4881e-07,\n",
            "         6.2620e-12],\n",
            "        [1.3446e-11, 1.3998e-07, 8.1897e-12,  ..., 4.1310e-10, 1.0512e-07,\n",
            "         2.3035e-11],\n",
            "        [5.2659e-07, 1.6210e-05, 1.7766e-07,  ..., 4.7629e-10, 6.1656e-07,\n",
            "         3.6837e-11],\n",
            "        ...,\n",
            "        [4.4067e-07, 2.9003e-04, 7.1843e-08,  ..., 3.5204e-08, 3.4335e-04,\n",
            "         2.7386e-11],\n",
            "        [1.4739e-07, 3.0986e-07, 8.9177e-08,  ..., 1.0348e-09, 4.7723e-07,\n",
            "         1.5658e-11],\n",
            "        [1.2048e-09, 5.9770e-04, 5.5027e-09,  ..., 1.0889e-08, 5.9129e-05,\n",
            "         1.0608e-12]])}, 139938764448536: {'step': 4690, 'exp_avg': tensor([-1.2694e-03, -2.6088e-04, -1.2199e-04, -2.7442e-03,  3.3557e-29,\n",
            "         1.2621e-03,  1.3546e-03, -1.0149e-05, -1.2716e-04,  1.0449e-03,\n",
            "         9.9274e-05,  6.9325e-19, -1.7420e-03,  5.2731e-04,  2.2307e-03,\n",
            "        -9.7052e-04,  1.9915e-04,  8.6799e-04,  8.5638e-04,  3.1660e-09,\n",
            "         1.7964e-03,  1.5173e-03,  1.2052e-03,  2.5808e-10, -4.4369e-08,\n",
            "         1.1477e-04,  3.6653e-05,  6.0222e-05,  1.1924e-03, -1.6580e-04,\n",
            "         1.1767e-04,  6.2779e-09, -4.1949e-06,  2.0211e-19, -1.4377e-03,\n",
            "        -7.9575e-05,  1.3026e-04, -3.3452e-04, -1.3891e-06, -1.9050e-03,\n",
            "        -3.8414e-05,  6.3988e-04,  3.1106e-03,  1.7934e-03, -2.4263e-03,\n",
            "        -8.7117e-04,  8.9961e-08,  1.3902e-03,  3.8136e-07, -3.1852e-04,\n",
            "         1.8891e-04,  1.9144e-03,  4.4033e-05,  1.7421e-03, -1.3275e-03,\n",
            "        -1.8738e-04, -5.0274e-04, -1.1329e-03,  1.1579e-03,  5.6052e-45,\n",
            "         1.1028e-03, -1.2924e-03, -1.1409e-03,  1.1723e-04]), 'exp_avg_sq': tensor([2.6705e-06, 1.1447e-05, 7.2141e-06, 3.1170e-05, 3.2152e-07, 8.5145e-05,\n",
            "        2.4227e-05, 1.1884e-06, 8.7847e-07, 2.8445e-05, 4.9708e-06, 7.0964e-07,\n",
            "        2.1961e-05, 1.7090e-05, 4.1470e-05, 5.5243e-05, 6.5839e-06, 1.2135e-05,\n",
            "        2.7767e-05, 4.6554e-07, 3.3138e-05, 3.2179e-05, 4.9872e-05, 2.5243e-07,\n",
            "        1.0274e-07, 2.8582e-05, 1.9342e-05, 2.9582e-05, 1.7365e-05, 9.5685e-06,\n",
            "        3.1398e-05, 1.1863e-06, 9.3599e-07, 3.4258e-07, 1.5000e-05, 4.0506e-05,\n",
            "        1.8539e-05, 1.0701e-05, 1.6539e-06, 1.3089e-05, 2.4012e-05, 1.4182e-05,\n",
            "        2.7168e-05, 3.8256e-05, 1.8248e-05, 4.0793e-05, 5.2570e-07, 6.6052e-05,\n",
            "        2.4673e-05, 2.2549e-06, 1.5263e-06, 3.7395e-05, 1.6370e-05, 3.2275e-05,\n",
            "        2.3170e-05, 1.0391e-06, 4.0327e-05, 2.7588e-05, 8.9917e-05, 7.6201e-09,\n",
            "        1.6539e-06, 2.2187e-05, 1.9825e-05, 3.7704e-05])}, 139938764448608: {'step': 4690, 'exp_avg': tensor([[ 4.2187e-05,  2.6604e-04, -1.3696e-03,  4.9864e-03, -1.7112e-29,\n",
            "          1.9029e-03,  5.2715e-04, -2.3401e-05,  3.9151e-06,  6.7277e-04,\n",
            "          7.4782e-03,  8.1841e-20,  6.3295e-04, -8.9461e-05,  1.4987e-03,\n",
            "          2.5820e-03,  1.3190e-04,  2.0128e-03,  7.1884e-04,  1.7423e-12,\n",
            "          2.1116e-02,  2.9078e-03,  2.0028e-03, -1.5851e-10,  2.6016e-09,\n",
            "          1.6087e-03,  2.5039e-03,  1.1108e-03,  1.6185e-04,  1.1327e-04,\n",
            "          7.1695e-04,  1.8841e-10, -4.6761e-05,  5.9265e-20,  1.9123e-04,\n",
            "          1.2160e-03,  9.9651e-04,  2.3551e-03, -2.5064e-06,  6.0944e-04,\n",
            "          3.2372e-04,  9.0761e-04,  1.7688e-03,  7.7591e-03,  3.7219e-03,\n",
            "          1.0351e-03,  4.7072e-10,  1.8399e-03,  2.6266e-03,  1.2512e-06,\n",
            "          5.1762e-07,  6.4767e-04,  2.6818e-04,  8.1789e-04,  4.2889e-03,\n",
            "         -7.9262e-07,  5.4598e-03,  7.5546e-04,  1.1576e-03,  5.6052e-45,\n",
            "          4.7542e-04,  5.8726e-04,  9.2837e-04,  5.2679e-04],\n",
            "        [-3.3542e-03, -3.6314e-04, -1.2586e-04,  1.0465e-03,  1.5351e-29,\n",
            "         -2.1748e-04, -2.8837e-04,  2.4769e-06,  2.5729e-05,  2.0451e-04,\n",
            "         -1.7791e-05,  3.1593e-20,  3.7534e-03, -1.7797e-03, -5.2851e-03,\n",
            "          4.1384e-03,  1.0063e-05,  2.5123e-04, -7.7640e-03,  8.3253e-13,\n",
            "          8.1167e-04,  4.0520e-04,  1.8341e-03,  9.3560e-13,  3.3592e-07,\n",
            "          1.2315e-03,  2.2896e-03,  2.6526e-03,  3.5277e-04,  4.5061e-05,\n",
            "         -4.7379e-03,  2.3253e-12,  5.1016e-08,  1.4028e-20, -9.6820e-04,\n",
            "         -7.2170e-03, -6.2380e-05,  8.8532e-04,  8.6003e-10, -4.6996e-03,\n",
            "         -4.1461e-04,  5.5056e-04, -7.2071e-03, -1.3774e-03,  1.3818e-03,\n",
            "          9.5002e-04,  2.5075e-10,  1.4318e-03, -1.3602e-03, -1.2410e-04,\n",
            "         -4.8825e-04, -5.7250e-03, -2.1955e-04, -1.7338e-03,  2.3564e-04,\n",
            "          2.4592e-05, -7.0211e-04, -3.5459e-05,  5.0628e-04,  5.6052e-45,\n",
            "         -1.5265e-03,  1.5642e-04, -1.2340e-03,  7.4013e-04],\n",
            "        [ 2.1434e-03,  4.5749e-04,  3.3681e-04,  8.9392e-03,  7.3956e-31,\n",
            "         -2.0396e-03, -2.7319e-03,  2.0980e-05,  2.0359e-05,  4.3048e-04,\n",
            "          5.9199e-04,  6.6045e-20,  2.4231e-03,  4.0255e-03,  3.2932e-02,\n",
            "          8.6814e-03,  2.2720e-03,  6.5535e-03,  9.4804e-03,  2.2633e-09,\n",
            "          4.1272e-03,  2.6908e-03,  4.9730e-03,  1.0601e-11,  2.0770e-07,\n",
            "          4.9314e-03,  2.8137e-02,  1.3110e-02,  2.9057e-02,  9.7522e-04,\n",
            "          7.6791e-03,  6.0080e-11,  3.7882e-05,  7.2717e-18,  1.1034e-03,\n",
            "          3.8246e-03,  3.4990e-03,  7.6195e-04,  7.2009e-07,  2.9840e-03,\n",
            "          5.3754e-03, -4.9280e-04,  9.1907e-03,  3.6835e-03,  3.3507e-03,\n",
            "          2.1576e-06,  8.1283e-10,  2.4717e-03, -2.4314e-03,  1.5895e-04,\n",
            "          4.1714e-04,  9.3145e-03,  9.6410e-04,  1.0803e-02,  2.1461e-02,\n",
            "          2.3276e-04,  3.3825e-03, -1.8260e-03, -2.4202e-03,  5.6052e-45,\n",
            "          4.1496e-03, -1.8345e-03,  1.0932e-02,  1.5034e-03],\n",
            "        [-1.8528e-03,  3.6868e-04,  4.8061e-04, -5.1192e-03, -7.8215e-29,\n",
            "          1.1526e-03, -2.9197e-03,  2.7166e-06,  3.6338e-05, -7.2724e-03,\n",
            "          5.1178e-04,  1.9198e-19,  1.5576e-03, -2.4770e-04, -1.9984e-02,\n",
            "         -3.6837e-03,  1.4203e-04, -4.5522e-03,  3.4927e-03,  2.8758e-12,\n",
            "          1.5391e-02, -1.7772e-03, -5.7509e-03,  3.5008e-11, -8.9722e-07,\n",
            "         -3.3415e-03, -7.0807e-03, -2.9294e-02, -2.0003e-02,  5.8361e-04,\n",
            "          3.1023e-03, -2.8464e-07,  7.0937e-07,  7.2191e-18,  8.2473e-04,\n",
            "          2.9577e-03,  1.7735e-03,  7.1002e-04,  2.0628e-07,  1.6079e-03,\n",
            "          1.2529e-03, -1.8871e-03, -2.3552e-03, -6.3152e-03,  1.7635e-03,\n",
            "          4.3763e-03,  1.4073e-09, -1.2899e-02,  1.8266e-03, -1.1865e-03,\n",
            "          2.8877e-05,  5.1754e-04,  2.1931e-04, -2.8071e-03, -1.3508e-02,\n",
            "         -3.9310e-03, -1.5481e-03,  2.3059e-04, -4.4032e-04, -5.6052e-45,\n",
            "         -1.9305e-02,  1.0420e-03, -1.6216e-02,  2.8147e-03],\n",
            "        [ 1.7235e-04, -1.7584e-04, -1.6904e-05, -8.5751e-03,  1.1478e-31,\n",
            "         -2.8169e-03, -3.3283e-03,  2.9893e-06,  1.0040e-04,  2.3429e-03,\n",
            "          3.2027e-04,  9.9320e-20, -1.2071e-02,  1.1296e-03, -8.9843e-03,\n",
            "         -4.6642e-03,  5.5587e-03,  3.3427e-04, -1.0583e-02, -4.6654e-09,\n",
            "         -4.0574e-03,  1.6450e-05, -3.1495e-02,  6.3359e-12,  6.2557e-10,\n",
            "          3.5299e-03, -1.4307e-02, -3.9670e-03,  3.6267e-04,  2.0332e-04,\n",
            "         -1.2003e-02,  4.1124e-11,  1.1215e-06,  1.1167e-21,  2.0489e-04,\n",
            "         -3.2243e-02, -1.5986e-03,  2.2387e-03,  2.6648e-08,  7.8832e-05,\n",
            "         -1.5170e-02, -3.7002e-03, -9.3753e-05, -8.2324e-04,  4.1171e-04,\n",
            "         -4.6575e-02,  3.1692e-08, -5.4948e-03, -2.8384e-03,  5.4727e-05,\n",
            "          3.6916e-06, -5.8607e-03,  1.8189e-02, -1.1772e-02, -4.7710e-03,\n",
            "          9.5761e-08, -1.4206e-04,  1.2007e-02, -3.1590e-03,  5.6052e-45,\n",
            "         -3.1731e-03,  3.9314e-03, -3.6387e-03, -9.0185e-05],\n",
            "        [ 2.6565e-03,  4.0214e-04, -7.2051e-05,  2.8062e-03,  7.7458e-29,\n",
            "         -6.5250e-04,  1.8336e-03, -1.4513e-05,  1.0764e-05, -9.1864e-04,\n",
            "          4.7937e-04, -8.0273e-19, -6.9907e-04, -1.6974e-03, -1.0601e-02,\n",
            "          4.2446e-04, -7.3027e-03, -1.6737e-03, -1.2562e-03,  7.5716e-11,\n",
            "         -2.8671e-02, -2.4980e-03,  6.2782e-03,  4.3258e-11,  1.2327e-07,\n",
            "         -4.9683e-04, -5.7727e-03,  1.6668e-02, -1.4866e-02,  2.7199e-04,\n",
            "         -2.2523e-03,  3.7639e-11,  2.1353e-06,  1.7547e-19,  3.9535e-04,\n",
            "          5.5373e-04, -2.8453e-04,  7.9367e-04,  2.5573e-07,  1.0797e-03,\n",
            "         -1.6025e-03,  1.7226e-03, -4.0876e-03,  6.9553e-03,  5.5377e-03,\n",
            "          1.3147e-03,  3.3806e-09,  6.5330e-03, -1.0541e-03,  9.9492e-04,\n",
            "          1.3157e-06, -8.3144e-04, -3.2759e-04,  1.0302e-03, -6.1528e-03,\n",
            "          3.4976e-03,  6.5879e-03,  1.3895e-03,  2.4253e-03,  5.6052e-45,\n",
            "          1.8945e-02,  5.8229e-04,  1.3690e-02,  2.3418e-05],\n",
            "        [-2.5989e-04, -2.0477e-03,  3.5936e-04, -1.1176e-02,  1.2803e-30,\n",
            "         -5.6811e-04,  1.3798e-05,  1.8071e-06,  1.4188e-06, -4.5631e-04,\n",
            "         -3.5439e-03,  1.0497e-19, -9.4282e-04, -3.8251e-04,  4.8304e-03,\n",
            "          1.3663e-03,  8.4795e-03, -1.6011e-03,  7.4922e-04,  1.9273e-09,\n",
            "         -3.1722e-03, -2.4984e-03, -1.2202e-03,  1.2138e-11,  1.8087e-10,\n",
            "         -1.1085e-02, -3.9690e-03, -1.0470e-03,  8.6844e-04,  1.1566e-03,\n",
            "         -3.2268e-04,  3.9250e-11,  1.4322e-06,  1.0944e-22, -1.7118e-03,\n",
            "         -2.9426e-03, -9.3638e-04,  9.2713e-05,  3.4617e-07, -4.7432e-03,\n",
            "          8.0696e-04,  9.9422e-05, -6.1617e-03,  4.7998e-04, -2.2151e-02,\n",
            "         -2.7853e-03,  5.5539e-10,  1.5012e-03,  1.7151e-03,  2.8639e-06,\n",
            "          6.4770e-06, -1.0184e-03, -6.4777e-03, -4.0793e-03, -5.8255e-03,\n",
            "          2.7091e-07, -4.1333e-03, -7.8449e-03,  7.8116e-04,  5.6052e-45,\n",
            "          2.1154e-04, -1.3857e-04,  2.6875e-04, -5.9623e-04],\n",
            "        [ 2.7871e-04,  5.0526e-04,  1.0888e-04,  1.6472e-03,  1.9279e-31,\n",
            "          1.1461e-03,  2.8544e-03, -6.7834e-05,  7.4633e-05,  2.1030e-02,\n",
            "          4.0587e-04,  5.9754e-20,  1.5222e-03,  8.4653e-03,  3.9053e-03,\n",
            "          1.6968e-03, -9.4106e-03,  4.8177e-04,  3.6705e-03,  2.2261e-11,\n",
            "          3.8753e-03, -2.6566e-03,  2.7575e-02,  1.8432e-11,  9.8482e-08,\n",
            "          2.8061e-03, -6.6835e-03,  8.4084e-04,  1.1431e-03, -9.6757e-04,\n",
            "          2.9601e-03,  2.0254e-07,  2.0740e-06, -1.4760e-17,  6.0227e-04,\n",
            "          1.0782e-02, -7.8397e-03, -3.5276e-03,  2.2876e-07, -1.7897e-04,\n",
            "          5.3194e-03,  1.8559e-03,  5.1801e-03, -2.0267e-03,  1.6155e-05,\n",
            "          6.1034e-03, -1.0208e-07,  8.6650e-04,  1.7652e-03,  6.7847e-05,\n",
            "          1.9505e-05, -4.8422e-04, -2.5172e-03,  7.8138e-03,  3.2107e-04,\n",
            "          1.5823e-04,  2.4303e-03,  5.9023e-04,  7.2540e-04,  5.6052e-45,\n",
            "          5.0805e-04, -8.9785e-05,  8.5664e-04, -1.1661e-02],\n",
            "        [ 2.2742e-04,  3.8786e-04,  1.5409e-04,  2.7231e-03,  1.5587e-32,\n",
            "          1.8539e-03,  3.1528e-03,  3.6302e-06,  3.0286e-05, -1.1336e-02,\n",
            "         -7.5908e-03,  1.0348e-19,  2.3551e-03,  2.0570e-03,  2.7650e-03,\n",
            "         -8.3763e-03,  8.3492e-05, -2.8526e-03,  2.0590e-03,  1.5670e-13,\n",
            "         -1.2165e-02,  1.9710e-03, -1.4280e-02,  7.2332e-12,  2.7520e-08,\n",
            "          3.0460e-03,  1.8374e-03,  3.2059e-03,  3.0098e-03, -2.5274e-03,\n",
            "          4.0717e-03,  2.1743e-11,  1.9299e-07,  1.4547e-21, -1.0289e-03,\n",
            "          1.1407e-03,  8.0659e-04,  3.8781e-04,  9.7354e-08,  2.2146e-03,\n",
            "          3.6262e-03,  7.0616e-04,  4.0699e-03, -4.6725e-03,  3.3164e-03,\n",
            "          1.5301e-03,  1.2151e-09,  4.5305e-03, -1.8262e-03,  1.0010e-05,\n",
            "          9.6836e-06,  3.1652e-03,  4.2519e-04,  4.5566e-03,  2.9541e-03,\n",
            "          4.8607e-06, -6.2740e-03,  8.0440e-04,  2.5020e-03,  5.6052e-45,\n",
            "          5.5837e-04,  1.1359e-03, -6.1988e-03,  9.3863e-04],\n",
            "        [-5.3688e-05,  1.9922e-04,  1.4470e-04,  2.7213e-03,  1.1635e-32,\n",
            "          2.3914e-04,  8.8667e-04,  7.1143e-05, -3.0384e-04, -4.6974e-03,\n",
            "          1.3650e-03,  6.3746e-20,  1.4686e-03, -1.1481e-02, -1.0770e-03,\n",
            "         -2.1650e-03,  3.5494e-05,  1.0461e-03, -5.6768e-04,  3.7119e-10,\n",
            "          2.7446e-03,  1.4389e-03,  1.0083e-02,  9.3071e-11,  1.0098e-07,\n",
            "         -2.2298e-03,  3.0451e-03, -3.2803e-03, -8.6808e-05,  1.4581e-04,\n",
            "          7.8564e-04,  8.1706e-08,  1.1659e-06,  1.8233e-20,  3.8704e-04,\n",
            "          2.1928e-02,  3.6460e-03, -4.6978e-03,  6.2464e-07,  1.0473e-03,\n",
            "          4.8239e-04,  2.3789e-04, -3.0410e-04, -3.6629e-03,  2.6507e-03,\n",
            "          3.4049e-02,  6.2293e-08, -7.8084e-04,  1.5769e-03,  2.0067e-05,\n",
            "          1.0421e-06,  2.7491e-04, -1.0524e-02, -4.6289e-03,  9.9575e-04,\n",
            "          1.3345e-05, -5.0609e-03, -6.0704e-03, -2.0782e-03,  5.6052e-45,\n",
            "         -8.4293e-04, -5.3723e-03,  6.1220e-04,  5.8004e-03]]), 'exp_avg_sq': tensor([[2.1883e-07, 1.6201e-05, 1.8958e-03, 1.4396e-03, 2.7279e-05, 6.7022e-04,\n",
            "         2.3353e-04, 7.8067e-05, 1.3287e-06, 5.7056e-05, 1.3278e-03, 2.9265e-06,\n",
            "         1.8967e-05, 7.9478e-05, 2.2681e-04, 2.5733e-04, 3.1245e-04, 1.7981e-03,\n",
            "         8.7399e-04, 1.5764e-06, 3.6113e-03, 5.6667e-03, 7.8409e-05, 5.4610e-05,\n",
            "         5.8497e-07, 1.5129e-04, 6.5978e-04, 8.8302e-05, 1.8075e-03, 1.8133e-05,\n",
            "         3.2921e-05, 1.2176e-06, 2.2422e-04, 1.0524e-05, 2.5766e-05, 7.3899e-05,\n",
            "         2.4203e-03, 1.4351e-04, 9.6987e-04, 8.9928e-05, 9.0379e-05, 8.3479e-04,\n",
            "         1.4924e-04, 5.0731e-03, 1.0961e-03, 6.0364e-05, 7.0784e-06, 2.7753e-04,\n",
            "         2.3630e-05, 1.4073e-04, 9.0809e-06, 3.6693e-05, 8.5705e-05, 9.5203e-05,\n",
            "         2.3112e-03, 3.7333e-06, 1.4568e-03, 2.6249e-04, 1.0183e-04, 2.4988e-09,\n",
            "         9.5163e-05, 1.4432e-04, 5.0964e-05, 4.8801e-05],\n",
            "        [2.4048e-04, 3.1028e-04, 2.7713e-05, 2.5750e-04, 3.2501e-07, 1.6522e-04,\n",
            "         1.1347e-03, 1.0616e-06, 1.5650e-06, 5.7413e-04, 1.8202e-05, 4.1395e-05,\n",
            "         1.7319e-03, 3.3016e-04, 5.4576e-04, 5.3653e-04, 4.3326e-05, 9.4813e-06,\n",
            "         9.6826e-04, 2.0495e-05, 7.6988e-05, 1.0435e-04, 1.2653e-04, 1.6680e-07,\n",
            "         1.0058e-06, 2.6033e-04, 2.0332e-03, 4.0074e-03, 1.1766e-04, 3.3264e-05,\n",
            "         3.1315e-03, 1.5847e-06, 4.1382e-06, 7.7828e-07, 1.7058e-04, 1.4049e-03,\n",
            "         5.7828e-05, 1.1395e-04, 1.4821e-06, 5.9042e-04, 3.3081e-03, 1.8140e-04,\n",
            "         4.4278e-04, 1.0983e-04, 1.1669e-04, 7.4690e-05, 2.6807e-06, 9.8761e-04,\n",
            "         3.9516e-04, 1.7912e-04, 8.8512e-05, 2.1097e-03, 3.1417e-05, 2.6026e-03,\n",
            "         4.5248e-05, 2.8926e-05, 6.7529e-04, 3.8187e-05, 2.4044e-04, 1.5909e-10,\n",
            "         2.4393e-05, 4.1368e-05, 9.5813e-04, 4.8991e-04],\n",
            "        [2.3655e-04, 2.9506e-04, 3.4635e-04, 3.4303e-03, 1.5792e-05, 3.1341e-04,\n",
            "         5.1791e-03, 9.9411e-06, 7.5563e-06, 9.0502e-04, 1.6560e-04, 5.6657e-05,\n",
            "         5.2367e-04, 1.2643e-03, 6.0990e-03, 5.2554e-04, 1.3155e-03, 4.8623e-04,\n",
            "         4.3162e-03, 6.1900e-05, 2.4950e-04, 6.6816e-04, 4.6569e-04, 4.1045e-06,\n",
            "         6.3059e-06, 3.2749e-03, 1.8341e-03, 1.6312e-03, 2.0322e-03, 7.7112e-04,\n",
            "         1.5096e-03, 8.4134e-06, 7.1783e-05, 5.7326e-06, 2.0501e-04, 6.9166e-04,\n",
            "         6.6931e-04, 1.1693e-03, 1.4508e-04, 1.8956e-04, 9.0558e-04, 2.3381e-03,\n",
            "         6.6214e-03, 1.7782e-04, 3.3667e-03, 2.8070e-04, 2.2509e-05, 2.9269e-03,\n",
            "         2.8544e-04, 1.0033e-04, 2.6134e-04, 3.3836e-03, 1.1883e-03, 5.5677e-03,\n",
            "         2.3527e-03, 1.5459e-04, 3.4185e-04, 1.5266e-03, 2.3956e-04, 1.5240e-10,\n",
            "         2.0902e-04, 1.1801e-03, 1.6187e-03, 4.8698e-04],\n",
            "        [4.5851e-05, 7.8271e-05, 1.8265e-04, 5.1994e-04, 7.5082e-06, 3.9947e-04,\n",
            "         1.5695e-03, 5.4561e-06, 1.3177e-05, 2.2902e-03, 1.1702e-04, 1.2063e-05,\n",
            "         1.5546e-04, 7.7580e-04, 7.4048e-03, 3.5775e-03, 1.5215e-04, 2.1205e-04,\n",
            "         1.8141e-03, 2.6420e-05, 2.9814e-03, 1.0827e-03, 1.0492e-03, 2.8220e-06,\n",
            "         1.5481e-05, 6.7187e-04, 3.0198e-04, 3.6090e-03, 2.6019e-03, 2.1732e-04,\n",
            "         4.2837e-04, 7.9770e-06, 1.1489e-05, 2.6411e-06, 1.0117e-04, 4.6658e-04,\n",
            "         4.2152e-04, 5.7535e-04, 2.9975e-05, 7.3328e-05, 4.4779e-04, 4.8551e-03,\n",
            "         3.8577e-03, 4.3257e-03, 3.4039e-04, 2.8620e-04, 5.5764e-06, 5.0075e-03,\n",
            "         1.8288e-04, 2.4694e-04, 5.3909e-05, 2.2587e-03, 4.7956e-05, 1.8179e-03,\n",
            "         6.7520e-04, 3.6343e-04, 6.3329e-03, 3.9789e-04, 2.3505e-04, 5.9825e-09,\n",
            "         4.7727e-04, 9.6603e-05, 5.0054e-03, 1.0437e-03],\n",
            "        [2.6561e-06, 9.3531e-06, 2.2761e-04, 3.3806e-03, 1.3117e-05, 1.0739e-04,\n",
            "         6.1340e-05, 6.1968e-06, 8.1040e-05, 3.0641e-04, 1.1544e-05, 1.2962e-05,\n",
            "         4.2817e-04, 8.0836e-05, 8.4481e-05, 8.3125e-05, 1.4681e-03, 3.8774e-04,\n",
            "         1.9937e-04, 1.3698e-07, 4.7705e-05, 2.5440e-04, 1.1247e-02, 4.5552e-07,\n",
            "         3.5694e-06, 5.4350e-03, 8.9811e-04, 1.7114e-04, 1.8749e-04, 1.3363e-05,\n",
            "         2.0362e-04, 1.8493e-04, 4.2076e-06, 5.7184e-07, 2.7908e-05, 1.3933e-02,\n",
            "         3.6804e-04, 6.1109e-05, 1.0562e-05, 1.5496e-04, 2.5871e-04, 5.5763e-04,\n",
            "         2.4781e-04, 1.0662e-04, 3.5568e-03, 1.1909e-02, 7.5131e-05, 5.5780e-05,\n",
            "         4.0226e-05, 1.0748e-05, 2.5218e-04, 7.8610e-05, 5.1592e-03, 1.4941e-04,\n",
            "         1.6202e-03, 7.4626e-07, 3.5627e-05, 5.8445e-03, 6.5999e-05, 2.6628e-10,\n",
            "         3.6195e-05, 4.0354e-03, 1.7310e-05, 3.7854e-03],\n",
            "        [1.1309e-05, 2.6879e-04, 7.3558e-05, 2.6527e-03, 3.8254e-05, 6.9366e-04,\n",
            "         1.2813e-04, 1.1271e-05, 5.7811e-06, 3.7325e-04, 1.3861e-04, 2.7521e-06,\n",
            "         1.1055e-04, 8.4964e-05, 2.0771e-03, 2.9751e-03, 1.1176e-03, 2.1562e-04,\n",
            "         1.7878e-04, 6.8023e-06, 8.1698e-03, 5.8499e-03, 4.7873e-04, 3.8173e-06,\n",
            "         3.7762e-06, 2.9106e-04, 7.5900e-04, 1.0646e-03, 5.3965e-04, 6.2286e-05,\n",
            "         1.0522e-04, 1.0194e-05, 5.0665e-06, 1.1644e-06, 1.0353e-04, 3.2729e-04,\n",
            "         5.0933e-04, 1.0797e-04, 4.0178e-04, 7.0512e-05, 2.8072e-04, 6.6640e-03,\n",
            "         2.2125e-04, 9.5707e-03, 1.3179e-03, 2.7743e-04, 4.9310e-06, 1.1070e-03,\n",
            "         7.6651e-05, 6.6183e-05, 1.8177e-05, 2.4570e-04, 1.1251e-04, 2.3701e-04,\n",
            "         1.6277e-03, 4.8575e-05, 7.7154e-03, 1.7327e-03, 3.6634e-04, 5.9991e-11,\n",
            "         4.0104e-04, 2.4623e-04, 1.9379e-03, 2.1698e-04],\n",
            "        [5.1316e-06, 3.9797e-04, 1.2645e-03, 9.3396e-03, 6.3092e-05, 1.5949e-04,\n",
            "         1.1881e-04, 8.7201e-06, 4.7116e-07, 9.7358e-06, 3.1328e-04, 1.0710e-05,\n",
            "         1.5784e-04, 4.7505e-05, 3.3834e-04, 2.6314e-04, 4.5849e-03, 3.2177e-04,\n",
            "         9.7748e-04, 5.8429e-06, 3.5750e-04, 6.8059e-04, 4.0331e-05, 3.8160e-06,\n",
            "         2.9832e-06, 1.1827e-03, 4.0724e-03, 6.7304e-05, 3.4520e-04, 3.8411e-04,\n",
            "         8.6784e-05, 3.5287e-05, 4.8034e-05, 1.0984e-05, 1.6166e-04, 1.4217e-04,\n",
            "         3.4588e-04, 1.0590e-05, 9.5894e-05, 9.7848e-05, 8.7347e-05, 3.3474e-03,\n",
            "         1.5401e-04, 6.0899e-04, 7.8273e-03, 1.5518e-04, 2.0792e-05, 9.9333e-05,\n",
            "         2.5404e-05, 1.7514e-05, 4.4748e-05, 1.3418e-04, 8.7772e-04, 8.1255e-04,\n",
            "         8.0990e-03, 1.6369e-05, 4.1441e-04, 2.8444e-03, 8.5878e-05, 3.1619e-11,\n",
            "         7.8518e-05, 4.2200e-04, 2.2154e-05, 6.2981e-05],\n",
            "        [4.1234e-05, 8.6418e-05, 1.8914e-04, 2.3980e-04, 2.2034e-06, 1.1017e-04,\n",
            "         1.7241e-03, 4.6592e-05, 1.4527e-04, 9.3969e-03, 1.1164e-04, 1.9347e-05,\n",
            "         3.5678e-04, 4.7883e-03, 5.1490e-04, 1.7553e-04, 7.6183e-05, 2.2816e-04,\n",
            "         5.7638e-04, 8.6959e-07, 2.5422e-03, 2.7337e-03, 9.6766e-03, 6.3478e-07,\n",
            "         2.0038e-06, 1.2156e-03, 4.3578e-04, 4.1045e-03, 2.2241e-04, 2.1181e-04,\n",
            "         1.3080e-03, 2.6378e-05, 4.2226e-05, 1.9739e-05, 2.0273e-05, 3.8175e-03,\n",
            "         1.9783e-03, 3.6547e-03, 5.5089e-05, 8.4570e-05, 2.8132e-03, 4.0964e-04,\n",
            "         1.6946e-03, 6.3677e-04, 1.6916e-04, 4.0120e-03, 3.6103e-06, 1.3213e-04,\n",
            "         5.8890e-05, 6.9878e-05, 5.9717e-05, 1.0631e-03, 5.5269e-04, 1.4137e-03,\n",
            "         1.0099e-04, 9.7276e-05, 1.6637e-04, 3.6001e-04, 4.1547e-05, 9.2326e-10,\n",
            "         1.3730e-04, 1.0714e-03, 1.4440e-04, 1.0531e-02],\n",
            "        [4.6148e-06, 5.4602e-05, 2.6673e-05, 5.9405e-04, 2.6433e-06, 1.2805e-03,\n",
            "         6.4936e-04, 8.9534e-06, 9.5573e-06, 1.6526e-04, 4.6374e-04, 2.4344e-05,\n",
            "         7.6101e-04, 6.3664e-05, 6.7481e-04, 1.6934e-03, 4.1299e-04, 1.8916e-04,\n",
            "         3.7590e-04, 9.4195e-06, 3.6665e-04, 4.7268e-04, 6.2545e-04, 3.2661e-05,\n",
            "         1.6038e-06, 1.8283e-04, 7.6535e-04, 5.7430e-04, 3.7495e-04, 2.8371e-05,\n",
            "         9.7615e-04, 5.9481e-06, 2.2854e-05, 1.2445e-06, 4.7114e-04, 5.7027e-04,\n",
            "         1.6382e-04, 7.1331e-05, 1.0255e-04, 1.6099e-04, 5.0110e-04, 7.3056e-04,\n",
            "         3.8673e-04, 4.0860e-04, 3.6084e-04, 3.6564e-04, 3.3205e-06, 1.5052e-03,\n",
            "         6.5719e-04, 3.7422e-05, 1.1424e-05, 3.8222e-04, 4.4701e-05, 6.1089e-04,\n",
            "         4.4008e-04, 2.0611e-05, 8.2164e-04, 3.1836e-04, 1.2158e-03, 4.0874e-11,\n",
            "         9.9986e-06, 5.6143e-05, 4.6974e-04, 2.2863e-04],\n",
            "        [4.2231e-06, 8.2340e-06, 5.5661e-05, 2.1004e-04, 1.8631e-06, 3.2681e-04,\n",
            "         2.5317e-04, 8.9826e-06, 2.3377e-04, 6.6816e-03, 2.3404e-04, 1.2092e-05,\n",
            "         1.7407e-04, 2.7830e-03, 2.0386e-04, 4.8117e-04, 1.9155e-05, 3.8743e-04,\n",
            "         1.0485e-04, 1.2309e-07, 1.5325e-03, 1.0844e-03, 2.1189e-02, 5.7349e-07,\n",
            "         1.6690e-06, 1.8420e-03, 2.1177e-04, 1.3098e-03, 2.9946e-04, 2.0876e-05,\n",
            "         2.1342e-04, 1.4792e-04, 2.4380e-05, 1.6891e-05, 3.4869e-05, 1.7264e-02,\n",
            "         1.2621e-03, 1.7604e-03, 1.7064e-04, 6.7255e-05, 7.4954e-04, 1.6332e-04,\n",
            "         1.1142e-04, 4.9184e-04, 6.7462e-04, 1.5406e-02, 4.6959e-05, 3.7598e-04,\n",
            "         4.4359e-05, 2.9294e-05, 1.1804e-04, 1.3615e-04, 2.7412e-03, 6.8589e-05,\n",
            "         1.7150e-04, 1.0845e-05, 3.7675e-04, 1.5465e-03, 2.3271e-04, 5.1179e-10,\n",
            "         5.8905e-05, 2.0981e-03, 1.8812e-04, 1.2163e-02]])}, 139938764448680: {'step': 4690, 'exp_avg': tensor([ 6.7802e-03, -1.5579e-03,  9.7195e-03, -7.1497e-03, -1.6484e-02,\n",
            "         9.7807e-03, -4.8916e-03,  5.7760e-03, -2.0609e-03,  8.7392e-05]), 'exp_avg_sq': tensor([0.0003, 0.0004, 0.0005, 0.0006, 0.0005, 0.0007, 0.0004, 0.0005, 0.0008,\n",
            "        0.0007])}}\n",
            "param_groups \t [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [139938764448248, 139938764448320, 139938764448032, 139938764448392, 139938764448464, 139938764448536, 139938764448608, 139938764448680]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "98bee7eaf09d225730006f2b24ca5662d28cfb65",
        "id": "umGzguf86zWB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kaggle- Multilayered Perceptron (MLP) implemention on MNIST dataset\n",
        "Untill now we were using the MNIST dataset that is available in torchvision.dataset.Let us now load the dataset from Kaggle repo and train our model"
      ]
    },
    {
      "metadata": {
        "_uuid": "4e83a9e422c10eb07cbfb779be01803d2b8a5334",
        "id": "SMGmxugC6zWB",
        "colab_type": "code",
        "colab": {},
        "outputId": "4de5d799-ec0b-453f-c313-261041ef4bd7"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset ,DataLoader\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "PATH=Path(\"../input/digit-recognizer\")\n",
        "print(os.listdir(\"../input/digit-recognizer\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train.csv', 'sample_submission.csv', 'test.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "661e23266c2c882d34cdae4c9074d26c0d2ac040",
        "id": "oFtrSx2_6zWF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data "
      ]
    },
    {
      "metadata": {
        "_uuid": "72b61aac15c29fc295d77130b07d1110c9cb1825",
        "id": "sAa_XHzn6zWF",
        "colab_type": "code",
        "colab": {},
        "outputId": "9299688f-a7fa-418e-f470-1ff6710fd78c"
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(PATH/'train.csv')\n",
        "test=pd.read_csv(PATH/'test.csv')\n",
        "train.shape,test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42000, 785), (28000, 784))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6d5eb02dbfc5e385ffd113560494e0b2275e5f66",
        "id": "V1iXJBdp6zWH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Input and Target Variable"
      ]
    },
    {
      "metadata": {
        "_uuid": "858a074c0e1dea92562d0f1bd93ff5302f861643",
        "id": "6A_S5NWh6zWI",
        "colab_type": "code",
        "colab": {},
        "outputId": "f4df5540-5b8c-449f-fb6a-4dba98e8d1e6"
      },
      "cell_type": "code",
      "source": [
        "x=train.drop(\"label\",axis=1)\n",
        "y=np.array(train['label'])\n",
        "x.shape,y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42000, 784), (42000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "387109d77b8c6f76d4de868158340faf1a38b2dc",
        "id": "mIoV0X316zWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalization "
      ]
    },
    {
      "metadata": {
        "_uuid": "b8ff683f50227c31ee39ec05a7bb8b4e2a8c5f94",
        "id": "bSLE_a1i6zWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_train=x/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "614e6245bdf2e3372dca00d5111b3fbe8e993a64",
        "id": "3SFwirEs6zWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "metadata": {
        "_uuid": "65e189e8b8c6232146d0dde1fceb773353ad8389",
        "id": "42JRzNu86zWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f37069db924850a07cd3e98378ae9c61bab1f8f",
        "id": "ebKwcFUX6zWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Test in Pytorch"
      ]
    },
    {
      "metadata": {
        "_uuid": "709aaea373c664fad6fd42d8779ae091145b8308",
        "id": "5FITJatH6zWu",
        "colab_type": "code",
        "colab": {},
        "outputId": "18b39311-80fb-4907-8d08-7af02fe81055"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "# create feature and targets tensor for train set.\n",
        "torch_X_train = torch.from_numpy(x_train.values).type(torch.FloatTensor)\n",
        "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
        "\n",
        "# create feature and targets tensor for test set.\n",
        "torch_X_test = torch.from_numpy(x_test.values).type(torch.FloatTensor)\n",
        "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
        "\n",
        "# Pytorch train and test sets\n",
        "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
        "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# create feature and targets tensor for train set.\\ntorch_X_train = torch.from_numpy(x_train.values).type(torch.FloatTensor)\\ntorch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\\n\\n# create feature and targets tensor for test set.\\ntorch_X_test = torch.from_numpy(x_test.values).type(torch.FloatTensor)\\ntorch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\\n\\n# Pytorch train and test sets\\ntrain = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\\ntest = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "80525db377fe319e7a3fc538ef8b76639c28c995",
        "id": "HCOVINHL6zW0",
        "colab_type": "code",
        "colab": {},
        "outputId": "8d3c12cc-a0fd-4324-d310-b0afa7367e82"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "BATCH_SIZE=64\n",
        "# data loader\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBATCH_SIZE=64\\n# data loader\\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\\ntest_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "454f4fb8e0a416e60aed471a49850c01a64a77e6",
        "id": "stREHomR6zW7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train -Test Split -Pytorch"
      ]
    },
    {
      "metadata": {
        "_uuid": "d616496f4e453322efe8aecfa71b17fcdddaa8dd",
        "id": "5Lf3MfUA6zW7",
        "colab_type": "code",
        "colab": {},
        "outputId": "072d417a-7e2d-4738-965c-0e187834f46d"
      },
      "cell_type": "code",
      "source": [
        "torch_X_train = torch.from_numpy(x.values).type(torch.FloatTensor)/255\n",
        "torch_y_train = torch.from_numpy(y).type(torch.LongTensor)\n",
        "myDataset = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
        "valid_no  = int(0.2 * len(myDataset))\n",
        "# so divide the data into trainset and testset\n",
        "trainSet,testSet = torch.utils.data.random_split(myDataset,(len(myDataset)-valid_no,valid_no))\n",
        "print(f\"len of trainSet {len(trainSet)} , len of testSet {len(testSet)}\")\n",
        "batch_size=64\n",
        "train_loader  = DataLoader(trainSet , batch_size=batch_size ,shuffle=True) \n",
        "test_loader  = DataLoader(testSet , batch_size=batch_size ,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of trainSet 33600 , len of testSet 8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a06249ad57104d74d0258fd5972961f720bdc3cf",
        "id": "h03DNkSb6zW_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "trainData = torch.from_numpy(x_train.values)\n",
        "trainLabel=torch.from_numpy(y_train)\n",
        "testData = torch.from_numpy(x_test.values)\n",
        "testLabel = torch.from_numpy(y_test)\n",
        "trainData, testData = trainData.type(torch.FloatTensor), testData.type(torch.LongTensor)\n",
        "trainLabel, testLabel = trainLabel.type(torch.FloatTensor), testLabel.type(torch.LongTensor)\n",
        "trainData.shape,testData.shape\n",
        "trainData = trainData.unsqueeze_(dim=1)\n",
        "testData = testData.unsqueeze_(dim=1)\n",
        "trainData.shape,testData.shape\n",
        "transforms =transforms.Compose(transforms.ToTensor())\n",
        "train_dataset = TensorDataset(trainData,trainLabel)\n",
        "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(testData,testLabel)\n",
        "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=True)"
      ]
    },
    {
      "metadata": {
        "_uuid": "97b2925a24f59b10a16864a76f00e02a4c92b36f",
        "id": "9sGYIda-6zXA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Network"
      ]
    },
    {
      "metadata": {
        "_uuid": "8cbe1c508bacadbb875014318a35fed17ab6a3a1",
        "id": "MdK3eiaB6zXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model=Network()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "criterion=nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bea512cd5bd9f4f41bd77044f471e644505a5fa",
        "id": "W51KK9Rr6zXJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train "
      ]
    },
    {
      "metadata": {
        "_uuid": "2df4882ed86f9b17b4bba52d56adfde46d1f718d",
        "id": "f1SHhCz_6zXT",
        "colab_type": "code",
        "colab": {},
        "outputId": "d74df2b4-a456-49c8-9cb1-8552d27b90d5"
      },
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        log_ps=model(images)\n",
        "        loss=criterion(log_ps,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images,labels in test_loader:\n",
        "                log_ps=model(images)\n",
        "                test_loss+=criterion(log_ps,labels)\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(train_loader))\n",
        "        test_losses.append(test_loss/len(test_loader))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5..  Training Loss: 0.446..  Test Loss: 0.214..  Test Accuracy: 0.943\n",
            "Epoch: 2/5..  Training Loss: 0.307..  Test Loss: 0.192..  Test Accuracy: 0.953\n",
            "Epoch: 3/5..  Training Loss: 0.286..  Test Loss: 0.231..  Test Accuracy: 0.948\n",
            "Epoch: 4/5..  Training Loss: 0.280..  Test Loss: 0.199..  Test Accuracy: 0.949\n",
            "Epoch: 5/5..  Training Loss: 0.267..  Test Loss: 0.206..  Test Accuracy: 0.953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9373aea13cc5891684fb8b801cbcd0ac17eff458",
        "id": "vj7oBCSR6zXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save our model"
      ]
    },
    {
      "metadata": {
        "_uuid": "bfcc3f17cadca0bf48cec130586b907965905ed6",
        "id": "olWbvyWR6zXc",
        "colab_type": "code",
        "colab": {},
        "outputId": "7b30654a-eb6a-449b-8522-adaea5e23ac5"
      },
      "cell_type": "code",
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " Network(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f83335753469344d38ac362053a74fad30b0ca3e",
        "id": "z6m5R8A06zXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "70b1a508fd748a8823a9e1af57538e4cacb0621d",
        "id": "0U9OyLVg6zXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load our model"
      ]
    },
    {
      "metadata": {
        "_uuid": "abd854b1c8bfed532cb4e64578c40fd29dea9a36",
        "id": "lycXnyCF6zXn",
        "colab_type": "code",
        "colab": {},
        "outputId": "05f9b551-3410-40de-9357-74a72604b83a"
      },
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7a889737952161eb1834f521476f0f1c8448570a",
        "id": "BdtvPufJ6zXr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a08eff1adeb3a8f8f7a31356828ee732a557c3d5",
        "id": "S_5ep7676zXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = {'input_size': 784,\n",
        "              'output_size': 10,\n",
        "              'hidden_layers': [256,128,64],\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3356c9580d6c01685a52a11f906ee1c9dbe7ef1",
        "id": "mxSNKq6f6zXw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "metadata": {
        "_uuid": "34abb15aa48ad4f133ee15a2f9a5268b45692c36",
        "id": "Jj7gSqQD6zXx",
        "colab_type": "code",
        "colab": {},
        "outputId": "32c14468-7580-40df-d01b-02d4ccb9eca6"
      },
      "cell_type": "code",
      "source": [
        "test_images = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n",
        "test_image = test_images.loc[:,test_images.columns != \"label\"].values\n",
        "test_dataset = torch.from_numpy(test_image).type(torch.FloatTensor)/255\n",
        "print(test_dataset.shape)\n",
        "#test_dataset = torch.utils.data.TensorDataset(test_dataset)\n",
        "new_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28000, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "790dd7f94c59a6de5441827b999b6cff6f686154",
        "id": "wT6QdpEy6zX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images in new_test_loader:\n",
        "        output = model(images)\n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        results += top_class.numpy().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f9b334b1c25c4bb1a48fda6b64634f1f90b7565",
        "id": "3aICZH416zX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check the results"
      ]
    },
    {
      "metadata": {
        "_uuid": "56ffd3051274596232af3ca4d1a9ee7d6322881a",
        "id": "CpSWGPLW6zX5",
        "colab_type": "code",
        "colab": {},
        "outputId": "b97a83e5-2339-4957-e3f2-e1e9c5440318"
      },
      "cell_type": "code",
      "source": [
        "predictions = np.array(results).flatten()\n",
        "print(predictions[:5])\n",
        "print(predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 0 9 8 3]\n",
            "(28000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f946013a3eab6274d1becd93dfe99aa9f7491cf9",
        "id": "QrdlPpyU6zYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submit for Scoring"
      ]
    },
    {
      "metadata": {
        "_uuid": "655160a6e2d490651c0fe70b8ba7480ed8ba1fcc",
        "id": "05dypqL36zYC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"my_submissions.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "177c548a264bbaaa76e216eeaf1d747db88b1030",
        "id": "LElvvLvz6zYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "[Introduction to Pytorch-Udacity](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch)"
      ]
    }
  ]
}