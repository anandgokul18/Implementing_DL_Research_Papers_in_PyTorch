{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To get a good understanding of how Cost function works and its important role in training the neural networks we need to take a step back and understand **Maximum Likelihood.**\n",
    "Models generally define a distribution **$p(y | x;Î¸ )$** and to do that a simple way is to use the principle of maximum likelihood.Most modern neural networks are trained using maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get started with this example\n",
    "<br>\n",
    "**Here our goal is to split data correctly**\n",
    "![](images/error_function.PNG)\n",
    "\n",
    "So to start with we need to define an error function that can help us to move this line so that all the points are correctly classified.But before that let us look what kind of functions can be used as an Error Function\n",
    "\n",
    "**Method 1**:**Discrete Measure**-Counting the number of mistakes it made.\n",
    "\n",
    "However there is problem in this approach.The algorithm will be taking smaller steps which are calculated by derivatives.So a small step might not be able to reduce the error size and then it is confused in which direction to move to reduce the error.\n",
    "\n",
    "**Method 2**:**Continuos Measure- Assigning penalty to each point.**\n",
    "We can assign high penalty to the misclassifed points and small to the correctly classifier points\n",
    "\n",
    "So we can calculate Total Error as  the sum of the errors corresponding to each point.\n",
    "\n",
    "**Hence,we prefer Error function to be continuos and differentiable**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete to Continuos\n",
    "\n",
    "\n",
    "As we saw above,we need to define an Error function which is continuos.However our predictions are still discrete(Say 1 for blue and 0 for red).\n",
    "**So how do we move from a discrete prediction to a continuos prediction?**\n",
    "\n",
    "\n",
    "![](images/error_2.PNG)\n",
    "\n",
    "![](https://pbs.twimg.com/media/DYijvWPUQAAZfwD.jpg)\n",
    "\n",
    "**We go for Sigmoid function which is continuos rather than step function which is discrete**\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
    "\n",
    "Earlier our model consited of a line denoting the positive region and the negative region.Now it consists of entire probability space for each point in the plane where label 1 is for blue probability and label 0 for red points.\n",
    "![](images/error_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As now we are dealing with probabilites,let us take a step back and look into some concept of Probabilites.\n",
    "\n",
    "\n",
    "Let us say we got a taks to classifiy points as red or blue.We also got a choice to choose our model based on how they are classifying points\n",
    "![](images/prob.PNG)\n",
    "**Which model will you choose(Left or Right)**\n",
    "<br>\n",
    "Obviously the Right one,since it classifies all the points correctly.\n",
    "\n",
    "**But how can we say the model on the right is better from the Probability perspective?**\n",
    "\n",
    "Let us calculate the probabilites of these points for the Left model\n",
    "![](images/prob1.PNG)\n",
    "\n",
    "Probabilites that two points are red$= 0.1 x 0.7 = 0.07$\n",
    "\n",
    "Probabilites that two points are blue$= 0.6x0.2 = 0.12$\n",
    "\n",
    "And Probability of the whole arrangement is $= 0.07 x 0.12 = 0.0084$\n",
    "\n",
    "So we cans see that this model representation is not able to classify these points accurately as the overall probability of the points of these colors is $<0.1$\n",
    "\n",
    "Now let us look at the Right Model\n",
    "![](images/prob2.PNG)\n",
    "\n",
    "Probability of the whole arrangement is $= 0.8 x 0.6 x 0.9 x0.7 = 0.3024$\n",
    "\n",
    "**Thus we can confirm model on the right is better because it makes the arrangement of the points much more likely to have those colors**\n",
    "\n",
    "So our goal in general is to move from a bad model(Left) to a good model(Right) to maximize this Probability and hence this method is called **Maximum Likelihood**\n",
    "\n",
    "So we concluded here that maximizing the proabability can give us a good model.\n",
    "<br>\n",
    "Also, before we learnt minimizing the error function gives us a good model.\n",
    "<br>\n",
    "So is there a way these two terms are related:\n",
    "<br>\n",
    "**Can Maximizing the probability lead to minimzing the error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Error Function\n",
    "\n",
    "We need to Maximize the Probability of the whole arrangement which is the product of individual probabilities \n",
    "\n",
    "**But we want to stay away from Product**\n",
    "1. Doing product on 1000s of numbers which ranges between 0 and 1 could result in 0.00000.... results.The numbers are too tiny adn we want to stay away from them.\n",
    "2. Changing a number when we have product of 1000s of numbers,the product will change drastically\n",
    "\n",
    "**Products are Bad but Sums are Good.Let's do Sums**\n",
    "\n",
    "**Which function will help us in coverting products to sums?**\n",
    "\n",
    "**Logs**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Binary_logarithm_plot_with_ticks.svg/300px-Binary_logarithm_plot_with_ticks.svg.png)\n",
    "\n",
    "\n",
    "Let us calculate the Error Function calculated above using Logs:\n",
    "\n",
    "**Using Product :**\n",
    "<br>\n",
    "Probability of the whole arrangement is $ 0.8 x 0.6 x 0.9 x0.7 = 0.3024$\n",
    "\n",
    "**Now:**\n",
    "<br>\n",
    "Probability of the whole arrangement is    $= ln(0.8) + ln(0.6) + (0.9) + (0.7) = 0.3024$\n",
    "<br>\n",
    "Probability of the whole arrangement is    $= -0.36 + -0.1 + -0.22 + -0.51$\n",
    "<br>\n",
    "**Let us take the negative of the of logarithm of the probabilites**\n",
    "<br>\n",
    "Probability of the whole arrangement is    $= 0.36 + 0.1 + 0.22 + 0.51 =1.2$\n",
    "\n",
    "**This value is genreally called Cross Entropy**\n",
    "<br>\n",
    "**Lower the Cross Entropy better is the model**\n",
    "\n",
    "![](images/erroR_5.PNG)\n",
    "\n",
    "**So now we can say Our goal has changed from maximizing the probability to minimizing the cross entropy which is essentially our Error function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "In simple terms Cost Function tells us how far we are from the solution.It helps the optimisation algorithm know how to update model parameters.As you can see there is a deep tie between backpropagation algorithm and a cost function.Goal of a backpropagation algorithm is to compute a partial derivative of a cost function with respect to any weight or bias in a model or more generally speaking with respect to any model parameter. In order for backpropagation to work cost function must satisfy these assumptions.\n",
    "\n",
    "1. Be an average of the losses\n",
    "2. Be a function of model outputs\n",
    "3. Assign high value to incorrect values and low values to correct values\n",
    "\n",
    "Lets look at each of those individually and think why does it need to keep that promise.\n",
    "\n",
    "Averaging is used such that cost function is not dependent on the number of training examples, sometimes we might use 100 examples and sometimes 1000 or even more, but a cost function should not depend on that.\n",
    "\n",
    "If output of a model would not be taken into account then there would be no way to compare correct values with predicted and in essence output of a model is a combination of inputs and model parameters, so it would not be possible to optimise model parameters.\n",
    "\n",
    "Backpropagation works with gradients of a cost function with respect to some model parameter, so by assigning higher value to the incorrect prediction we would get steeper gradient at that point and will move quicker to lower plane compared to if cost would not assign error values in the same way. Overall if error surface is relatively flat then learning process will be slower.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Cost Functions\n",
    "\n",
    "\n",
    "\n",
    "**Cross Entropy**\n",
    "![](https://i.stack.imgur.com/7poun.png)\n",
    "**If we have a bunch of events and a bunch of probabilites,then how likely is it that those events happen based on probabilities?**\n",
    "<br>\n",
    "If it is very likely then we have a small cross entropy and if it is unlikely then we have a large cross entropy\n",
    "<br>\n",
    "<BR>\n",
    "    <BR>\n",
    "**For two classes-**\n",
    "**Binary Cross Entropy**\n",
    "\n",
    "![](images/bce.PNG)\n",
    "\n",
    "**For multi class**\n",
    "**Categorical Cross Entropy**\n",
    "![](images/cce.PNG)\n",
    "\n",
    "It is quite similar to BCE but has an extra summation term to sum over all output nodes which in formula is denoted as J.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upcoming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Triplet-loss - multi-class loss from [1503.03832] FaceNet: A Unified Embedding for Face Recognition and Clustering\n",
    "2. Center Loss from this paper\n",
    "3. Angular Softmax from SphereFace: Deep Hypersphere Embedding for Face Recognition\n",
    "4. Custom Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Loss Function](https://medium.com/@dmitrijtichonov/debunking-loss-functions-in-deep-learning-4b9abc4c8d4c)\n",
    "<br>\n",
    "[Udacity](https://classroom.udacity.com/courses/ud188/lessons/b4ca7aaa-b346-43b1-ae7d-20d27b2eab65/concepts/7a42d26d-7d7e-4c76-a014-5bf8b4413179)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
