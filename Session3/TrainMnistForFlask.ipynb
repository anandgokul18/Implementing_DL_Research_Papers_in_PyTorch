{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)#13\n",
    "        x = F.relu(self.conv2(x))#11\n",
    "        x = F.max_pool2d(x, 2, 2)#5*5*50\n",
    "        x = x.view(-1, 5*5*50)#\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument(object):\n",
    "    def __init__(self, *initial_data, **kwargs):\n",
    "        for dictionary in initial_data:\n",
    "            for key in dictionary:\n",
    "                setattr(self, key, dictionary[key])\n",
    "        for key in kwargs:\n",
    "            setattr(self, key, kwargs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_args = {'batch_size':64, 'test_batch_size':1000, 'epochs':5, 'save_model':True, 'lr':0.01, 'log_interval':10, 'momentum':0.5}\n",
    "    \n",
    "args = Argument(dict_args)\n",
    "\n",
    "use_cuda = not False and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "#Train Loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()#,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "#test Loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor()#,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = NN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293756\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.298661\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.282684\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279946\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.266068\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.262856\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.235043\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.225290\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.213677\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181577\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.149939\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.103021\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.058856\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.948759\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.839440\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.766229\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.623598\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.335483\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.069836\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.985036\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.731842\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.801555\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.691284\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.680037\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.445320\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.525458\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.569082\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.433958\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.517970\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.450061\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.504665\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.419239\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.554753\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.408247\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.492694\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.457000\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.304787\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.499651\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.394679\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.399299\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.415203\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.545590\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.379903\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.290406\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.242133\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.413288\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.370526\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.359828\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.367001\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.274561\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.350664\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.168494\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.189157\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.244348\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.325805\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.464853\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.239368\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.334208\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.313796\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.370264\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.656248\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.396756\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.281463\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.312791\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.245271\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.565851\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.290648\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.227653\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.313955\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.212331\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.339603\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.169057\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.160511\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.368120\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.420874\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.183466\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.212252\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.206830\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.186374\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.393680\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.164164\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.195075\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.159131\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.121640\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.261621\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.377524\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.237793\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.229387\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.381672\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.146512\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.295981\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.302583\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.263047\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.323641\n",
      "\n",
      "Test set: Average loss: 0.2159, Accuracy: 9373/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.356980\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.270522\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.243732\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.121648\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.085431\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.189682\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.293011\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.278972\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.116390\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.342271\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.112519\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.179476\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.135786\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.146333\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.366615\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.163379\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.297333\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.095895\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.294499\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.171432\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.150978\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.241542\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.264591\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.360795\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.087079\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.188458\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.221018\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.224304\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.093389\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.126169\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.130647\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.179093\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.147103\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.180104\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.228884\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.281949\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.114218\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.184487\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.284050\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.236401\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.202542\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.272181\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.188731\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.096447\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.217183\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.109599\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.058745\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.210376\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.128765\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.241331\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.240054\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.190799\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.131532\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.159274\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.072700\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.173903\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.103699\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.093776\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.163170\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.085118\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.102569\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.173853\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.072841\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.244852\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.055228\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.092617\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.164424\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.125377\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.040262\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.091970\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.140682\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.151142\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.091241\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.202977\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.139892\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.061105\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.127424\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.177498\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.116749\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.114670\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134293\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.150069\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.089281\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.097961\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.040319\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.088266\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.169606\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.185281\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.162804\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.018754\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.359293\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.235460\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.101665\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.098177\n",
      "\n",
      "Test set: Average loss: 0.1230, Accuracy: 9626/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.177484\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.092470\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.148018\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.113548\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.153913\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.072910\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.064856\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.069661\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.080174\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.162177\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.078931\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.096040\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.072125\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.098166\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.016438\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.056388\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.077207\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.134491\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.115960\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.144701\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.100004\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.308070\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.155195\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.051923\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.131986\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.129060\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.072864\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.167998\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.192769\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.126283\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.107306\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.060628\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.087977\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.102285\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.050668\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.191208\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.127158\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.034418\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.055838\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.075688\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.055085\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.134020\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.035193\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.056049\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.036008\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.088620\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.099159\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.114312\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.140272\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.129762\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.143668\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.139556\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.127806\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.026758\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.120076\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.060815\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.093809\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.121208\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.233539\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.057621\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.036477\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.067131\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.128677\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.072114\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.099589\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.111255\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.128290\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.133964\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.057981\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.081224\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.110994\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.133940\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.070957\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.152914\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.063953\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.048960\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.094854\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.066999\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.073014\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.231596\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.022655\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.035141\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.182179\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.101447\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.077265\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.100653\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.129390\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.128904\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.079980\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.188696\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.119126\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.116192\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.081079\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.076542\n",
      "\n",
      "Test set: Average loss: 0.0836, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.067508\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.060575\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.065163\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.015474\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.034061\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.066772\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.021054\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.034374\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.028106\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.051907\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.049285\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.106345\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.053387\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.049443\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.093980\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.079964\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.051686\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.237756\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.217923\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.078176\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.035159\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.087583\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.096534\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.057130\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.074602\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.128794\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.069587\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.084517\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.043341\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.107122\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.151231\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.038361\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.055557\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.078719\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.101518\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.022512\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.153771\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.109765\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.070218\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.052931\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.065485\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.017709\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.113226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.020364\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.060391\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.114169\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.046408\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.112381\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.081570\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.024183\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.183454\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.063648\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.022916\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.070456\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.167829\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.086851\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.084041\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.026451\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.015320\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.052019\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.068813\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.077089\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.134661\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.039928\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.119019\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.050048\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.116144\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.163167\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.036557\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.126601\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.082975\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.023094\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.197056\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.103361\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.024586\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.047502\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.139991\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.090211\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.080404\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.047001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.110379\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.099197\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.073568\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.034807\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.164203\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.108405\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.067219\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.044920\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.099823\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.048820\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.070220\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.118320\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.086317\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.076642\n",
      "\n",
      "Test set: Average loss: 0.0754, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.137544\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.139241\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.037104\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.118582\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.047641\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.072268\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.025310\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.053564\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.057942\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.056168\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.022590\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.114756\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.064568\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.078915\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.107077\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.082032\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.059729\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.057373\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.173364\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.029591\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.019186\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.200010\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.043512\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.034364\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.060995\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.108847\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.023492\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.105168\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.059572\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.048397\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.151767\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.016339\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.081935\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.060082\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.458775\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.105678\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.145039\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.031401\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.081410\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.091984\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.022856\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.034954\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.127222\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.132374\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.146968\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.021742\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.058661\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.024333\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.041717\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.080465\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.302444\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.079625\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.062528\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.084861\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.035685\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.020301\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.040113\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.062495\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.032535\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.022681\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.035275\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.059750\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.092579\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.100369\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.015415\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.118211\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.018542\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.075360\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.038181\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.056244\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.032562\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.137073\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.109670\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.123357\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.142914\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.171328\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.027416\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.067781\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.092077\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.039659\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.023542\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.160420\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.091871\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.069736\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.067461\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.070711\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.020076\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.031202\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.041702\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.070637\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.060035\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.065312\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.102617\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.137889\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 9810/10000 (98%)\n",
      "\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
    "    print('model saved')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight', tensor([[[[ 0.2173, -0.1457, -0.0472],\n",
       "                        [ 0.1698, -0.3446,  0.1965],\n",
       "                        [-0.0918,  0.1328,  0.0325]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0287,  0.1225,  0.0109],\n",
       "                        [ 0.1853, -0.0514,  0.0128],\n",
       "                        [ 0.0664,  0.1725,  0.0913]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2920,  0.1032, -0.1245],\n",
       "                        [-0.2014, -0.0571, -0.1447],\n",
       "                        [-0.1063,  0.0156,  0.1977]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3205, -0.1145,  0.3317],\n",
       "                        [ 0.1673,  0.4429,  0.3268],\n",
       "                        [-0.4580, -0.4813, -0.2726]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3805,  0.0457,  0.1936],\n",
       "                        [-0.0620,  0.3935, -0.1528],\n",
       "                        [ 0.3280,  0.0301, -0.2189]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1375, -0.1843,  0.1061],\n",
       "                        [-0.1754, -0.0982, -0.3698],\n",
       "                        [-0.2142,  0.1304, -0.1075]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5653,  0.5808,  0.2086],\n",
       "                        [-0.0943,  0.4852,  0.3634],\n",
       "                        [-0.2255,  0.3702,  0.4075]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2939, -0.1870, -0.0549],\n",
       "                        [-0.0076,  0.0483, -0.2530],\n",
       "                        [-0.2367,  0.1810, -0.0785]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3684,  0.2685,  0.3234],\n",
       "                        [ 0.2321,  0.3329,  0.3665],\n",
       "                        [-0.3372,  0.1953, -0.1630]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2639, -0.0141, -0.0710],\n",
       "                        [ 0.2229, -0.1877, -0.2773],\n",
       "                        [ 0.3333,  0.0599,  0.2887]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1831, -0.2500,  0.0402],\n",
       "                        [ 0.1768, -0.2257, -0.1980],\n",
       "                        [ 0.2011,  0.0716, -0.1749]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0215,  0.4396, -0.0038],\n",
       "                        [ 0.2977,  0.2979, -0.1641],\n",
       "                        [-0.1601,  0.4653,  0.0774]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4801,  0.3680,  0.2726],\n",
       "                        [ 0.6270,  0.7666,  0.0294],\n",
       "                        [ 0.3917,  0.6532,  0.5274]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2130, -0.2350, -0.2396],\n",
       "                        [-0.1835,  0.0744,  0.3723],\n",
       "                        [-0.1650,  0.2325, -0.1390]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3055, -0.0262,  0.2631],\n",
       "                        [ 0.5090,  0.0583,  0.3348],\n",
       "                        [ 0.6055,  0.5757,  0.5153]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3111,  0.3148,  0.1928],\n",
       "                        [-0.2269, -0.2531, -0.1157],\n",
       "                        [-0.0410, -0.1944,  0.1424]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3366,  0.2045,  0.3701],\n",
       "                        [ 0.4198,  0.6668,  0.6465],\n",
       "                        [ 0.1074,  0.6822,  0.2932]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0233, -0.3321, -0.2086],\n",
       "                        [-0.2156, -0.1562,  0.2818],\n",
       "                        [-0.1417, -0.2314,  0.2086]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1848,  0.2803, -0.1157],\n",
       "                        [ 0.0510, -0.1579, -0.1158],\n",
       "                        [-0.3204, -0.2238, -0.1879]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4247,  0.3827,  0.3425],\n",
       "                        [ 0.6890,  0.7665,  0.6170],\n",
       "                        [ 0.1635, -0.0337,  0.5076]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-4.3368e-02, -2.0234e-04, -2.8650e-01, -3.1179e-04, -1.4763e-01,\n",
       "                       3.9016e-01, -6.7833e-02, -2.6155e-01,  3.4922e-04, -8.4111e-02,\n",
       "                       1.9442e-01, -3.4621e-02, -3.0676e-05,  6.7921e-02,  1.4479e-01,\n",
       "                      -1.3066e-01,  2.5294e-02,  2.6081e-01,  4.3812e-01,  2.2860e-04])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 1.7924e-02, -5.5196e-02,  6.3524e-02],\n",
       "                        [-2.9344e-02,  4.4951e-02,  2.2321e-03],\n",
       "                        [-7.8055e-03, -2.9311e-03,  1.2548e-02]],\n",
       "              \n",
       "                       [[ 3.4055e-02,  1.1110e-02,  2.2153e-02],\n",
       "                        [-6.8390e-02,  5.4222e-02,  6.4968e-02],\n",
       "                        [ 5.7988e-02,  5.3761e-02, -5.3938e-02]],\n",
       "              \n",
       "                       [[-2.7649e-02,  6.5695e-02, -5.6792e-02],\n",
       "                        [ 6.7622e-02, -5.8623e-02, -5.2503e-02],\n",
       "                        [ 3.6430e-02, -5.3552e-02, -1.7076e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1143e-02,  4.4335e-02,  5.0185e-02],\n",
       "                        [ 6.8143e-02,  1.7778e-02,  3.2052e-02],\n",
       "                        [-6.7497e-03, -4.6905e-02, -7.6816e-02]],\n",
       "              \n",
       "                       [[ 3.6782e-02, -1.9101e-02,  2.2687e-02],\n",
       "                        [-2.7663e-03, -7.6387e-02, -4.3357e-02],\n",
       "                        [ 1.8461e-02, -2.0803e-02, -7.8379e-02]],\n",
       "              \n",
       "                       [[ 1.7483e-02, -3.7343e-03, -2.8344e-02],\n",
       "                        [ 2.6847e-02,  1.5738e-02, -2.7498e-02],\n",
       "                        [ 4.3047e-02,  3.1808e-02,  8.4212e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7535e-02, -6.2387e-02,  6.2678e-02],\n",
       "                        [-2.1662e-02, -4.6633e-02,  5.2156e-02],\n",
       "                        [ 2.8176e-02,  1.9069e-02, -8.5636e-03]],\n",
       "              \n",
       "                       [[-3.1675e-02,  1.0661e-02,  6.6247e-02],\n",
       "                        [-5.4677e-04, -3.6834e-02,  8.2817e-02],\n",
       "                        [-4.2994e-03,  6.3688e-02,  2.6299e-02]],\n",
       "              \n",
       "                       [[ 6.6676e-02,  4.2428e-02, -3.8648e-02],\n",
       "                        [ 5.2263e-02,  6.3815e-05, -6.0430e-02],\n",
       "                        [-1.5762e-02, -6.7638e-02, -3.9107e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.2722e-02, -6.6690e-04, -5.4731e-02],\n",
       "                        [ 5.3834e-02,  1.6098e-02, -5.1268e-02],\n",
       "                        [ 7.7074e-02,  5.2911e-02, -6.2838e-02]],\n",
       "              \n",
       "                       [[-3.3078e-02, -3.6465e-02, -6.1868e-02],\n",
       "                        [ 3.7021e-02, -7.1959e-02,  5.4099e-02],\n",
       "                        [-3.1218e-02,  2.1119e-02, -4.1134e-03]],\n",
       "              \n",
       "                       [[-8.4890e-02,  4.5903e-02, -5.7144e-03],\n",
       "                        [-1.1517e-01,  9.4064e-03,  1.0302e-01],\n",
       "                        [-7.5941e-02, -2.6426e-02, -9.5582e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8651e-02, -2.3383e-02, -1.7529e-02],\n",
       "                        [-5.6707e-02,  1.0393e-02, -3.3487e-03],\n",
       "                        [-6.3447e-02, -5.8124e-02,  5.4438e-03]],\n",
       "              \n",
       "                       [[ 6.0380e-03,  6.4360e-02,  2.1565e-04],\n",
       "                        [-6.6629e-02, -5.1513e-02, -1.5681e-02],\n",
       "                        [-3.9887e-02, -9.7172e-03, -3.0242e-02]],\n",
       "              \n",
       "                       [[ 2.7577e-02, -2.4484e-02,  5.5080e-02],\n",
       "                        [ 1.7190e-02, -2.1897e-02,  6.7984e-02],\n",
       "                        [ 1.9766e-02, -4.9685e-02, -4.0205e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.9518e-02, -6.2794e-02, -1.9050e-02],\n",
       "                        [-3.3629e-02, -2.9144e-03, -1.1508e-02],\n",
       "                        [-3.6058e-02,  4.9829e-02,  5.2972e-02]],\n",
       "              \n",
       "                       [[ 2.8139e-03, -9.3924e-02,  1.2907e-02],\n",
       "                        [ 7.6511e-02,  4.3925e-02, -3.9204e-03],\n",
       "                        [ 2.5583e-02, -5.1704e-03,  9.8323e-02]],\n",
       "              \n",
       "                       [[ 1.4136e-01,  6.9561e-02,  2.6665e-02],\n",
       "                        [ 8.1537e-02,  1.3100e-01,  1.6761e-01],\n",
       "                        [-2.1587e-01, -1.0234e-01, -9.6817e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.9466e-02, -7.0076e-03, -1.2081e-02],\n",
       "                        [ 6.4718e-02, -4.6380e-02,  4.5843e-02],\n",
       "                        [-6.7410e-02,  1.8660e-02, -6.5023e-02]],\n",
       "              \n",
       "                       [[-3.8955e-02,  4.3240e-02,  5.4266e-02],\n",
       "                        [-1.6360e-02, -6.4461e-02, -5.2736e-02],\n",
       "                        [ 4.5615e-03,  3.4257e-02,  2.8062e-02]],\n",
       "              \n",
       "                       [[-7.2496e-02, -3.4409e-02,  3.0106e-02],\n",
       "                        [ 5.2988e-02,  3.8166e-02, -1.7276e-02],\n",
       "                        [-8.9605e-03, -2.4365e-02, -2.6022e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.4448e-02, -2.8275e-02,  1.1550e-02],\n",
       "                        [ 3.2970e-02, -4.5902e-02, -6.7650e-02],\n",
       "                        [ 1.1479e-02, -3.8366e-03, -6.0451e-02]],\n",
       "              \n",
       "                       [[-1.9967e-02, -7.2726e-02, -1.5162e-02],\n",
       "                        [ 5.9522e-02, -1.8646e-02,  2.7535e-03],\n",
       "                        [ 2.9730e-02, -3.3550e-02,  6.6663e-02]],\n",
       "              \n",
       "                       [[ 4.5101e-02,  1.0180e-01,  1.6618e-02],\n",
       "                        [-1.4881e-02, -6.5941e-02,  5.4639e-02],\n",
       "                        [-4.6326e-02, -1.0192e-01, -5.4682e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9967e-02, -2.8954e-02,  4.9880e-02],\n",
       "                        [ 2.3555e-02, -3.7594e-02,  1.1746e-02],\n",
       "                        [-5.6200e-02, -5.2369e-02, -1.5110e-02]],\n",
       "              \n",
       "                       [[-3.1547e-02,  8.9900e-03,  2.2250e-02],\n",
       "                        [-6.1582e-02, -2.8350e-02, -2.9409e-02],\n",
       "                        [-5.1766e-02,  3.3211e-02, -6.3380e-04]],\n",
       "              \n",
       "                       [[-1.1092e-02, -2.9853e-02,  7.4259e-02],\n",
       "                        [-6.4806e-02, -4.0382e-02, -6.7471e-02],\n",
       "                        [ 2.5595e-02, -5.3743e-02,  1.0372e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.9383e-02, -5.4649e-02, -5.2371e-02],\n",
       "                        [ 6.8576e-02,  4.0551e-02, -3.9841e-02],\n",
       "                        [ 3.8701e-02,  6.1034e-02, -4.0938e-02]],\n",
       "              \n",
       "                       [[ 3.7360e-02, -3.4675e-02,  2.6954e-02],\n",
       "                        [-6.0940e-02, -2.8442e-02,  3.7132e-02],\n",
       "                        [-5.3749e-02, -2.7328e-02,  5.1605e-02]],\n",
       "              \n",
       "                       [[-5.6194e-02,  7.8706e-02,  6.5008e-02],\n",
       "                        [ 5.3982e-02,  7.5680e-02, -2.1018e-02],\n",
       "                        [-4.3796e-02, -3.7072e-02, -9.1583e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9874e-02,  2.0266e-02, -5.8858e-02],\n",
       "                        [-5.2457e-02,  2.6356e-02, -1.8467e-02],\n",
       "                        [ 3.4663e-02,  1.1389e-02, -8.9124e-03]],\n",
       "              \n",
       "                       [[-6.6879e-02,  2.0915e-02,  2.1878e-02],\n",
       "                        [-4.7812e-02,  2.1701e-02,  7.0217e-02],\n",
       "                        [-1.2722e-03,  1.8076e-02,  8.3152e-03]],\n",
       "              \n",
       "                       [[ 1.4099e-02, -4.1381e-02,  2.0476e-02],\n",
       "                        [-4.9874e-02, -5.8028e-02,  6.3146e-02],\n",
       "                        [ 5.6219e-02,  6.8585e-02, -4.4483e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.5911e-03,  3.6908e-02, -4.0237e-02],\n",
       "                        [-3.6657e-02, -5.5051e-02, -3.3583e-02],\n",
       "                        [ 5.2567e-02,  7.4371e-02,  6.8518e-02]],\n",
       "              \n",
       "                       [[ 1.8640e-02, -3.1024e-02, -9.3747e-03],\n",
       "                        [ 3.5233e-02,  3.5066e-02, -3.2202e-02],\n",
       "                        [-1.1039e-02, -7.8103e-02,  2.5050e-02]],\n",
       "              \n",
       "                       [[-3.6106e-02, -9.9178e-02,  1.7497e-02],\n",
       "                        [-4.0311e-02, -5.9954e-02,  1.3445e-01],\n",
       "                        [-6.2236e-02,  8.0128e-02,  3.6119e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.0644, -0.0235,  0.0120, -0.0228,  0.0640, -0.0608, -0.0386, -0.0363,\n",
       "                      -0.0585, -0.0596, -0.0403,  0.0273, -0.0371,  0.0849,  0.0636, -0.0437,\n",
       "                       0.0573, -0.0064,  0.0547,  0.0201,  0.0151, -0.0234,  0.0332, -0.0093,\n",
       "                       0.0047, -0.0020, -0.0697,  0.0491, -0.0650,  0.0597, -0.0545,  0.0104,\n",
       "                       0.0367, -0.0681, -0.0427,  0.0382, -0.0542,  0.0760,  0.0337, -0.0428,\n",
       "                      -0.0655, -0.0691, -0.0211, -0.0303,  0.0496, -0.0564, -0.0841,  0.0104,\n",
       "                       0.0019,  0.0762])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0248,  0.0064,  0.0119,  ...,  0.0199,  0.0261,  0.0129],\n",
       "                      [-0.0005,  0.0044, -0.0262,  ...,  0.0130,  0.0186, -0.0074],\n",
       "                      [ 0.0010,  0.0219, -0.0211,  ..., -0.0299,  0.0203,  0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0150, -0.0205, -0.0213,  ...,  0.0005,  0.0234, -0.0245],\n",
       "                      [ 0.0225,  0.0164,  0.0119,  ..., -0.0034, -0.0123,  0.0153],\n",
       "                      [-0.0257,  0.0218, -0.0026,  ...,  0.0124, -0.0185, -0.0256]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-2.2620e-02,  1.8668e-02, -9.1381e-03,  2.6241e-03, -1.5070e-02,\n",
       "                       2.3038e-02,  9.1841e-03, -1.4248e-02,  2.3583e-02,  3.1428e-02,\n",
       "                      -7.7191e-03, -1.7598e-02, -2.1006e-02,  5.6508e-03,  3.1430e-02,\n",
       "                      -2.4965e-03,  2.1895e-02, -2.0426e-02,  1.3958e-02,  2.2857e-02,\n",
       "                       3.7659e-03,  1.1707e-02, -1.3399e-02, -9.7188e-03, -6.8364e-03,\n",
       "                       2.6119e-02,  3.6171e-03, -1.2546e-02, -2.3483e-02,  1.6955e-02,\n",
       "                      -1.8563e-02, -4.0539e-03,  1.6108e-02, -2.6207e-02,  2.5666e-02,\n",
       "                      -2.6198e-02, -3.2429e-02, -1.3962e-02,  2.6450e-02,  2.4104e-02,\n",
       "                      -1.6585e-02,  3.0539e-02, -2.5758e-02,  5.2773e-03,  3.3262e-02,\n",
       "                       3.4374e-02, -2.5085e-02, -2.3442e-02,  2.9464e-03,  1.9285e-02,\n",
       "                       1.8314e-02,  2.8583e-02, -2.6011e-02, -2.7782e-02,  6.2600e-03,\n",
       "                      -1.6279e-02, -2.6597e-02, -2.0331e-02,  1.1224e-02,  2.9951e-03,\n",
       "                       2.0916e-02,  3.5110e-02, -4.0070e-03, -2.6774e-02, -2.2465e-02,\n",
       "                       1.2083e-02, -1.7015e-02,  2.6030e-02,  3.7587e-03,  2.0372e-02,\n",
       "                       8.0882e-03,  2.9444e-03,  3.6506e-03, -3.0610e-02,  3.1808e-02,\n",
       "                      -2.1240e-02,  9.9804e-03, -1.3824e-02,  1.0150e-02,  1.0054e-02,\n",
       "                      -1.0405e-02,  2.4617e-02,  2.7296e-04,  1.1806e-02,  2.7842e-02,\n",
       "                      -1.5127e-02,  4.9895e-03,  1.5006e-02, -1.5128e-02, -4.5373e-03,\n",
       "                       2.0055e-02, -2.2482e-02, -1.9083e-02, -2.4454e-02, -1.8281e-02,\n",
       "                       1.0260e-02, -6.5457e-03, -5.7198e-03, -1.9842e-02, -2.2518e-02,\n",
       "                      -7.5828e-03,  1.0376e-02,  4.5078e-03, -2.3782e-02,  2.9827e-02,\n",
       "                      -1.4051e-02,  2.3097e-02, -2.2393e-02, -1.3554e-02,  6.7367e-03,\n",
       "                      -6.5360e-03,  1.0862e-02, -5.7334e-03,  4.7377e-03, -2.6559e-02,\n",
       "                       1.5041e-02,  1.3699e-02, -1.2956e-02, -1.8701e-02,  2.8366e-03,\n",
       "                       2.5203e-02,  1.5264e-02,  1.2893e-02, -1.7671e-02,  7.1385e-04,\n",
       "                      -1.5752e-02,  4.7018e-03,  1.3319e-02, -2.2703e-02,  6.1877e-03,\n",
       "                      -2.4996e-02,  7.0401e-03, -2.7933e-02,  3.0354e-02, -5.1024e-03,\n",
       "                      -2.8330e-02, -2.5473e-02, -1.2317e-02, -1.8090e-02, -1.9478e-02,\n",
       "                       9.6319e-03,  8.1339e-03,  1.6871e-02,  1.0191e-02, -8.3284e-03,\n",
       "                      -2.2354e-02,  3.0219e-02,  2.8536e-02,  3.0696e-02,  2.2361e-02,\n",
       "                      -2.2056e-02,  1.8852e-02,  1.0742e-02,  1.1333e-03, -1.2179e-02,\n",
       "                      -8.6105e-03,  1.6683e-02, -2.4048e-02,  1.0621e-02,  2.0466e-02,\n",
       "                      -3.8937e-03,  1.4346e-02,  2.9532e-02, -2.5974e-02,  2.1092e-02,\n",
       "                      -1.3737e-02,  2.1986e-02, -2.2103e-02,  5.8607e-03,  2.5783e-03,\n",
       "                       1.3189e-02, -1.5829e-02,  9.0409e-04, -2.2387e-02, -1.8733e-02,\n",
       "                      -1.9862e-02, -2.2356e-02,  8.1948e-03,  6.1465e-03, -9.1312e-03,\n",
       "                      -2.7452e-02,  3.0796e-04,  1.8151e-02, -2.7960e-02,  1.8288e-02,\n",
       "                      -4.3620e-03,  1.0715e-02, -2.1764e-02,  9.3278e-03,  1.6350e-02,\n",
       "                      -7.7468e-03,  2.6842e-02, -2.6429e-02, -1.2667e-02,  1.2284e-02,\n",
       "                      -1.8697e-02, -1.9072e-02,  3.1831e-02,  1.8352e-02,  2.4225e-02,\n",
       "                       1.3102e-02,  1.1478e-02,  1.9624e-02,  2.5817e-03,  2.4365e-02,\n",
       "                      -1.0816e-02, -2.7552e-02, -6.3841e-03,  1.7944e-02,  2.2453e-02,\n",
       "                      -1.8959e-02,  7.4738e-03,  9.1900e-03, -2.7006e-02, -7.7359e-06,\n",
       "                      -3.9713e-03,  2.0675e-02,  4.3061e-03, -2.1922e-02,  1.7501e-02,\n",
       "                       3.3350e-03,  1.2867e-02, -9.9136e-03, -8.8770e-03,  7.7523e-03,\n",
       "                       3.4730e-03,  2.3574e-02,  5.0153e-03,  1.0147e-02, -9.7457e-04,\n",
       "                       8.8190e-03, -2.0173e-02,  1.5555e-02, -5.2576e-03, -1.0533e-02,\n",
       "                      -8.2505e-03, -1.4611e-02,  1.1489e-03,  3.3381e-02, -1.3897e-03,\n",
       "                       7.3983e-03, -2.1813e-02, -1.0177e-02, -2.1495e-02, -4.0198e-03,\n",
       "                      -1.8043e-02, -2.1018e-02, -1.6048e-02, -5.5847e-03,  1.4082e-02,\n",
       "                       1.8502e-02, -4.6204e-03,  1.2379e-02, -8.4843e-03,  9.2427e-03,\n",
       "                      -1.3940e-02,  4.1127e-03, -5.1527e-03,  2.6357e-02, -2.0299e-02,\n",
       "                       1.9733e-02,  1.1528e-02,  1.7575e-02, -2.2291e-02, -1.8832e-02,\n",
       "                      -1.3831e-03,  2.2996e-02,  1.8788e-02, -2.4406e-02, -8.0016e-03,\n",
       "                      -2.8430e-03,  5.3163e-04,  8.0692e-03,  1.7586e-02, -8.5248e-03,\n",
       "                      -2.0823e-02, -1.0040e-02, -2.4092e-02,  9.3057e-03, -1.7870e-02,\n",
       "                      -2.9120e-02,  1.0445e-02,  1.1691e-02,  1.6346e-02,  2.0979e-02,\n",
       "                      -1.1874e-02, -8.9437e-03, -1.2360e-02,  3.2556e-02, -1.7362e-02,\n",
       "                       3.5986e-03,  4.2862e-03,  8.2236e-03,  3.7935e-03,  4.7194e-03,\n",
       "                      -1.3470e-02, -1.7365e-02,  2.0132e-02,  8.2339e-03,  2.8845e-02,\n",
       "                      -2.2130e-02,  1.3967e-03,  1.7970e-02,  2.7227e-02,  1.7706e-02,\n",
       "                      -1.8007e-02, -4.2428e-03,  4.7344e-03,  1.7404e-03,  7.1902e-03,\n",
       "                       1.9126e-02,  6.6462e-03, -1.5315e-02, -3.0769e-02,  2.2023e-02,\n",
       "                       8.8205e-03,  2.4588e-02,  1.1967e-02, -2.0810e-02, -1.0518e-02,\n",
       "                       2.5190e-02, -2.0709e-02, -1.4549e-02, -6.3494e-04, -1.0063e-02,\n",
       "                      -2.8593e-02, -1.7768e-02,  1.7945e-02, -1.9361e-02, -7.5678e-03,\n",
       "                      -1.3953e-03,  1.4597e-02, -8.6028e-03,  2.6429e-02, -2.5476e-02,\n",
       "                       6.4321e-03,  1.4243e-02,  3.4403e-03, -2.5900e-02,  1.0578e-02,\n",
       "                       1.8644e-02,  1.5219e-02,  1.3234e-02, -2.2664e-02, -1.9680e-02,\n",
       "                      -2.9074e-03,  2.5300e-02,  2.6284e-02, -6.9562e-03, -1.8950e-02,\n",
       "                      -6.9033e-03,  1.5055e-02, -4.8220e-03, -5.9294e-03,  1.6631e-02,\n",
       "                      -1.8264e-02, -2.1497e-02,  2.0512e-02, -2.7331e-02,  2.4358e-02,\n",
       "                      -2.8192e-02,  4.2312e-03, -1.7125e-02,  1.5567e-02, -4.0521e-03,\n",
       "                      -1.0237e-04,  2.7532e-02,  1.4242e-02, -4.9924e-03, -1.5109e-02,\n",
       "                      -7.8090e-03,  1.7053e-02, -3.4911e-03, -2.3235e-02,  1.9658e-02,\n",
       "                      -9.9789e-03,  2.6251e-02, -6.9072e-03,  2.5117e-02, -4.5504e-05,\n",
       "                      -2.2215e-02, -8.7194e-03, -1.2664e-03, -9.5892e-03,  2.0977e-02,\n",
       "                       1.9834e-02,  1.6540e-02,  1.9419e-02, -1.7029e-02, -2.1919e-02,\n",
       "                       1.0455e-02,  2.3051e-02, -3.3404e-03,  4.4170e-03, -2.4759e-02,\n",
       "                      -1.9209e-02,  2.1571e-02,  2.2529e-02, -2.7191e-02,  2.4084e-02,\n",
       "                      -1.0267e-02, -7.5428e-03,  8.6810e-03,  2.5663e-02, -1.5766e-02,\n",
       "                       1.8764e-02, -4.4006e-03,  8.6696e-03, -2.5445e-02, -1.8916e-02,\n",
       "                      -2.1461e-02, -6.5903e-03, -1.4252e-02, -8.5121e-03, -3.1332e-03,\n",
       "                       2.1915e-02,  2.4375e-02, -8.9111e-03, -6.9376e-03,  8.8141e-03,\n",
       "                       1.1711e-02,  1.9778e-04,  9.0330e-03,  2.0724e-02, -6.9018e-03,\n",
       "                       1.4242e-02,  1.7033e-02,  1.7341e-02,  1.3938e-02,  3.4368e-03,\n",
       "                      -2.3723e-02, -1.5929e-02,  1.4961e-02, -1.9349e-02,  2.1171e-02,\n",
       "                       1.3744e-02,  1.7573e-02,  1.7653e-02,  6.7528e-03,  1.8081e-02,\n",
       "                      -1.6843e-03, -2.3947e-02, -1.5380e-02,  1.1106e-02, -1.2125e-02,\n",
       "                      -5.7192e-03,  9.5153e-03,  1.7357e-02, -1.3549e-02, -2.8177e-02,\n",
       "                       1.1660e-02, -7.8184e-03,  2.0808e-02,  7.1636e-03, -1.0945e-02,\n",
       "                      -7.0617e-03, -1.5989e-02, -2.6141e-02,  1.6459e-02, -1.5848e-02,\n",
       "                      -2.0567e-03, -4.1412e-03,  2.6748e-02,  7.4867e-03,  1.3427e-02,\n",
       "                      -1.8095e-02,  1.8153e-02, -8.8570e-03, -8.6354e-03,  6.9716e-03,\n",
       "                       2.4907e-02, -1.0041e-02,  2.9358e-02,  7.5174e-03, -2.6224e-03,\n",
       "                      -1.0617e-02, -5.1627e-04, -2.2118e-02, -1.8117e-02, -1.5597e-02,\n",
       "                      -1.5346e-02,  1.6617e-03,  2.7242e-02,  3.4583e-03,  3.5666e-03,\n",
       "                       7.1135e-03,  4.9404e-03,  6.7858e-03,  2.4601e-02, -1.1730e-02,\n",
       "                       2.8738e-02,  2.2563e-02,  5.4341e-03, -9.4271e-03,  1.9870e-02,\n",
       "                      -2.6460e-02,  1.7716e-02, -4.4090e-03, -7.7836e-03, -1.3590e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0295, -0.0169,  0.0072,  ..., -0.1124,  0.0580, -0.0556],\n",
       "                      [ 0.0054,  0.0929, -0.1428,  ..., -0.0521,  0.0450,  0.0706],\n",
       "                      [-0.0496,  0.0080,  0.0057,  ...,  0.0840,  0.0490,  0.0944],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0511,  0.0696,  ...,  0.1006,  0.0253,  0.0619],\n",
       "                      [-0.0734, -0.0795,  0.0413,  ..., -0.0218, -0.0305, -0.0306],\n",
       "                      [ 0.0905,  0.0364,  0.0229,  ..., -0.1184, -0.0007, -0.0017]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-5.6293e-02,  5.9484e-02, -6.4281e-02, -7.3216e-05, -3.1389e-02,\n",
       "                       2.3380e-02,  5.1528e-03, -1.2511e-02,  3.4379e-02,  2.0154e-02]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def result():\n",
    "    img = Image.open(\"draw.png\")\n",
    "    plt.imshow(img, cmap='gray', interpolation='none')\n",
    "    img = np.array(img)/255\n",
    "    img = np.around(img, decimals = 4) \n",
    "    img = np.dot(img[...,:4],[0, 0, 0, 1])\n",
    "    img = torch.Tensor(img)\n",
    "    img = torch.unsqueeze(img , 0)\n",
    "    img = torch.unsqueeze(img , 0)\n",
    "    prediction = inference(img)\n",
    "    print(prediction)\n",
    "result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img):\n",
    "  output = model(img)\n",
    "  output = torch.exp(output)\n",
    "  top_prob,top_class=output.topk(1,dim=1)\n",
    "  return top_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running on colab\n",
    "from google.colab import files\n",
    "files.download('mnist_cnn.pt') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
